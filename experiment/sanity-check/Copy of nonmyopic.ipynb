{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1HZgAwnVqIGd6JGtCuAUSCENqXA01xPEX",
     "timestamp": 1664667529073
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNKWzDcWLoe2Qxkx23OMK4V"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# @title Utils\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import gpytorch\n",
    "except:\n",
    "    !pip install gpytorch\n",
    "    import gpytorch\n",
    "\n",
    "try:\n",
    "    import botorch\n",
    "except:\n",
    "    !pip install botorch\n",
    "    import botorch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from math import exp\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# from botorch.models import SingleTaskGP\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "# https://gist.github.com/neubig/e859ef0cc1a63d1c2ea4\n",
    "def rbf_kernel(x1, x2, variance=0.05):\n",
    "    return exp(-1 * ((x1 - x2) ** 2) / (2 * variance))\n",
    "\n",
    "\n",
    "def gram_matrix(xs):\n",
    "    return [[rbf_kernel(x1, x2) for x2 in xs] for x1 in xs]\n",
    "\n",
    "\n",
    "def ground_truth(draw=False):\n",
    "    xs = np.arange(0, 1, 0.001)\n",
    "    mean = np.zeros(xs.shape[0])\n",
    "    gram = gram_matrix(xs)\n",
    "\n",
    "    np.random.seed(62)\n",
    "    ys = np.random.multivariate_normal(mean, gram)\n",
    "    if draw:\n",
    "        plt.plot(xs, ys, color=\"blue\", alpha=0.1)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def query(x):\n",
    "    xs, ys = ground_truth()\n",
    "    xs_new = np.concatenate([xs, x])\n",
    "    mean_new = np.zeros(xs_new.shape[0])\n",
    "    gram_new = gram_matrix(xs_new)\n",
    "\n",
    "    np.random.seed(62)\n",
    "    ys_new = np.random.multivariate_normal(mean_new, gram_new)\n",
    "\n",
    "    return ys_new[-x.shape[0] :]\n",
    "\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "GeIQs5d-WzeG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1664667563484,
     "user_tz": 420,
     "elapsed": 22669,
     "user": {
      "displayName": "Sang Truong",
      "userId": "02872493733615210272"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20ad835e-4dbc-4201-bf93-8ae06c03776f"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gpytorch\n",
      "  Downloading gpytorch-1.8.1-py2.py3-none-any.whl (361 kB)\n",
      "\u001b[K     |████████████████████████████████| 361 kB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.7.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10->gpytorch) (4.1.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (3.1.0)\n",
      "Installing collected packages: gpytorch\n",
      "Successfully installed gpytorch-1.8.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting botorch\n",
      "  Downloading botorch-0.6.6-py3-none-any.whl (387 kB)\n",
      "\u001b[K     |████████████████████████████████| 387 kB 4.5 MB/s \n",
      "\u001b[?25hCollecting pyro-ppl>=1.8.0\n",
      "  Downloading pyro_ppl-1.8.2-py3-none-any.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 9.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from botorch) (1.7.3)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.12.1+cu113)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.7/dist-packages (from botorch) (0.6.0)\n",
      "Requirement already satisfied: gpytorch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from botorch) (1.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpytorch>=1.8.1->botorch) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch>=1.8.1->botorch) (1.0.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl>=1.8.0->botorch) (3.3.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl>=1.8.0->botorch) (4.64.1)\n",
      "Collecting pyro-api>=0.1.1\n",
      "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10->botorch) (4.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch->botorch) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.8.1->botorch) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch>=1.8.1->botorch) (3.1.0)\n",
      "Installing collected packages: pyro-api, pyro-ppl, botorch\n",
      "Successfully installed botorch-0.6.6 pyro-api-0.1.2 pyro-ppl-1.8.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title SingleTaskGP\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "r\"\"\"\n",
    "Gaussian Process Regression models based on GPyTorch models.\n",
    "\n",
    "These models are often a good starting point and are further documented in the\n",
    "tutorials.\n",
    "\n",
    "`SingleTaskGP`, `FixedNoiseGP`, and `HeteroskedasticSingleTaskGP` are all\n",
    "single-task exact GP models, differing in how they treat noise. They use\n",
    "relatively strong priors on the Kernel hyperparameters, which work best when\n",
    "covariates are normalized to the unit cube and outcomes are standardized (zero\n",
    "mean, unit variance).\n",
    "\n",
    "These models all work in batch mode (each batch having its own hyperparameters).\n",
    "When the training observations include multiple outputs, these models use\n",
    "batching to model outputs independently.\n",
    "\n",
    "These models all support multiple outputs. However, as single-task models,\n",
    "`SingleTaskGP`, `FixedNoiseGP`, and `HeteroskedasticSingleTaskGP` should be\n",
    "used only when the outputs are independent and all use the same training data.\n",
    "If outputs are independent and outputs have different training data, use the\n",
    "`ModelListGP`. When modeling correlations between outputs, use a multi-task\n",
    "model like `MultiTaskGP`.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from botorch import settings\n",
    "from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel\n",
    "from botorch.models.transforms.input import InputTransform\n",
    "from botorch.models.transforms.outcome import Log, OutcomeTransform\n",
    "from botorch.models.utils import fantasize as fantasize_flag, validate_input_scaling\n",
    "from botorch.sampling.samplers import MCSampler\n",
    "from gpytorch.constraints.constraints import GreaterThan\n",
    "from gpytorch.distributions.multivariate_normal import MultivariateNormal\n",
    "from gpytorch.kernels.matern_kernel import MaternKernel\n",
    "from gpytorch.kernels.scale_kernel import ScaleKernel\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.likelihoods.gaussian_likelihood import (\n",
    "    _GaussianLikelihoodBase,\n",
    "    FixedNoiseGaussianLikelihood,\n",
    "    GaussianLikelihood,\n",
    ")\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.likelihoods.noise_models import HeteroskedasticNoise\n",
    "from gpytorch.means.constant_mean import ConstantMean\n",
    "from gpytorch.means.mean import Mean\n",
    "from gpytorch.mlls.noise_model_added_loss_term import NoiseModelAddedLossTerm\n",
    "from gpytorch.models.exact_gp import ExactGP\n",
    "from gpytorch.module import Module\n",
    "from gpytorch.priors.smoothed_box_prior import SmoothedBoxPrior\n",
    "from gpytorch.priors.torch_priors import GammaPrior\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "MIN_INFERRED_NOISE_LEVEL = 1e-4\n",
    "\n",
    "\n",
    "class SingleTaskGP(BatchedMultiOutputGPyTorchModel, ExactGP):\n",
    "    r\"\"\"A single-task exact GP model.\n",
    "\n",
    "    A single-task exact GP using relatively strong priors on the Kernel\n",
    "    hyperparameters, which work best when covariates are normalized to the unit\n",
    "    cube and outcomes are standardized (zero mean, unit variance).\n",
    "\n",
    "    This model works in batch mode (each batch having its own hyperparameters).\n",
    "    When the training observations include multiple outputs, this model will use\n",
    "    batching to model outputs independently.\n",
    "\n",
    "    Use this model when you have independent output(s) and all outputs use the\n",
    "    same training data. If outputs are independent and outputs have different\n",
    "    training data, use the ModelListGP. When modeling correlations between\n",
    "    outputs, use the MultiTaskGP.\n",
    "\n",
    "    Example:\n",
    "        >>> train_X = torch.rand(20, 2)\n",
    "        >>> train_Y = torch.sin(train_X).sum(dim=1, keepdim=True)\n",
    "        >>> model = SingleTaskGP(train_X, train_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_X: Tensor,\n",
    "        train_Y: Tensor,\n",
    "        likelihood: Optional[Likelihood] = None,\n",
    "        covar_module: Optional[Module] = None,\n",
    "        mean_module: Optional[Mean] = None,\n",
    "        outcome_transform: Optional[OutcomeTransform] = None,\n",
    "        input_transform: Optional[InputTransform] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            train_X: A `batch_shape x n x d` tensor of training features.\n",
    "            train_Y: A `batch_shape x n x m` tensor of training observations.\n",
    "            likelihood: A likelihood. If omitted, use a standard\n",
    "                GaussianLikelihood with inferred noise level.\n",
    "            covar_module: The module computing the covariance (Kernel) matrix.\n",
    "                If omitted, use a `MaternKernel`.\n",
    "            mean_module: The mean function to be used. If omitted, use a\n",
    "                `ConstantMean`.\n",
    "            outcome_transform: An outcome transform that is applied to the\n",
    "                training data during instantiation and to the posterior during\n",
    "                inference (that is, the `Posterior` obtained by calling\n",
    "                `.posterior` on the model will be on the original scale).\n",
    "            input_transform: An input transform that is applied in the model's\n",
    "                forward pass.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            transformed_X = self.transform_inputs(\n",
    "                X=train_X, input_transform=input_transform\n",
    "            )\n",
    "        if outcome_transform is not None:\n",
    "            train_Y, _ = outcome_transform(train_Y)\n",
    "        self._validate_tensor_args(X=transformed_X, Y=train_Y)\n",
    "        ignore_X_dims = getattr(self, \"_ignore_X_dims_scaling_check\", None)\n",
    "        validate_input_scaling(\n",
    "            train_X=transformed_X, train_Y=train_Y, ignore_X_dims=ignore_X_dims\n",
    "        )\n",
    "        self._set_dimensions(train_X=train_X, train_Y=train_Y)\n",
    "        train_X, train_Y, _ = self._transform_tensor_args(X=train_X, Y=train_Y)\n",
    "        if likelihood is None:\n",
    "            # noise_prior = GammaPrior(1.1, 0.05)\n",
    "            noise_prior = GammaPrior(concentration=0.5, rate=1)\n",
    "            noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate\n",
    "            likelihood = GaussianLikelihood(\n",
    "                noise_prior=noise_prior,\n",
    "                batch_shape=self._aug_batch_shape,\n",
    "                noise_constraint=GreaterThan(\n",
    "                    MIN_INFERRED_NOISE_LEVEL,\n",
    "                    transform=None,\n",
    "                    initial_value=noise_prior_mode,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self._is_custom_likelihood = True\n",
    "        ExactGP.__init__(self, train_X, train_Y, likelihood)\n",
    "        if mean_module is None:\n",
    "            mean_module = ConstantMean(batch_shape=self._aug_batch_shape)\n",
    "        self.mean_module = mean_module\n",
    "        if covar_module is None:\n",
    "            covar_module = ScaleKernel(\n",
    "                # MaternKernel(\n",
    "                #     nu=2.5,\n",
    "                #     ard_num_dims=transformed_X.shape[-1],\n",
    "                #     batch_shape=self._aug_batch_shape,\n",
    "                #     lengthscale_prior=GammaPrior(3.0, 6.0),),\n",
    "                RBFKernel(\n",
    "                    ard_num_dims=transformed_X.shape[-1],\n",
    "                    batch_shape=self._aug_batch_shape,\n",
    "                    lengthscale_prior=GammaPrior(3.0, 6.0),\n",
    "                ),\n",
    "                batch_shape=self._aug_batch_shape,\n",
    "                outputscale_prior=GammaPrior(2.0, 0.15),\n",
    "            )\n",
    "            self._subset_batch_dict = {\n",
    "                \"likelihood.noise_covar.raw_noise\": -2,\n",
    "                \"mean_module.raw_constant\": -1,\n",
    "                \"covar_module.raw_outputscale\": -1,\n",
    "                \"covar_module.base_kernel.raw_lengthscale\": -3,\n",
    "            }\n",
    "        self.covar_module = covar_module\n",
    "        # TODO: Allow subsetting of other covar modules\n",
    "        if outcome_transform is not None:\n",
    "            self.outcome_transform = outcome_transform\n",
    "        if input_transform is not None:\n",
    "            self.input_transform = input_transform\n",
    "        self.to(train_X)\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        if self.training:\n",
    "            x = self.transform_inputs(x)\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "l8_WSd62RInC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1664667563638,
     "user_tz": 420,
     "elapsed": 169,
     "user": {
      "displayName": "Sang Truong",
      "userId": "02872493733615210272"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ModelInit\n",
    "\n",
    "train = False\n",
    "noise = False\n",
    "\n",
    "train_x = np.linspace(0, 1, 4)\n",
    "train_x = np.append(train_x, np.array([0.2]))\n",
    "train_y = query(train_x)\n",
    "\n",
    "train_x = torch.tensor(train_x, dtype=torch.float64)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float64)\n",
    "train_x = train_x[:, None]\n",
    "train_y = train_y[:, None]\n",
    "\n",
    "model = SingleTaskGP(train_X=train_x, train_Y=train_y)\n",
    "if noise:\n",
    "    model.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-6))\n",
    "\n",
    "mll = ExactMarginalLogLikelihood(likelihood=model.likelihood, model=model)\n",
    "mll = mll.to(train_x)\n",
    "\n",
    "if not train:\n",
    "    model.covar_module.base_kernel.lengthscale = 1\n",
    "    model.covar_module.outputscale = 1\n",
    "    model.likelihood.noise = 1e-10\n",
    "else:\n",
    "    optimizer = SGD([{\"params\": model.parameters()}], lr=0.05)\n",
    "    NUM_EPOCHS = 10000\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model to\n",
    "        # obtain the output MultivariateNormal\n",
    "        output = model(train_x)\n",
    "        # Compute negative marginal log likelihood\n",
    "        loss = -mll(output, model.train_targets)\n",
    "        # back prop gradients\n",
    "        loss.backward()\n",
    "        # print every 200 iterations\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1:>3}/{NUM_EPOCHS} - Loss: {loss.item():>4.10f} \"\n",
    "                f\"lengthscale: {model.covar_module.base_kernel.lengthscale.item():>4.10f} \"\n",
    "                f\"outputscale: {model.covar_module.outputscale.item():>4.10f} \"\n",
    "                f\"noise: {model.likelihood.noise.item():>4.10f}\"\n",
    "            )\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "# set model (and likelihood)\n",
    "model.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "wNDCIyf_WzYo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1664667570567,
     "user_tz": 420,
     "elapsed": 6942,
     "user": {
      "displayName": "Sang Truong",
      "userId": "02872493733615210272"
     }
    },
    "outputId": "a26357b8-c82b-4b31-b2d0-5550db5e2672"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SingleTaskGP(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (noise_prior): GammaPrior()\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (lengthscale_prior): GammaPrior()\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (outputscale_prior): GammaPrior()\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Naive Myopic\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "for x0 in torch.linspace(prev_x - 0.1, prev_x + 0.1, 10):\n",
    "    x0 = x0.reshape(1)\n",
    "    p_y0_on_x0_D0 = model.posterior(x0)\n",
    "\n",
    "    p_f_on_D1 = model.condition_on_observations(x0, p_y0_on_x0_D0.mean)\n",
    "\n",
    "    for a in torch.linspace(x0.item() - 0.1, x0.item() + 0.1, 10):\n",
    "        a = a.reshape(1)\n",
    "        p_y1_a_D1 = p_f_on_D1.posterior(a)\n",
    "        temp.append([x0, a, p_y1_a_D1.mean])\n",
    "\n",
    "temp = torch.tensor(temp)\n",
    "best = torch.argmin(temp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = temp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(test_x.cpu().detach().numpy(), test_y.cpu().detach().numpy(), \"green\")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "Bc9HLrTIzy2p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Naive 3 steps\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "for x0 in tqdm(torch.linspace(prev_x - 0.1, prev_x + 0.1, 10)):\n",
    "    x0 = x0.reshape(1)\n",
    "    p_y0_on_x0_D0 = model.posterior(x0)\n",
    "    p_f_on_D1 = model.condition_on_observations(x0, p_y0_on_x0_D0.mean)\n",
    "\n",
    "    for x1 in torch.linspace(x0.item() - 0.1, x0.item() + 0.1, 10):\n",
    "        x1 = x1.reshape(1)\n",
    "        p_y1_on_x1_D1 = p_f_on_D1.posterior(x1)\n",
    "        p_f_on_D2 = p_f_on_D1.condition_on_observations(x1, p_y1_on_x1_D1.mean)\n",
    "\n",
    "        for x2 in torch.linspace(x1.item() - 0.1, x1.item() + 0.1, 10):\n",
    "            x2 = x2.reshape(1)\n",
    "            p_y2_on_x2_D2 = p_f_on_D2.posterior(x2)\n",
    "            p_f_on_D3 = p_f_on_D2.condition_on_observations(x2, p_y2_on_x2_D2.mean)\n",
    "\n",
    "            for a in torch.linspace(x2.item() - 0.1, x2.item() + 0.1, 10):\n",
    "                a = a.reshape(1)\n",
    "                p_y3_a_D3 = p_f_on_D3.posterior(a)\n",
    "                temp.append([x0, a, p_y3_a_D3.mean])\n",
    "\n",
    "temp = torch.tensor(temp)\n",
    "best = torch.argmin(temp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = temp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(test_x.cpu().detach().numpy(), test_y.cpu().detach().numpy(), \"green\")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "hAMdEuM24Chq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 3 Steps\n",
    "\n",
    "\n",
    "def compute_ehig(x0, x1, x2, a):\n",
    "    p_y0_on_x0_D0 = model.posterior(x0)\n",
    "    p_f_on_D1 = model.condition_on_observations(x0, p_y0_on_x0_D0.mean)\n",
    "\n",
    "    p_y1_on_x1_D1 = p_f_on_D1.posterior(x1)\n",
    "    p_f_on_D2 = p_f_on_D1.condition_on_observations(x1, p_y1_on_x1_D1.mean)\n",
    "\n",
    "    p_y2_on_x2_D2 = p_f_on_D2.posterior(x2)\n",
    "    p_f_on_D3 = p_f_on_D2.condition_on_observations(x2, p_y2_on_x2_D2.mean)\n",
    "\n",
    "    p_y3_a_D3 = p_f_on_D3.posterior(a)\n",
    "\n",
    "    ehig = p_y3_a_D3.mean\n",
    "\n",
    "    return ehig\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "for x0 in tqdm(torch.linspace(prev_x - 0.1, prev_x + 0.1, 10)):\n",
    "    if not 0 <= x0.item() <= 1:\n",
    "        continue\n",
    "    x0 = x0.reshape(1)\n",
    "    for x1 in torch.linspace(x0.item() - 0.1, x0.item() + 0.1, 10):\n",
    "        if not 0 <= x1.item() <= 1:\n",
    "            continue\n",
    "        x1 = x1.reshape(1)\n",
    "        for x2 in torch.linspace(x1.item() - 0.1, x1.item() + 0.1, 10):\n",
    "            if not 0 <= x2.item() <= 1:\n",
    "                continue\n",
    "            x2 = x2.reshape(1)\n",
    "            for a in torch.linspace(x2.item() - 0.1, x2.item() + 0.1, 10):\n",
    "                if not 0 <= a.item() <= 1:\n",
    "                    continue\n",
    "                a = a.reshape(1)\n",
    "\n",
    "                ehig = compute_ehig(x0, x1, x2, a)\n",
    "\n",
    "                temp.append([x0, a, ehig])\n",
    "\n",
    "temp = torch.tensor(temp)\n",
    "best = torch.argmin(temp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = temp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(test_x.cpu().detach().numpy(), test_y.cpu().detach().numpy(), \"green\")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "jb-NL-Iu4CeF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 3 Steps with sigmoid\n",
    "\n",
    "\n",
    "def compute_ehig(x0, x1, x2, a):\n",
    "    \"\"\"\n",
    "    x0 to a: unconstraint optimization parameters\n",
    "    \"\"\"\n",
    "    x0 = torch.sigmoid(x0) * 0.2 + (prev_x - 0.1)\n",
    "\n",
    "    x1 = torch.sigmoid(x1) * 0.2 + (x0 - 0.1)\n",
    "\n",
    "    x2 = torch.sigmoid(x2) * 0.2 + (x1 - 0.1)\n",
    "    # x2 = x2 * 2\n",
    "\n",
    "    a = torch.sigmoid(a) * 0.2 + (x2 - 0.1)\n",
    "    # a = a * 2\n",
    "\n",
    "    x0 = x0.reshape(1)\n",
    "    x1 = x1.reshape(1)\n",
    "    x2 = x2.reshape(1)\n",
    "    a = a.reshape(1)\n",
    "\n",
    "    p_y0_on_x0_D0 = model.posterior(x0)\n",
    "    p_f_on_D1 = model.condition_on_observations(x0, p_y0_on_x0_D0.mean)\n",
    "\n",
    "    p_y1_on_x1_D1 = p_f_on_D1.posterior(x1)\n",
    "    p_f_on_D2 = p_f_on_D1.condition_on_observations(x1, p_y1_on_x1_D1.mean)\n",
    "\n",
    "    p_y2_on_x2_D2 = p_f_on_D2.posterior(x2)\n",
    "    p_f_on_D3 = p_f_on_D2.condition_on_observations(x2, p_y2_on_x2_D2.mean)\n",
    "\n",
    "    p_y3_a_D3 = p_f_on_D3.posterior(a)\n",
    "\n",
    "    ehig = p_y3_a_D3.mean\n",
    "\n",
    "    return x0, a, ehig\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "for x0 in tqdm(torch.linspace(-10, 10, 2)):\n",
    "    for x1 in torch.linspace(-10, 10, 2):\n",
    "        for x2 in torch.linspace(-10, 10, 2):\n",
    "            for a in torch.linspace(-10, 10, 2):\n",
    "                x0_, a_, ehig = compute_ehig(x0, x1, x2, a)\n",
    "\n",
    "                temp.append([x0_, a_, ehig])\n",
    "\n",
    "temp = torch.tensor(temp)\n",
    "best = torch.argmin(temp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = temp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(test_x.cpu().detach().numpy(), test_y.cpu().detach().numpy(), \"green\")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "d89m5IxqajwO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 3 Steps with sigmoid and adam\n",
    "\n",
    "\n",
    "def compute_ehig(x0, x1, x2, a):\n",
    "    \"\"\"\n",
    "    x0 to a: unconstraint optimization parameters\n",
    "    \"\"\"\n",
    "\n",
    "    x0 = torch.sigmoid(x0) * 0.2 + (prev_x - 0.1)\n",
    "\n",
    "    x1 = torch.sigmoid(x1) * 0.2 + (x0 - 0.1)\n",
    "\n",
    "    x2 = torch.sigmoid(x2) * 0.2 + (x1 - 0.1)\n",
    "    # x2 = x2 * 2\n",
    "\n",
    "    a = torch.sigmoid(a) * 0.2 + (x2 - 0.1)\n",
    "    # a = a * 2\n",
    "\n",
    "    x0 = x0.reshape(1)\n",
    "    x1 = x1.reshape(1)\n",
    "    x2 = x2.reshape(1)\n",
    "    a = a.reshape(1)\n",
    "\n",
    "    p_y0_on_x0_D0 = model.posterior(x0)\n",
    "    p_f_on_D1 = model.condition_on_observations(x0, p_y0_on_x0_D0.mean)\n",
    "\n",
    "    p_y1_on_x1_D1 = p_f_on_D1.posterior(x1)\n",
    "    p_f_on_D2 = p_f_on_D1.condition_on_observations(x1, p_y1_on_x1_D1.mean)\n",
    "\n",
    "    p_y2_on_x2_D2 = p_f_on_D2.posterior(x2)\n",
    "    p_f_on_D3 = p_f_on_D2.condition_on_observations(x2, p_y2_on_x2_D2.mean)\n",
    "\n",
    "    p_y3_a_D3 = p_f_on_D3.posterior(a)\n",
    "    ehig = p_y3_a_D3.mean\n",
    "\n",
    "    return x0, a, ehig.squeeze()\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "x0 = (torch.rand(1) * 6).requires_grad_(True)\n",
    "x1 = (torch.rand(1) * 6).requires_grad_(True)\n",
    "x2 = (torch.rand(1) * 6).requires_grad_(True)\n",
    "a = (torch.rand(1) * 6).requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam([x0, x1, x2, a], lr=0.1)\n",
    "for epoch in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    x0_, a_, ehig = compute_ehig(x0, x1, x2, a)\n",
    "    temp.append([x0_, a_, ehig])\n",
    "    ehig.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(ehig.item(), x0, x1, x2, a)\n",
    "\n",
    "\n",
    "temp = torch.tensor(temp)\n",
    "best = torch.argmin(temp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = temp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(test_x.cpu().detach().numpy(), test_y.cpu().detach().numpy(), \"green\")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "ca3QIWZyhB-R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 3 Steps with sigmoid and adam and sampling\n",
    "\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch import settings\n",
    "\n",
    "\n",
    "def compute_ehig(x0, x1, x2, a):\n",
    "    \"\"\"\n",
    "    x0 to a: unconstraint optimization parameters\n",
    "    \"\"\"\n",
    "    with settings.propagate_grads(state=True):\n",
    "        x0 = torch.sigmoid(x0) * 0.2 + (prev_x - 0.1)\n",
    "        x1 = torch.sigmoid(x1) * 0.2 + (x0 - 0.1)\n",
    "        x2 = torch.sigmoid(x2) * 0.2 + (x1 - 0.1)\n",
    "        a = torch.sigmoid(a) * 0.2 + (x2 - 0.1)\n",
    "\n",
    "        ehigs = 0\n",
    "        p_y0_on_x0_D0 = model.posterior(x0)\n",
    "        sampler = SobolQMCNormalSampler(\n",
    "            num_samples=4, resample=False, collapse_batch_dims=True\n",
    "        )\n",
    "        sample_y0_on_x0_D0 = sampler(p_y0_on_x0_D0)\n",
    "\n",
    "        for ind_x1, y0_on_x0_D0 in enumerate(sample_y0_on_x0_D0):\n",
    "            p_f_on_D1 = model.condition_on_observations(x0, y0_on_x0_D0)\n",
    "            p_y1_on_x1_D1 = p_f_on_D1.posterior(x1[ind_x1, :])\n",
    "            sample_y1_on_x1_D1 = sampler(p_y1_on_x1_D1)\n",
    "\n",
    "            for ind_x2, y1_on_x1_D1 in enumerate(sample_y1_on_x1_D1):\n",
    "                p_f_on_D2 = p_f_on_D1.condition_on_observations(\n",
    "                    x1[ind_x1, :], y1_on_x1_D1\n",
    "                )\n",
    "                p_y2_on_x2_D2 = p_f_on_D2.posterior(x2[ind_x1, ind_x2, :])\n",
    "                sample_y2_on_x2_D2 = sampler(p_y2_on_x2_D2)\n",
    "\n",
    "                for ind_a, p_y2_on_x2_D2 in enumerate(sample_y2_on_x2_D2):\n",
    "                    p_f_on_D3 = p_f_on_D2.condition_on_observations(\n",
    "                        x2[ind_x1, ind_x2, :], p_y2_on_x2_D2\n",
    "                    )\n",
    "                    p_y3_a_D3 = p_f_on_D3.posterior(a[ind_x1, ind_x2, ind_a, :])\n",
    "\n",
    "                    ehig = p_y3_a_D3.mean\n",
    "                    ehigs = ehigs + ehig\n",
    "\n",
    "        ehig = ehigs / (4**3)\n",
    "    return x0, a, ehig.squeeze()\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "\n",
    "x0 = (torch.rand(1) * 10).requires_grad_(True)\n",
    "x1 = (torch.rand(4, 1) * 10).requires_grad_(True)\n",
    "x2 = (torch.rand(4, 4, 1) * 10).requires_grad_(True)\n",
    "a = (torch.rand(4, 4, 4, 1) * 10).requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam([x0, x1, x2, a], lr=5)\n",
    "for epoch in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    x0_, a_, ehig = compute_ehig(x0, x1, x2, a)\n",
    "    temp.append([x0_, a_, ehig])\n",
    "    ehig.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"x0 \", x0.item(), \"a\", a[0, 0, 0].item(), \"loss\", ehig.item())\n",
    "\n",
    "# for x0 in tqdm(torch.linspace(-10, 10, 2)):\n",
    "#     x0 = x0.reshape(1)\n",
    "#     x0_, a_, ehig = compute_ehig(x0, x1, x2, a)\n",
    "#     print(ehig)\n",
    "\n",
    "tmp = []\n",
    "for i in range(len(temp)):\n",
    "    tmp.append([temp[i][0], temp[i][1][0, 0, 0], temp[i][2]])\n",
    "\n",
    "tmp = torch.tensor(tmp)\n",
    "best = torch.argmin(tmp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = tmp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\", label=\"current location\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\", label=\"optimal query\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\", label=\"optimal action\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    test_y.cpu().detach().numpy(),\n",
    "    \"green\",\n",
    "    label=\"Posterior mean\",\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Mg8tDhfgJpLW",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 5 Steps with sigmoid and adam and sampling\n",
    "\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch import settings\n",
    "\n",
    "\n",
    "def compute_ehig(x0, x1, x2, x3, x4, a):\n",
    "    \"\"\"\n",
    "    x0 to a: unconstraint optimization parameters\n",
    "    \"\"\"\n",
    "    with settings.propagate_grads(state=True):\n",
    "        x0 = torch.sigmoid(x0) * 0.2 + (prev_x - 0.1)\n",
    "        x1 = torch.sigmoid(x1) * 0.2 + (x0 - 0.1)\n",
    "        x2 = torch.sigmoid(x2) * 0.2 + (x1 - 0.1)\n",
    "        x3 = torch.sigmoid(x3) * 0.2 + (x2 - 0.1)\n",
    "        x4 = torch.sigmoid(x4) * 0.2 + (x3 - 0.1)\n",
    "        a = torch.sigmoid(a) * 0.2 + (x4 - 0.1)\n",
    "\n",
    "        ehigs = 0\n",
    "        p_y0_on_x0_D0 = model.posterior(x0)\n",
    "        sampler = SobolQMCNormalSampler(\n",
    "            num_samples=4, resample=False, collapse_batch_dims=True\n",
    "        )\n",
    "        sample_y0_on_x0_D0 = sampler(p_y0_on_x0_D0)\n",
    "\n",
    "        for ind_x1, y0_on_x0_D0 in enumerate(sample_y0_on_x0_D0):\n",
    "            p_f_on_D1 = model.condition_on_observations(x0, y0_on_x0_D0)\n",
    "            p_y1_on_x1_D1 = p_f_on_D1.posterior(x1[ind_x1, :])\n",
    "            sample_y1_on_x1_D1 = sampler(p_y1_on_x1_D1)\n",
    "\n",
    "            for ind_x2, y1_on_x1_D1 in enumerate(sample_y1_on_x1_D1):\n",
    "                p_f_on_D2 = p_f_on_D1.condition_on_observations(\n",
    "                    x1[ind_x1, :], y1_on_x1_D1\n",
    "                )\n",
    "                p_y2_on_x2_D2 = p_f_on_D2.posterior(x2[ind_x1, ind_x2, :])\n",
    "                sample_y2_on_x2_D2 = sampler(p_y2_on_x2_D2)\n",
    "\n",
    "                for ind_x3, y2_on_x2_D2 in enumerate(sample_y2_on_x2_D2):\n",
    "                    p_f_on_D3 = p_f_on_D2.condition_on_observations(\n",
    "                        x2[ind_x1, ind_x2, :], y2_on_x2_D2\n",
    "                    )\n",
    "                    p_y3_x3_D3 = p_f_on_D3.posterior(x3[ind_x1, ind_x2, ind_x3, :])\n",
    "                    sample_y3_x3_D3 = sampler(p_y3_x3_D3)\n",
    "\n",
    "                    for ind_x4, y3_on_x3_D3 in enumerate(sample_y3_x3_D3):\n",
    "                        p_f_on_D4 = p_f_on_D3.condition_on_observations(\n",
    "                            x3[ind_x1, ind_x2, ind_x3, :], y3_on_x3_D3\n",
    "                        )\n",
    "                        p_y4_on_x4_D4 = p_f_on_D4.posterior(\n",
    "                            x4[ind_x1, ind_x2, ind_x3, ind_x4, :]\n",
    "                        )\n",
    "                        sample_y4_on_x4_D4 = sampler(p_y4_on_x4_D4)\n",
    "\n",
    "                        for ind_a, y4_on_x4_D4 in enumerate(sample_y4_on_x4_D4):\n",
    "                            p_f_on_D5 = p_f_on_D4.condition_on_observations(\n",
    "                                x4[ind_x1, ind_x2, ind_x3, ind_x4, :], y4_on_x4_D4\n",
    "                            )\n",
    "                            p_y5_on_a_D5 = p_f_on_D5.posterior(\n",
    "                                a[ind_x1, ind_x2, ind_x3, ind_x4, ind_a, :]\n",
    "                            )\n",
    "\n",
    "                            ehig = p_y5_on_a_D5.mean\n",
    "                            ehigs = ehigs + ehig\n",
    "\n",
    "        ehig = ehigs / (4**5)\n",
    "    return x0, a, ehig.squeeze()\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "\n",
    "x0 = (torch.rand(1) * 10).requires_grad_(True)\n",
    "x1 = (torch.rand(4, 1) * 10).requires_grad_(True)\n",
    "x2 = (torch.rand(4, 4, 1) * 10).requires_grad_(True)\n",
    "x3 = (torch.rand(4, 4, 4, 1) * 10).requires_grad_(True)\n",
    "x4 = (torch.rand(4, 4, 4, 4, 1) * 10).requires_grad_(True)\n",
    "a = (torch.rand(4, 4, 4, 4, 4, 1) * 10).requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam([x0, x1, x2, x3, x4, a], lr=2)\n",
    "for epoch in tqdm(range(20)):\n",
    "    optimizer.zero_grad()\n",
    "    x0_, a_, ehig = compute_ehig(x0, x1, x2, x3, x4, a)\n",
    "    temp.append([x0_, a_, ehig])\n",
    "    ehig.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(\"x0 \", x0.item(), \"a\", a[0, 0, 0, 0, 0].item(), \"loss\", ehig.item())\n",
    "\n",
    "# for x0 in tqdm(torch.linspace(-10, 10, 2)):\n",
    "#     x0 = x0.reshape(1)\n",
    "#     x0_, a_, ehig = compute_ehig(x0, x1, x2, a)\n",
    "#     print(ehig)\n",
    "\n",
    "tmp = []\n",
    "for i in range(len(temp)):\n",
    "    tmp.append([temp[i][0], temp[i][1][0, 0, 0, 0, 0], temp[i][2]])\n",
    "\n",
    "tmp = torch.tensor(tmp)\n",
    "best = torch.argmin(tmp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = tmp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\", label=\"current location\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\", label=\"optimal query\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\", label=\"optimal action\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    test_y.cpu().detach().numpy(),\n",
    "    \"green\",\n",
    "    label=\"Posterior mean\",\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "hIb-aNUzcjYD",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Joint 5 Steps with sigmoid and adam and sampling -- parallel\n",
    "\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch import settings\n",
    "import itertools\n",
    "\n",
    "\n",
    "def compute_ehig(xi, horizon=5, num_samples=2):\n",
    "    with settings.propagate_grads(state=True):\n",
    "        for h in range(0, horizon + 1):\n",
    "            pvs_x = xi[h - 1] if h > 0 else prev_x\n",
    "            xi[h] = torch.sigmoid(xi[h]) * 0.2 + (pvs_x - 0.1)\n",
    "\n",
    "        ehigs = 0\n",
    "        sample_yi_on_xi_Di = {}\n",
    "        prev_ind = np.ones(horizon) * -1\n",
    "        p_f_on_Di = {}\n",
    "        p_f_on_Di[0] = model\n",
    "        sampler = SobolQMCNormalSampler(\n",
    "            num_samples=num_samples, resample=False, collapse_batch_dims=True\n",
    "        )\n",
    "\n",
    "        p_y0_on_x0_D0 = model.posterior(xi[0])\n",
    "        sample_yi_on_xi_Di[0] = sampler(p_y0_on_x0_D0)\n",
    "\n",
    "        for ind in itertools.product(range(num_samples), repeat=horizon):\n",
    "            equal_bool = np.equal(ind, prev_ind)\n",
    "            prev_ind = ind\n",
    "\n",
    "            for i in range(horizon):\n",
    "                if not equal_bool[i]:\n",
    "                    p_f_on_Di[i + 1] = p_f_on_Di[i].condition_on_observations(\n",
    "                        xi[i][ind[:i]], sample_yi_on_xi_Di[i][ind[i]]\n",
    "                    )\n",
    "                    sample_yi_on_xi_Di[i + 1] = sampler(\n",
    "                        p_f_on_Di[i + 1].posterior(xi[i + 1][ind[: i + 1]])\n",
    "                    )\n",
    "\n",
    "            ehig = sample_yi_on_xi_Di[horizon].mean()\n",
    "            ehigs = ehigs + ehig\n",
    "\n",
    "        ehig = ehigs / (num_samples**horizon)\n",
    "\n",
    "        return xi[0], xi[-1], ehig.squeeze()\n",
    "\n",
    "\n",
    "prev_x = 0.2\n",
    "temp = []\n",
    "\n",
    "xi = []\n",
    "# xi[-1] = prev_x\n",
    "dim_xi = [1]\n",
    "horizon = 10\n",
    "for h in range(horizon + 1):\n",
    "    xi.append((torch.rand(dim_xi) * 10).requires_grad_(True))\n",
    "    dim_xi.insert(0, 4)\n",
    "\n",
    "optimizer = optim.Adam(xi, lr=0.5)\n",
    "for epoch in tqdm(range(20)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xi_in = [element_xi.clone() for element_xi in xi]\n",
    "    x0_, a_, ehig = compute_ehig(xi_in, horizon=horizon)\n",
    "    temp.append([x0_, a_, ehig.detach()])\n",
    "\n",
    "    ehig.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(\n",
    "            \"x0 \",\n",
    "            xi[0].item(),\n",
    "            \"a\",\n",
    "            xi[horizon][tuple([0] * horizon)].item(),\n",
    "            \"loss\",\n",
    "            ehig.item(),\n",
    "        )\n",
    "\n",
    "tmp = []\n",
    "for i in range(len(temp)):\n",
    "    tmp.append([temp[i][0], temp[i][1][tuple([0] * horizon)], temp[i][2]])\n",
    "\n",
    "tmp = torch.tensor(tmp)\n",
    "best = torch.argmin(tmp, dim=0)[2].item()\n",
    "best_x, best_a, best_hes = tmp[best].numpy().tolist()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ground_truth(draw=True)\n",
    "\n",
    "plt.vlines(prev_x, -1, 1, color=\"black\", label=\"current location\")\n",
    "plt.vlines(prev_x - 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "plt.vlines(prev_x + 0.1, -1, 1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.vlines(best_x, -1, 1, color=\"red\", label=\"optimal query\")\n",
    "plt.vlines(best_a, -1, 1, color=\"blue\", label=\"optimal action\")\n",
    "\n",
    "plt.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "\n",
    "# compute posterior\n",
    "test_x = torch.linspace(0, 1, 100)\n",
    "posterior = model.posterior(test_x)\n",
    "test_y = posterior.mean\n",
    "lower, upper = posterior.mvn.confidence_region()\n",
    "\n",
    "plt.plot(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    test_y.cpu().detach().numpy(),\n",
    "    \"green\",\n",
    "    label=\"Posterior mean\",\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    test_x.cpu().detach().numpy(),\n",
    "    lower.cpu().detach().numpy(),\n",
    "    upper.cpu().detach().numpy(),\n",
    "    alpha=0.25,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKdI-AwB1icx",
    "outputId": "9811af47-0aa7-4ebf-ac2d-023f7f771136"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# gold standard -- exhastive search over x and a\n",
    "\n",
    "# set noise level to almost 0\n",
    "\n",
    "# first query at a\n",
    "\n",
    "# for x in (a-0.1, a+0.1):\n",
    "\n",
    "# compute y | x, D\n",
    "\n",
    "# for y in sample y (only need 1 sample if the noise is small)\n",
    "\n",
    "# for a in (x-0.1, x+0.1):\n",
    "\n",
    "# compute  y' | x, y, a\n",
    "\n",
    "# compute the mean"
   ],
   "metadata": {
    "id": "_Wu0K-G76Qbb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i = 4\n",
    "i + i**2 + i**3 + i**4 + i**5 + i**6 + i**7 + i**8 + i**9 + i**10"
   ],
   "metadata": {
    "id": "NI7aXiagsVGn"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}