{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pdpMRCxqrMT_"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "test = False\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "path = \"/content/drive/My Drive/Colab Notebooks/nonmyopic/\"\n",
    "\n",
    "botorch_version = \"0.4.0\"  # @param ['0.4.0', 'latest']\n",
    "\n",
    "try:\n",
    "    import botorch\n",
    "except:\n",
    "    if botorch_version == \"latest\":\n",
    "        !pip install botorch\n",
    "    else:\n",
    "        !pip install botorch==0.4.0\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import exp\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import botorch\n",
    "import gpytorch\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch import settings\n",
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "import os\n",
    "from copy import copy\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "color = {\n",
    "    \"C0\": \"#1f77b4\",\n",
    "    \"C1\": \"#ff7f0e\",\n",
    "    \"C2\": \"#2ca02c\",\n",
    "    \"C3\": \"#d62728\",\n",
    "    \"C4\": \"#9467bd\",\n",
    "    \"C5\": \"#bcbd22\",\n",
    "    \"C6\": \"#e377c2\",\n",
    "    \"C7\": \"#17becf\",\n",
    "    # 'C8':'#bcbd22',\n",
    "    # 'C9':'#17becf',\n",
    "}\n",
    "\n",
    "figsize = [7, 3]\n",
    "plt.rcParams[\"figure.figsize\"] = figsize\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "\n",
    "dpi = 100\n",
    "imgtype = \"png\"\n",
    "\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=16)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=16)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utilities for Gaussian process (GP) inference.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import solve_triangular\n",
    "from scipy.spatial.distance import cdist\n",
    "import itertools\n",
    "\n",
    "\n",
    "def kern_exp_quad_ard(xmat1, xmat2, ls, alpha):\n",
    "    \"\"\"\n",
    "    Exponentiated quadratic kernel function with\n",
    "    dimensionwise lengthscales if ls is an ndarray.\n",
    "    \"\"\"\n",
    "    xmat1 = np.expand_dims(xmat1, axis=1)\n",
    "    xmat2 = np.expand_dims(xmat2, axis=0)\n",
    "    diff = xmat1 - xmat2\n",
    "    diff /= ls\n",
    "    norm = np.sum(diff**2, axis=-1) / 2.0\n",
    "    kern = alpha**2 * np.exp(-norm)\n",
    "    return kern\n",
    "\n",
    "\n",
    "def kern_exp_quad_ard_sklearn(xmat1, xmat2, ls, alpha):\n",
    "    \"\"\"\n",
    "    Exponentiated quadratic kernel function with dimensionwise lengthscales if ls is an\n",
    "    ndarray, based on scikit-learn implementation.\n",
    "    \"\"\"\n",
    "    dists = cdist(xmat1 / ls, xmat2 / ls, metric=\"sqeuclidean\")\n",
    "    exp_neg_norm = np.exp(-0.5 * dists)\n",
    "    return alpha**2 * exp_neg_norm\n",
    "\n",
    "\n",
    "def kern_exp_quad_ard_per(xmat1, xmat2, ls, alpha, pdims, period=2):\n",
    "    \"\"\"\n",
    "    Exponentiated quadratic kernel function with\n",
    "    - dimensionwise lengthscales if ls is an ndarray\n",
    "    - periodic dimensions denoted by pdims. We assume that the period\n",
    "    is 2.\n",
    "    \"\"\"\n",
    "    xmat1 = np.expand_dims(xmat1, axis=1)\n",
    "    xmat2 = np.expand_dims(xmat2, axis=0)\n",
    "    diff = xmat1 - xmat2\n",
    "    diff[..., pdims] = np.sin((np.pi * diff[..., pdims] / period) % (2 * np.pi))\n",
    "    # diff[..., pdims] = np.cos( (np.pi/2) + (np.pi * diff[..., pdims] / period) )\n",
    "    diff /= ls\n",
    "    norm = np.sum(diff**2, axis=-1) / 2.0\n",
    "    kern = alpha**2 * np.exp(-norm)\n",
    "\n",
    "    return kern\n",
    "\n",
    "\n",
    "def kern_exp_quad_noard(xmat1, xmat2, ls, alpha):\n",
    "    \"\"\"\n",
    "    Exponentiated quadratic kernel function (aka squared exponential kernel aka\n",
    "    RBF kernel).\n",
    "    \"\"\"\n",
    "    kern = alpha**2 * kern_exp_quad_noard_noscale(xmat1, xmat2, ls)\n",
    "    return kern\n",
    "\n",
    "\n",
    "def kern_exp_quad_noard_noscale(xmat1, xmat2, ls):\n",
    "    \"\"\"\n",
    "    Exponentiated quadratic kernel function (aka squared exponential kernel aka\n",
    "    RBF kernel), without scale parameter.\n",
    "    \"\"\"\n",
    "    distmat = squared_euc_distmat(xmat1, xmat2)\n",
    "    norm = distmat / (2 * ls**2)\n",
    "    exp_neg_norm = np.exp(-norm)\n",
    "    return exp_neg_norm\n",
    "\n",
    "\n",
    "def squared_euc_distmat(xmat1, xmat2, coef=1.0):\n",
    "    \"\"\"\n",
    "    Distance matrix of squared euclidean distance (multiplied by coef) between\n",
    "    points in xmat1 and xmat2.\n",
    "    \"\"\"\n",
    "    return coef * cdist(xmat1, xmat2, \"sqeuclidean\")\n",
    "\n",
    "\n",
    "def kern_distmat(xmat1, xmat2, ls, alpha, distfn):\n",
    "    \"\"\"\n",
    "    Kernel for a given distmat, via passed in distfn (which is assumed to be fn\n",
    "    of xmat1 and xmat2 only).\n",
    "    \"\"\"\n",
    "    distmat = distfn(xmat1, xmat2)\n",
    "    kernmat = alpha**2 * np.exp(-distmat / (2 * ls**2))\n",
    "    return kernmat\n",
    "\n",
    "\n",
    "def kern_simple_list(xlist1, xlist2, ls, alpha, base_dist=5.0):\n",
    "    \"\"\"\n",
    "    Kernel for two lists containing elements that can be compared for equality.\n",
    "    K(a,b) = 1 + base_dist if a and b are equal and K(a,b) = base_dist otherwise.\n",
    "    \"\"\"\n",
    "    distmat = simple_list_distmat(xlist1, xlist2)\n",
    "    distmat = distmat + base_dist\n",
    "    kernmat = alpha**2 * np.exp(-distmat / (2 * ls**2))\n",
    "    return kernmat\n",
    "\n",
    "\n",
    "def simple_list_distmat(xlist1, xlist2, weight=1.0, additive=False):\n",
    "    \"\"\"\n",
    "    Return distance matrix containing zeros when xlist1[i] == xlist2[j] and 0 otherwise.\n",
    "    \"\"\"\n",
    "    prod_list = list(itertools.product(xlist1, xlist2))\n",
    "    len1 = len(xlist1)\n",
    "    len2 = len(xlist2)\n",
    "    try:\n",
    "        binary_mat = np.array([x[0] != x[1] for x in prod_list]).astype(int)\n",
    "    except:\n",
    "        # For cases where comparison returns iterable of bools\n",
    "        binary_mat = np.array([all(x[0] != x[1]) for x in prod_list]).astype(int)\n",
    "\n",
    "    binary_mat = binary_mat.reshape(len1, len2)\n",
    "\n",
    "    if additive:\n",
    "        distmat = weight + binary_mat\n",
    "    else:\n",
    "        distmat = weight * binary_mat\n",
    "\n",
    "    return distmat\n",
    "\n",
    "\n",
    "def get_product_kernel(kernel_list, additive=False):\n",
    "    \"\"\"Given a list of kernel functions, return product kernel.\"\"\"\n",
    "\n",
    "    def product_kernel(x1, x2, ls, alpha):\n",
    "        \"\"\"Kernel returning elementwise-product of kernel matrices from kernel_list.\"\"\"\n",
    "        mat_prod = kernel_list[0](x1, x2, ls, 1.0)\n",
    "        for kernel in kernel_list[1:]:\n",
    "            if additive:\n",
    "                mat_prod = mat_prod + kernel(x1, x2, ls, 1.0)\n",
    "            else:\n",
    "                mat_prod = mat_prod * kernel(x1, x2, ls, 1.0)\n",
    "        mat_prod = alpha**2 * mat_prod\n",
    "        return mat_prod\n",
    "\n",
    "    return product_kernel\n",
    "\n",
    "\n",
    "def get_cholesky_decomp(k11_nonoise, sigma, psd_str):\n",
    "    \"\"\"Return cholesky decomposition.\"\"\"\n",
    "    if psd_str == \"try_first\":\n",
    "        k11 = k11_nonoise + sigma**2 * np.eye(k11_nonoise.shape[0])\n",
    "        try:\n",
    "            return stable_cholesky(k11, False)\n",
    "        except np.linalg.linalg.LinAlgError:\n",
    "            return get_cholesky_decomp(k11_nonoise, sigma, \"project_first\")\n",
    "    elif psd_str == \"project_first\":\n",
    "        k11_nonoise = project_symmetric_to_psd_cone(k11_nonoise)\n",
    "        return get_cholesky_decomp(k11_nonoise, sigma, \"is_psd\")\n",
    "    elif psd_str == \"is_psd\":\n",
    "        k11 = k11_nonoise + sigma**2 * np.eye(k11_nonoise.shape[0])\n",
    "        return stable_cholesky(k11)\n",
    "\n",
    "\n",
    "def stable_cholesky(mmat, make_psd=True, verbose=False):\n",
    "    \"\"\"Return a 'stable' cholesky decomposition of mmat.\"\"\"\n",
    "    if mmat.size == 0:\n",
    "        return mmat\n",
    "    try:\n",
    "        lmat = np.linalg.cholesky(mmat)\n",
    "    except np.linalg.linalg.LinAlgError as e:\n",
    "        if not make_psd:\n",
    "            raise e\n",
    "        diag_noise_power = -11\n",
    "        max_mmat = np.diag(mmat).max()\n",
    "        diag_noise = np.diag(mmat).max() * 1e-11\n",
    "        break_loop = False\n",
    "        while not break_loop:\n",
    "            try:\n",
    "                lmat = np.linalg.cholesky(\n",
    "                    mmat + ((10**diag_noise_power) * max_mmat) * np.eye(mmat.shape[0])\n",
    "                )\n",
    "                break_loop = True\n",
    "            except np.linalg.linalg.LinAlgError:\n",
    "                if diag_noise_power > -9:\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            \"\\t*stable_cholesky failed with \"\n",
    "                            \"diag_noise_power=%d.\" % (diag_noise_power)\n",
    "                        )\n",
    "                diag_noise_power += 1\n",
    "            if diag_noise_power >= 5:\n",
    "                print(\"\\t*stable_cholesky failed: added diag noise = %e\" % (diag_noise))\n",
    "    return lmat\n",
    "\n",
    "\n",
    "def project_symmetric_to_psd_cone(mmat, is_symmetric=True, epsilon=0):\n",
    "    \"\"\"Project symmetric matrix mmat to the PSD cone.\"\"\"\n",
    "    if is_symmetric:\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eigh(mmat)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"\\tLinAlgError encountered with np.eigh. Defaulting to eig.\")\n",
    "            eigvals, eigvecs = np.linalg.eig(mmat)\n",
    "            eigvals = np.real(eigvals)\n",
    "            eigvecs = np.real(eigvecs)\n",
    "    else:\n",
    "        eigvals, eigvecs = np.linalg.eig(mmat)\n",
    "    clipped_eigvals = np.clip(eigvals, epsilon, np.inf)\n",
    "    return (eigvecs * clipped_eigvals).dot(eigvecs.T)\n",
    "\n",
    "\n",
    "def solve_lower_triangular(amat, b):\n",
    "    \"\"\"Solves amat*x=b when amat is lower triangular.\"\"\"\n",
    "    return solve_triangular_base(amat, b, lower=True)\n",
    "\n",
    "\n",
    "def solve_upper_triangular(amat, b):\n",
    "    \"\"\"Solves amat*x=b when amat is upper triangular.\"\"\"\n",
    "    return solve_triangular_base(amat, b, lower=False)\n",
    "\n",
    "\n",
    "def solve_triangular_base(amat, b, lower):\n",
    "    \"\"\"Solves amat*x=b when amat is a triangular matrix.\"\"\"\n",
    "    if amat.size == 0 and b.shape[0] == 0:\n",
    "        return np.zeros((b.shape))\n",
    "    else:\n",
    "        return solve_triangular(amat, b, lower=lower)\n",
    "\n",
    "\n",
    "def sample_mvn(mu, covmat, nsamp):\n",
    "    \"\"\"\n",
    "    Sample from multivariate normal distribution with mean mu and covariance\n",
    "    matrix covmat.\n",
    "    \"\"\"\n",
    "    mu = mu.reshape(-1)\n",
    "    ndim = len(mu)\n",
    "    lmat = stable_cholesky(covmat)\n",
    "    umat = np.random.normal(size=(ndim, nsamp))\n",
    "    return lmat.dot(umat).T + mu\n",
    "\n",
    "\n",
    "def gp_post(x_train, y_train, x_pred, ls, alpha, full_cov=True):\n",
    "    \"\"\"Compute parameters of GP posterior\"\"\"\n",
    "    k11_nonoise = kern_exp_quad_noard(x_train, x_train, alpha=alpha, ls=ls)\n",
    "    lmat = get_cholesky_decomp(k11_nonoise, 1e-2, \"try_first\")\n",
    "    smat = solve_upper_triangular(lmat.T, solve_lower_triangular(lmat, y_train))\n",
    "    k21 = kern_exp_quad_noard(x_pred, x_train, alpha=alpha, ls=ls)\n",
    "    mu2 = k21.dot(smat)\n",
    "    k22 = kern_exp_quad_noard(x_pred, x_pred, alpha=alpha, ls=ls)\n",
    "    vmat = solve_lower_triangular(lmat, k21.T)\n",
    "    k2 = k22 - vmat.T.dot(vmat)\n",
    "    if full_cov is False:\n",
    "        k2 = np.sqrt(np.diag(k2))\n",
    "    return mu2, k2\n",
    "\n",
    "\n",
    "def ground_truth(\n",
    "    draw_true_model=False, n_dim=1, draw_3D=False, alpha=1, ls=math.sqrt(0.05)\n",
    "):\n",
    "    if n_dim == 1:\n",
    "        xs = np.linspace(0, 1, 1000)[:, None]\n",
    "        mean = np.zeros(xs.shape[0])\n",
    "        cov = kern_exp_quad_noard(xs, xs, alpha=alpha, ls=ls)\n",
    "\n",
    "        np.random.seed(24)\n",
    "        ys = sample_mvn(mean, cov, 1).squeeze()\n",
    "        if draw_true_model:\n",
    "            plt.plot(xs, ys, color=\"blue\", alpha=0.1)\n",
    "        return xs, ys\n",
    "\n",
    "    elif n_dim == 2:\n",
    "        grid = 20j\n",
    "        xs = np.mgrid[0:1:grid, 0:1:grid].reshape(2, -1).T\n",
    "        mean = np.zeros(xs.shape[0])\n",
    "        cov = kern_exp_quad_noard(xs, xs, alpha=alpha, ls=ls)\n",
    "\n",
    "        np.random.seed(5)\n",
    "        ys = sample_mvn(mean, cov, 1).squeeze()\n",
    "\n",
    "        if draw_true_model:\n",
    "            xpts = np.linspace(0, 1, int(abs(grid)))\n",
    "            ypts = np.linspace(0, 1, int(abs(grid)))\n",
    "            X, Y = np.meshgrid(xpts, ypts)\n",
    "            resol = int(abs(grid))\n",
    "            Z = ys.reshape(resol, resol).T\n",
    "\n",
    "            if draw_3D:\n",
    "                fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "                cf = ax.plot_surface(\n",
    "                    X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False\n",
    "                )\n",
    "                cbar = plt.colorbar(cf, fraction=0.046, pad=0.04)\n",
    "\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.title(\"Ground truth function\")\n",
    "            cf = plt.contourf(\n",
    "                X,\n",
    "                Y,\n",
    "                Z,\n",
    "                40,\n",
    "                cmap=cm.coolwarm,\n",
    "                zorder=0,\n",
    "                levels=np.linspace(-3, 3, 40),\n",
    "                extend=\"both\",\n",
    "            )\n",
    "            cbar = plt.colorbar(cf, fraction=0.046, pad=0.04, ticks=range(-3, 3, 1))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        return xs, ys\n",
    "\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "def p_loss(x):\n",
    "    return torch.maximum(torch.abs(x - 0.5) - 0.5, torch.tensor([0]))\n",
    "\n",
    "\n",
    "def func(train_x):\n",
    "    alpha = 1\n",
    "    ls = math.sqrt(0.05)\n",
    "\n",
    "    xs, ys = ground_truth(draw_true_model=False, n_dim=n_dim, alpha=alpha, ls=ls)\n",
    "\n",
    "    train_y, _ = gp_post(x_train=xs, y_train=ys, x_pred=train_x, alpha=alpha, ls=ls)\n",
    "\n",
    "    train_y = torch.tensor(train_y, dtype=torch.float64)\n",
    "\n",
    "    return train_y\n",
    "\n",
    "\n",
    "def init_data(n_init=4):\n",
    "    if n_dim == 1:\n",
    "        train_x = torch.linspace(0, 1, n_init).reshape(-1, n_dim)\n",
    "    elif n_dim == 2:\n",
    "        n_init = complex(0, n_init)\n",
    "        train_x = np.mgrid[0:1:n_init, 0:1:n_init].reshape(2, -1).T\n",
    "        train_x = torch.from_numpy(train_x)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    train_x = torch.cat([train_x, prev_x.reshape(-1, n_dim)], axis=0)\n",
    "    train_y = func(train_x)\n",
    "\n",
    "    train_x = train_x.double()\n",
    "    train_y = train_y.double()\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def init_model(n_dim, prev_x, train_x, train_y, train=False):\n",
    "    train_y = train_y[:, None]\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_y)\n",
    "    if train:\n",
    "        model.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-6))\n",
    "\n",
    "    mll = ExactMarginalLogLikelihood(likelihood=model.likelihood, model=model)\n",
    "    mll = mll.to(train_x)\n",
    "\n",
    "    if not train:\n",
    "        alpha = 1\n",
    "        ls = math.sqrt(0.05)\n",
    "        model.covar_module.base_kernel.lengthscale = ls\n",
    "        model.covar_module.outputscale = alpha\n",
    "        model.likelihood.noise = 1e-10\n",
    "    else:\n",
    "        optimizer = SGD([{\"params\": model.parameters()}], lr=0.05)\n",
    "        NUM_EPOCHS = 20000\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, model.train_targets)\n",
    "            loss.backward()\n",
    "            if (epoch + 1) % 2000 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1:>3}/{NUM_EPOCHS} - Loss: {loss.item():>2.2} \",\n",
    "                    \"lengthscale: \",\n",
    "                    model.covar_module.base_kernel.lengthscale.data,\n",
    "                    f\"outputscale: {model.covar_module.outputscale.item():>2.2f} \",\n",
    "                    f\"noise: {model.likelihood.noise.item():>2.5f}\",\n",
    "                )\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def draw_posterior(n_dim, model):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if n_dim == 1:\n",
    "        test_x = torch.linspace(0, 1, 100)\n",
    "        posterior = model.posterior(test_x)\n",
    "        test_y = posterior.mean.squeeze()\n",
    "        lower, upper = posterior.mvn.confidence_region()\n",
    "        lower = lower.squeeze()\n",
    "        upper = upper.squeeze()\n",
    "\n",
    "        if len(test_y.shape) == 2:\n",
    "            test_y = test_y[0, :]\n",
    "            lower = lower[0, :]\n",
    "            upper = upper[0, :]\n",
    "\n",
    "        plt.plot(\n",
    "            test_x.cpu().detach().numpy(),\n",
    "            test_y.cpu().detach().numpy(),\n",
    "            color[\"C1\"],\n",
    "            label=\"Best h-step Posterior mean (before observed data)\",\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            test_x.cpu().detach().numpy(),\n",
    "            lower.cpu().detach().numpy(),\n",
    "            upper.cpu().detach().numpy(),\n",
    "            alpha=0.25,\n",
    "            color=color[\"C2\"],\n",
    "        )\n",
    "\n",
    "    elif n_dim == 2:\n",
    "        grid = 20j\n",
    "        test_x = np.mgrid[0:1:grid, 0:1:grid].reshape(2, -1).T\n",
    "        test_x = torch.tensor(test_x)\n",
    "\n",
    "        posterior = model.posterior(test_x)\n",
    "        test_y = posterior.mean.detach().numpy().squeeze()\n",
    "\n",
    "        if len(test_y.shape) == 3:\n",
    "            test_y = test_y[0, 0, :]\n",
    "        elif len(test_y.shape) == 2:\n",
    "            test_y = test_y[0, :]\n",
    "\n",
    "        xpts = np.linspace(0, 1, int(abs(grid)))\n",
    "        ypts = np.linspace(0, 1, int(abs(grid)))\n",
    "        X, Y = np.meshgrid(xpts, ypts)\n",
    "        resol = int(abs(grid))\n",
    "        Z = test_y.reshape(resol, resol).T\n",
    "\n",
    "        # levels = np.linspace(-1.5,2.5,40)\n",
    "\n",
    "        cf = plt.contourf(\n",
    "            X,\n",
    "            Y,\n",
    "            Z,\n",
    "            40,\n",
    "            cmap=cm.coolwarm,\n",
    "            zorder=0,\n",
    "            levels=np.linspace(-3, 3, 40),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        cbar = plt.colorbar(cf, fraction=0.046, pad=0.04, ticks=range(-3, 3, 1))\n",
    "\n",
    "        plt.scatter(train_x[:, 0], train_x[:, 1], marker=\"*\", color=\"black\")\n",
    "\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "# #!/usr/bin/env python3\n",
    "# # Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# #\n",
    "# # This source code is licensed under the MIT license found in the\n",
    "# # LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# r\"\"\"\n",
    "# Gaussian Process Regression models based on GPyTorch models.\n",
    "\n",
    "# These models are often a good starting point and are further documented in the\n",
    "# tutorials.\n",
    "\n",
    "# `SingleTaskGP`, `FixedNoiseGP`, and `HeteroskedasticSingleTaskGP` are all\n",
    "# single-task exact GP models, differing in how they treat noise. They use\n",
    "# relatively strong priors on the Kernel hyperparameters, which work best when\n",
    "# covariates are normalized to the unit cube and outcomes are standardized (zero\n",
    "# mean, unit variance).\n",
    "\n",
    "# These models all work in batch mode (each batch having its own hyperparameters).\n",
    "# When the training observations include multiple outputs, these models use\n",
    "# batching to model outputs independently.\n",
    "\n",
    "# These models all support multiple outputs. However, as single-task models,\n",
    "# `SingleTaskGP`, `FixedNoiseGP`, and `HeteroskedasticSingleTaskGP` should be\n",
    "# used only when the outputs are independent and all use the same training data.\n",
    "# If outputs are independent and outputs have different training data, use the\n",
    "# `ModelListGP`. When modeling correlations between outputs, use a multi-task\n",
    "# model like `MultiTaskGP`.\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "\n",
    "# from typing import Any, List, Optional, Union\n",
    "\n",
    "# import torch\n",
    "# from botorch import settings\n",
    "# from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel\n",
    "# from botorch.models.transforms.input import InputTransform\n",
    "# from botorch.models.transforms.outcome import Log, OutcomeTransform\n",
    "# from botorch.models.utils import fantasize as fantasize_flag, validate_input_scaling\n",
    "# from botorch.sampling.samplers import MCSampler\n",
    "# from gpytorch.constraints.constraints import GreaterThan\n",
    "# from gpytorch.distributions.multivariate_normal import MultivariateNormal\n",
    "# from gpytorch.kernels.matern_kernel import MaternKernel\n",
    "# from gpytorch.kernels.scale_kernel import ScaleKernel\n",
    "# from gpytorch.kernels import RBFKernel\n",
    "# from gpytorch.likelihoods.gaussian_likelihood import (\n",
    "#     _GaussianLikelihoodBase,\n",
    "#     FixedNoiseGaussianLikelihood,\n",
    "#     GaussianLikelihood,\n",
    "# )\n",
    "# from gpytorch.likelihoods.likelihood import Likelihood\n",
    "# from gpytorch.likelihoods.noise_models import HeteroskedasticNoise\n",
    "# from gpytorch.means.constant_mean import ConstantMean\n",
    "# from gpytorch.means.mean import Mean\n",
    "# from gpytorch.mlls.noise_model_added_loss_term import NoiseModelAddedLossTerm\n",
    "# from gpytorch.models.exact_gp import ExactGP\n",
    "# from gpytorch.module import Module\n",
    "# from gpytorch.priors.smoothed_box_prior import SmoothedBoxPrior\n",
    "# from gpytorch.priors.torch_priors import GammaPrior\n",
    "# from torch import Tensor\n",
    "\n",
    "\n",
    "# MIN_INFERRED_NOISE_LEVEL = 1e-4\n",
    "\n",
    "\n",
    "# class SingleTaskGP(BatchedMultiOutputGPyTorchModel, ExactGP):\n",
    "#     r\"\"\"A single-task exact GP model.\n",
    "\n",
    "#     A single-task exact GP using relatively strong priors on the Kernel\n",
    "#     hyperparameters, which work best when covariates are normalized to the unit\n",
    "#     cube and outcomes are standardized (zero mean, unit variance).\n",
    "\n",
    "#     This model works in batch mode (each batch having its own hyperparameters).\n",
    "#     When the training observations include multiple outputs, this model will use\n",
    "#     batching to model outputs independently.\n",
    "\n",
    "#     Use this model when you have independent output(s) and all outputs use the\n",
    "#     same training data. If outputs are independent and outputs have different\n",
    "#     training data, use the ModelListGP. When modeling correlations between\n",
    "#     outputs, use the MultiTaskGP.\n",
    "\n",
    "#     Example:\n",
    "#         >>> train_X = torch.rand(20, 2)\n",
    "#         >>> train_Y = torch.sin(train_X).sum(dim=1, keepdim=True)\n",
    "#         >>> model = SingleTaskGP(train_X, train_Y)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         train_X: Tensor,\n",
    "#         train_Y: Tensor,\n",
    "#         likelihood: Optional[Likelihood] = None,\n",
    "#         covar_module: Optional[Module] = None,\n",
    "#         mean_module: Optional[Mean] = None,\n",
    "#         outcome_transform: Optional[OutcomeTransform] = None,\n",
    "#         input_transform: Optional[InputTransform] = None,\n",
    "#     ) -> None:\n",
    "#         r\"\"\"\n",
    "#         Args:\n",
    "#             train_X: A `batch_shape x n x d` tensor of training features.\n",
    "#             train_Y: A `batch_shape x n x m` tensor of training observations.\n",
    "#             likelihood: A likelihood. If omitted, use a standard\n",
    "#                 GaussianLikelihood with inferred noise level.\n",
    "#             covar_module: The module computing the covariance (Kernel) matrix.\n",
    "#                 If omitted, use a `MaternKernel`.\n",
    "#             mean_module: The mean function to be used. If omitted, use a\n",
    "#                 `ConstantMean`.\n",
    "#             outcome_transform: An outcome transform that is applied to the\n",
    "#                 training data during instantiation and to the posterior during\n",
    "#                 inference (that is, the `Posterior` obtained by calling\n",
    "#                 `.posterior` on the model will be on the original scale).\n",
    "#             input_transform: An input transform that is applied in the model's\n",
    "#                 forward pass.\n",
    "#         \"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             transformed_X = self.transform_inputs(\n",
    "#                 X=train_X, input_transform=input_transform\n",
    "#             )\n",
    "#         if outcome_transform is not None:\n",
    "#             train_Y, _ = outcome_transform(train_Y)\n",
    "#         self._validate_tensor_args(X=transformed_X, Y=train_Y)\n",
    "#         ignore_X_dims = getattr(self, \"_ignore_X_dims_scaling_check\", None)\n",
    "#         validate_input_scaling(\n",
    "#             train_X=transformed_X, train_Y=train_Y, ignore_X_dims=ignore_X_dims\n",
    "#         )\n",
    "#         self._set_dimensions(train_X=train_X, train_Y=train_Y)\n",
    "#         train_X, train_Y, _ = self._transform_tensor_args(X=train_X, Y=train_Y)\n",
    "#         if likelihood is None:\n",
    "#             # noise_prior = GammaPrior(1.1, 0.05)\n",
    "#             noise_prior = GammaPrior(concentration=0.5, rate=1)\n",
    "#             noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate\n",
    "#             likelihood = GaussianLikelihood(\n",
    "#                 noise_prior=noise_prior,\n",
    "#                 batch_shape=self._aug_batch_shape,\n",
    "#                 noise_constraint=GreaterThan(\n",
    "#                     MIN_INFERRED_NOISE_LEVEL,\n",
    "#                     transform=None,\n",
    "#                     initial_value=noise_prior_mode,\n",
    "#                 ),\n",
    "#             )\n",
    "#         else:\n",
    "#             self._is_custom_likelihood = True\n",
    "#         ExactGP.__init__(self, train_X, train_Y, likelihood)\n",
    "#         if mean_module is None:\n",
    "#             mean_module = ConstantMean(batch_shape=self._aug_batch_shape)\n",
    "#         self.mean_module = mean_module\n",
    "#         if covar_module is None:\n",
    "#             covar_module = \\\n",
    "#                 ScaleKernel(\n",
    "#                     # MaternKernel(\n",
    "#                     #     nu=2.5,\n",
    "#                     #     ard_num_dims=transformed_X.shape[-1],\n",
    "#                     #     batch_shape=self._aug_batch_shape,\n",
    "#                     #     lengthscale_prior=GammaPrior(3.0, 6.0),),\n",
    "#                     RBFKernel(\n",
    "#                         ard_num_dims=transformed_X.shape[-1],\n",
    "#                         batch_shape=self._aug_batch_shape,\n",
    "#                         lengthscale_prior=GammaPrior(3.0, 6.0),),\n",
    "#                     batch_shape=self._aug_batch_shape,\n",
    "#                     outputscale_prior=GammaPrior(2.0, 0.15),\n",
    "#                 )\n",
    "#             self._subset_batch_dict = {\n",
    "#                 \"likelihood.noise_covar.raw_noise\": -2,\n",
    "#                 \"mean_module.raw_constant\": -1,\n",
    "#                 \"covar_module.raw_outputscale\": -1,\n",
    "#                 \"covar_module.base_kernel.raw_lengthscale\": -3,\n",
    "#             }\n",
    "#         self.covar_module = covar_module\n",
    "#         # TODO: Allow subsetting of other covar modules\n",
    "#         if outcome_transform is not None:\n",
    "#             self.outcome_transform = outcome_transform\n",
    "#         if input_transform is not None:\n",
    "#             self.input_transform = input_transform\n",
    "#         self.to(train_X)\n",
    "\n",
    "#     def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "#         if self.training:\n",
    "#             x = self.transform_inputs(x)\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utSp1gSWsUsS"
   },
   "source": [
    "# HES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BFXZFyBp8vl6"
   },
   "outputs": [],
   "source": [
    "# @title EHIG\n",
    "\n",
    "\n",
    "def sang_sampler(posterior, num_samples=5):\n",
    "    assert num_samples >= 1\n",
    "    assert num_samples % 2 == 1\n",
    "\n",
    "    sample = []\n",
    "    mean = posterior.mean\n",
    "    std = torch.sqrt(posterior.variance)\n",
    "    std_coeff = np.linspace(0, 2, num_samples // 2 + 1)\n",
    "    for s in std_coeff:\n",
    "        if s == 0:\n",
    "            sample.append(mean)\n",
    "        else:\n",
    "            sample.append(mean + s * std)\n",
    "            sample.append(mean - s * std)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def compute_ehig(\n",
    "    xi, horizon, neighbor_size, seed, draw_last_posterior, sampler_name=\"sang\"\n",
    "):\n",
    "    with settings.propagate_grads(state=True):\n",
    "        for h in range(0, horizon + 1):\n",
    "            pvs_x = xi[h - 1] if h > 0 else prev_x\n",
    "            xi[h] = torch.sigmoid(xi[h]) * (neighbor_size * 2) + (pvs_x - neighbor_size)\n",
    "\n",
    "            if torch.any(xi[h] < 0) or torch.any(xi[h] > 1):\n",
    "                xi[h] = torch.clamp(xi[h], min=0, max=1)\n",
    "\n",
    "        ehigs = 0\n",
    "        sample_yi_on_xi_Di = {}\n",
    "        prev_ind = np.ones(horizon) * -1\n",
    "        p_f_on_Di = {}\n",
    "        p_f_on_Di[0] = model\n",
    "\n",
    "        if sampler_name == \"sang\":\n",
    "            sampler = sang_sampler\n",
    "        elif sampler_name == \"sobol\":\n",
    "            sampler = SobolQMCNormalSampler(\n",
    "                num_samples=4, resample=False, collapse_batch_dims=True, seed=seed\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "        p_y0_on_x0_D0 = model.posterior(xi[0])\n",
    "        sample_yi_on_xi_Di[0] = sampler(p_y0_on_x0_D0)\n",
    "\n",
    "        # if horizon >= 3:\n",
    "        #     iter_range = itertools.product(range(5), range(5), range(5))\n",
    "        #     norm_const = 5*5*5\n",
    "        if horizon >= 2:\n",
    "            iter_range = itertools.product(range(5), range(5))\n",
    "            norm_const = 5 * 5\n",
    "        elif horizon == 1:\n",
    "            iter_range = itertools.product(range(5))\n",
    "            norm_const = 5\n",
    "        else:\n",
    "            print(\"horizon has to be larger than 0\")\n",
    "            raise\n",
    "\n",
    "        for ind in iter_range:\n",
    "            ind = ind + tuple([2] * (horizon - len(ind)))\n",
    "            equal_bool = np.equal(ind, prev_ind)\n",
    "            # finding the first false\n",
    "            comp_ind = np.argmin(equal_bool)\n",
    "            prev_ind = ind\n",
    "\n",
    "            for i in range(horizon):\n",
    "                if i >= comp_ind:\n",
    "                    p_f_on_Di[i + 1] = p_f_on_Di[i].condition_on_observations(\n",
    "                        xi[i],\n",
    "                        # xi[ind[:i]],\n",
    "                        sample_yi_on_xi_Di[i][ind[i]],\n",
    "                    )\n",
    "\n",
    "                    sample_yi_on_xi_Di[i + 1] = sampler(\n",
    "                        p_f_on_Di[i + 1].posterior(xi[i + 1])\n",
    "                    )\n",
    "                    # p_f_on_Di[i+1].posterior(xi[ind[:i+1]]))\n",
    "\n",
    "            if sampler_name == \"sang\":\n",
    "                ehig = sample_yi_on_xi_Di[horizon][0].mean()\n",
    "            elif sampler_name == \"sobol\":\n",
    "                ehig = sample_yi_on_xi_Di[horizon].mean()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            ehigs = ehigs + ehig\n",
    "\n",
    "        # compute distance\n",
    "        # expand_xi = []\n",
    "        # for h in range(horizon+1):\n",
    "        #     total_dim_left = xi[horizon].shape[\n",
    "        #         0:len(xi[horizon].shape)-len(xi[h].shape)]\n",
    "        #     total_dim_left = total_dim_left[::-1]\n",
    "\n",
    "        #     exp_xi = xi[h] # copy.deepcopy(xi[h])\n",
    "        #     for dim_left in total_dim_left:\n",
    "        #         exp_xi = torch.repeat_interleave(\n",
    "        #             exp_xi[None,...], dim_left, dim=0)\n",
    "        #     expand_xi.append(exp_xi)\n",
    "\n",
    "        # total_dist = 0\n",
    "        # for h in range(1, horizon+1):\n",
    "        #     dist = ((expand_xi[h] - expand_xi[h-1])**2).sum(-1).sqrt()\n",
    "        #     total_dist = total_dist + dist\n",
    "\n",
    "        # cost = torch.ones(total_dist.shape)*100\n",
    "        # cost[total_dist < neighbor_size] = 0\n",
    "\n",
    "        # ehig = ehig + cost\n",
    "\n",
    "        ehig = ehigs / norm_const\n",
    "\n",
    "        if draw_last_posterior:\n",
    "            draw_posterior(model=p_f_on_Di[horizon], n_dim=len(xi[0].shape))\n",
    "\n",
    "        return xi, ehig.squeeze()\n",
    "\n",
    "\n",
    "def argmax_ehig(\n",
    "    model,\n",
    "    prev_x,\n",
    "    seed,\n",
    "    horizon=10,\n",
    "    ehig_opt_epoch=100,\n",
    "    ehig_opt_lr=0.1,\n",
    "    neighbor_size=0.1,\n",
    "    use_lr_schedule=False,\n",
    "):\n",
    "    xi = []\n",
    "    for h in range(horizon + 1):\n",
    "        xi.append((torch.rand(dim_xi)).requires_grad_(True))\n",
    "\n",
    "        if h == 0:\n",
    "            dim_xi.insert(0, 5)\n",
    "        elif h == 1:\n",
    "            dim_xi.insert(0, 5)\n",
    "        else:\n",
    "            dim_xi.insert(0, 1)\n",
    "\n",
    "    optimizer = optim.Adam(xi, lr=ehig_opt_lr)\n",
    "    if use_lr_schedule:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=10, eta_min=0.0001\n",
    "        )\n",
    "\n",
    "    result = []\n",
    "    metric = []\n",
    "    for epoch in tqdm(range(ehig_opt_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        xi_in = [element_xi.clone() for element_xi in xi]\n",
    "\n",
    "        if epoch == ehig_opt_epoch - 1:\n",
    "            draw_last_posterior = True\n",
    "        else:\n",
    "            draw_last_posterior = False\n",
    "        xi_out, ehig = compute_ehig(\n",
    "            xi_in,\n",
    "            horizon=horizon,\n",
    "            neighbor_size=neighbor_size,\n",
    "            seed=seed,\n",
    "            draw_last_posterior=draw_last_posterior,\n",
    "        )\n",
    "\n",
    "        ehig.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if use_lr_schedule:\n",
    "            scheduler.step()\n",
    "\n",
    "        result.append([xi_out[i].detach() for i in range(horizon + 1)])\n",
    "        metric.append(ehig.clone().detach().numpy())\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                \"x0: \",\n",
    "                result[epoch][0].data.tolist(),\n",
    "                \", a: \",\n",
    "                result[epoch][-1].tolist(),\n",
    "                # 'xi', xi_out,\n",
    "                \", loss: \",\n",
    "                metric[epoch].item(),\n",
    "            )\n",
    "\n",
    "        del xi_in\n",
    "        del xi_out\n",
    "        del ehig\n",
    "\n",
    "    metric = np.array(metric)\n",
    "    metric_round = metric.round(decimals=3)\n",
    "\n",
    "    best = np.random.choice(np.where(metric_round == metric_round.min())[0])\n",
    "\n",
    "    # best = torch.argmin(metric, dim=0).item()\n",
    "    best_result = result[best]\n",
    "\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AHhTRtKuQzv"
   },
   "source": [
    "#BudgetBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1665960361332,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "_ZtXzyS7v-bs"
   },
   "outputs": [],
   "source": [
    "# Objective and cost functions\n",
    "\n",
    "from botorch.test_functions.synthetic import Ackley\n",
    "\n",
    "\n",
    "def get_objective_cost_function(seed: int) -> Callable:\n",
    "    def objective_function(X: Tensor) -> Tensor:\n",
    "        X_unnorm = (2.0 * X) - 1.0\n",
    "        ackley = Ackley(dim=1)\n",
    "        objective_X = -ackley.evaluate_true(X_unnorm)\n",
    "\n",
    "        # objective_X = func(X)\n",
    "        return objective_X\n",
    "\n",
    "    def cost_function(X: Tensor, prev_X: Tensor) -> Tensor:\n",
    "        # X_unnorm = (2.0 * X) - 1.0\n",
    "        # a, b, c = get_cost_function_parameters(seed=seed % 20)\n",
    "        # ln_cost_X = a * torch.cos(b * (2 * pi) * (X_unnorm + c)).mean(dim=-1)\n",
    "        # cost_X = torch.exp(ln_cost_X)\n",
    "\n",
    "        cost_X = torch.abs(X - prev_X).sum(-1)\n",
    "        return cost_X\n",
    "\n",
    "    return [objective_function, cost_function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1665960302842,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "XF4P6p0nuPS2"
   },
   "outputs": [],
   "source": [
    "# @title budgeted_bo_trial.py\n",
    "from locale import Error\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from torch import Tensor\n",
    "\n",
    "# from budgeted_bo.acquisition_functions.budgeted_multi_step_ei import BudgetedMultiStepExpectedImprovement\n",
    "# from budgeted_bo.acquisition_functions.ei_puc import ExpectedImprovementPerUnitOfCost\n",
    "# from budgeted_bo.utils import (\n",
    "#     evaluate_obj_and_cost_at_X,\n",
    "#     fit_model,\n",
    "#     generate_initial_design,\n",
    "#     get_suggested_budget,\n",
    "#     optimize_acqf_and_get_suggested_point\n",
    "# )\n",
    "\n",
    "\n",
    "def budgeted_bo_trial(\n",
    "    problem: str,\n",
    "    algo: str,\n",
    "    algo_params: Optional[Dict],\n",
    "    trial: int,\n",
    "    restart: bool,\n",
    "    objective_function: Optional[Callable],\n",
    "    cost_function: Optional[Callable],\n",
    "    objective_cost_function: Optional[Callable],\n",
    "    input_dim: int,\n",
    "    n_init_evals: int,\n",
    "    budget: float,\n",
    "    n_max_iter: int = 200,\n",
    "    ignore_failures: bool = False,\n",
    "):\n",
    "    # Make sure that objective and cost functions are passed\n",
    "    if (objective_cost_function is None) and (\n",
    "        objective_function is None or cost_function is None\n",
    "    ):\n",
    "        raise RuntimeError(\n",
    "            \"Both the objective and cost functions must be passed as inputs.\"\n",
    "        )\n",
    "\n",
    "    # Modify algo's name to account for hyperparameters\n",
    "    if algo == \"B-MS-EI\":\n",
    "        algo_id = algo + \"_\"\n",
    "\n",
    "        for n in algo_params.get(\"lookahead_n_fantasies\"):\n",
    "            algo_id += str(n)\n",
    "\n",
    "        algo_id += \"_\"\n",
    "\n",
    "        if algo_params.get(\"refill_until_lower_bound_is_reached\"):\n",
    "            algo_id += \"1\"\n",
    "        else:\n",
    "            algo_id += \"0\"\n",
    "\n",
    "        algo_id += \"_\" + str(int(budget))\n",
    "    elif algo == \"EI-PUC-CC\":\n",
    "        algo_id = algo + \"_\" + str(int(budget))\n",
    "    else:\n",
    "        algo_id = algo\n",
    "\n",
    "    # Get script directory\n",
    "    script_dir = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "    project_path = script_dir[:-11]\n",
    "    results_folder = (\n",
    "        project_path + \"/experiments/results/\" + problem + \"/\" + algo_id + \"/\"\n",
    "    )\n",
    "\n",
    "    if restart:\n",
    "        # Check if training data is already available\n",
    "        try:\n",
    "            # Current available evaluations\n",
    "            X = torch.tensor(np.loadtxt(results_folder + \"X/X_\" + str(trial) + \".txt\"))\n",
    "            objective_X = torch.tensor(\n",
    "                np.loadtxt(\n",
    "                    results_folder + \"objective_X/objective_X_\" + str(trial) + \".txt\"\n",
    "                )\n",
    "            )\n",
    "            cost_X = torch.tensor(\n",
    "                np.loadtxt(results_folder + \"cost_X/cost_X_\" + str(trial) + \".txt\")\n",
    "            )\n",
    "\n",
    "            # Historical best observed objective values and running times\n",
    "            hist_best_obs_vals = list(\n",
    "                np.loadtxt(results_folder + \"best_obs_vals_\" + str(trial) + \".txt\")\n",
    "            )\n",
    "            running_times = list(\n",
    "                np.loadtxt(\n",
    "                    results_folder\n",
    "                    + \"running_times/running_times_\"\n",
    "                    + str(trial)\n",
    "                    + \".txt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Current best observed objective value\n",
    "            best_obs_val = torch.tensor(hist_best_obs_vals[-1])\n",
    "\n",
    "            iteration = len(hist_best_obs_vals) - 1\n",
    "            print(\"Restarting experiment from available data.\")\n",
    "\n",
    "        except:\n",
    "            # Initial evaluations\n",
    "            X = generate_initial_design(\n",
    "                num_samples=n_init_evals, input_dim=input_dim, seed=trial\n",
    "            )\n",
    "            objective_X, cost_X = evaluate_obj_and_cost_at_X(\n",
    "                X=X,\n",
    "                prev_X=prev_X,\n",
    "                objective_function=objective_function,\n",
    "                cost_function=cost_function,\n",
    "                objective_cost_function=objective_cost_function,\n",
    "            )\n",
    "\n",
    "            # Current best objective value\n",
    "            best_obs_val = objective_X.max().item()\n",
    "\n",
    "            # Historical best observed objective values and running times\n",
    "            hist_best_obs_vals = [best_obs_val]\n",
    "            running_times = []\n",
    "\n",
    "            iteration = 0\n",
    "    else:\n",
    "        # Initial evaluations\n",
    "        X = generate_initial_design(\n",
    "            num_samples=n_init_evals, input_dim=input_dim, seed=trial\n",
    "        )\n",
    "\n",
    "        objective_X, cost_X = evaluate_obj_and_cost_at_X(\n",
    "            X=X,\n",
    "            prev_X=torch.cat([X[0][None, :] + 0.01, X[:-1]]),\n",
    "            objective_function=objective_function,\n",
    "            cost_function=cost_function,\n",
    "            objective_cost_function=objective_cost_function,\n",
    "        )\n",
    "\n",
    "        # Current best objective value\n",
    "        best_obs_val = objective_X.max().item()\n",
    "\n",
    "        # Historical best observed objective values and running times\n",
    "        hist_best_obs_vals = [best_obs_val]\n",
    "        running_times = []\n",
    "\n",
    "        iteration = 0\n",
    "\n",
    "    cumulative_cost = cost_X.sum().item()\n",
    "    budget_plus_init_cost = cost_X[:n_init_evals].sum().item() + budget\n",
    "\n",
    "    algo_params[\"init_budget\"] = budget\n",
    "\n",
    "    while cumulative_cost <= budget_plus_init_cost and iteration <= n_max_iter:\n",
    "        iteration += 1\n",
    "        print(\"Problem: \" + problem)\n",
    "        print(\"Sampling policy: \" + algo_id)\n",
    "        print(\"Trial: \" + str(trial))\n",
    "        print(\"Iteration: \" + str(iteration))\n",
    "\n",
    "        # New suggested point\n",
    "        t0 = time.time()\n",
    "\n",
    "        try:\n",
    "            new_x = get_new_suggested_point(\n",
    "                algo=algo,\n",
    "                X=X,\n",
    "                objective_X=objective_X,\n",
    "                cost_X=cost_X,\n",
    "                budget_left=budget_plus_init_cost - cumulative_cost,\n",
    "                algo_params=algo_params,\n",
    "            )\n",
    "        except:\n",
    "            if ignore_failures:\n",
    "                print(\n",
    "                    \"An error ocurred when computing the next point to evaluate. Instead, a point will be chosen uniformly at random.\"\n",
    "                )\n",
    "                new_x = get_new_suggested_point(\n",
    "                    algo=\"Random\",\n",
    "                    X=X,\n",
    "                    objective_X=objective_X,\n",
    "                    cost_X=cost_X,\n",
    "                    budget_left=budget_plus_init_cost - cumulative_cost,\n",
    "                    algo_params=algo_params,\n",
    "                )\n",
    "            else:\n",
    "                raise Error(\n",
    "                    \"An error ocurred when computing the next point to evaluate.\"\n",
    "                )\n",
    "\n",
    "        t1 = time.time()\n",
    "        running_times.append(t1 - t0)\n",
    "\n",
    "        # Evaluate objective at new point\n",
    "        objective_new_x, cost_new_x = evaluate_obj_and_cost_at_X(\n",
    "            X=new_x,\n",
    "            prev_X=X[-1],\n",
    "            objective_function=objective_function,\n",
    "            cost_function=cost_function,\n",
    "            objective_cost_function=objective_cost_function,\n",
    "        )\n",
    "\n",
    "        # Update training data\n",
    "        X = torch.cat([X, new_x], 0)\n",
    "        objective_X = torch.cat([objective_X, objective_new_x], 0)\n",
    "        cost_X = torch.cat([cost_X, cost_new_x], 0)\n",
    "\n",
    "        # Update historical best observed objective values and cumulative cost\n",
    "        cumulative_cost = cost_X.sum().item()\n",
    "        best_obs_val = objective_X.max().item()\n",
    "        hist_best_obs_vals.append(best_obs_val)\n",
    "        print(\"Best value found so far: \" + str(best_obs_val))\n",
    "        print(\"Remaining budget: \" + str(budget_plus_init_cost - cumulative_cost))\n",
    "\n",
    "        # Save data\n",
    "        if not os.path.exists(results_folder):\n",
    "            os.makedirs(results_folder)\n",
    "        if not os.path.exists(results_folder + \"running_times/\"):\n",
    "            os.makedirs(results_folder + \"running_times/\")\n",
    "        if not os.path.exists(results_folder + \"X/\"):\n",
    "            os.makedirs(results_folder + \"X/\")\n",
    "        if not os.path.exists(results_folder + \"objective_X/\"):\n",
    "            os.makedirs(results_folder + \"objective_X/\")\n",
    "        if not os.path.exists(results_folder + \"cost_X/\"):\n",
    "            os.makedirs(results_folder + \"cost_X/\")\n",
    "        np.savetxt(results_folder + \"X/X_\" + str(trial) + \".txt\", X.numpy())\n",
    "        np.savetxt(\n",
    "            results_folder + \"objective_X/objective_X_\" + str(trial) + \".txt\",\n",
    "            objective_X.numpy(),\n",
    "        )\n",
    "        np.savetxt(\n",
    "            results_folder + \"cost_X/cost_X_\" + str(trial) + \".txt\", cost_X.numpy()\n",
    "        )\n",
    "        np.savetxt(\n",
    "            results_folder + \"best_obs_vals_\" + str(trial) + \".txt\",\n",
    "            np.atleast_1d(hist_best_obs_vals),\n",
    "        )\n",
    "        np.savetxt(\n",
    "            results_folder + \"running_times/running_times_\" + str(trial) + \".txt\",\n",
    "            np.atleast_1d(running_times),\n",
    "        )\n",
    "\n",
    "    return X, objective_X\n",
    "\n",
    "\n",
    "def get_new_suggested_point(\n",
    "    algo: str,\n",
    "    X: Tensor,\n",
    "    objective_X: Tensor,\n",
    "    cost_X: Tensor,\n",
    "    budget_left: float,\n",
    "    algo_params: Optional[Dict] = None,\n",
    ") -> Tensor:\n",
    "    input_dim = X.shape[-1]\n",
    "    algo_params[\"budget_left\"] = budget_left\n",
    "\n",
    "    if algo == \"Random\":\n",
    "        return torch.rand([1, input_dim])\n",
    "    elif algo == \"B-MS-EI\":\n",
    "        # Model\n",
    "        model = fit_model(\n",
    "            X=X,\n",
    "            objective_X=objective_X,\n",
    "            cost_X=cost_X,\n",
    "            training_mode=\"objective_and_cost\",\n",
    "            noiseless_obs=True,\n",
    "        )\n",
    "\n",
    "        # Acquisition function\n",
    "        suggested_budget, lower_bound = get_suggested_budget(\n",
    "            strategy=\"fantasy_costs_from_aux_policy\",\n",
    "            refill_until_lower_bound_is_reached=algo_params.get(\n",
    "                \"refill_until_lower_bound_is_reached\"\n",
    "            ),\n",
    "            budget_left=budget_left,\n",
    "            model=model,\n",
    "            n_lookahead_steps=len(algo_params.get(\"lookahead_n_fantasies\")) + 1,\n",
    "            X=X,\n",
    "            objective_X=objective_X,\n",
    "            cost_X=cost_X,\n",
    "            init_budget=algo_params.get(\"init_budget\"),\n",
    "            previous_budget=algo_params.get(\"current_budget\"),\n",
    "            lower_bound=algo_params.get(\"lower_bound\"),\n",
    "        )\n",
    "\n",
    "        algo_params[\"current_budget\"] = suggested_budget\n",
    "        algo_params[\"current_budget_plus_cumulative_cost\"] = (\n",
    "            suggested_budget + cost_X.sum().item()\n",
    "        )\n",
    "        algo_params[\"lower_bound\"] = lower_bound\n",
    "\n",
    "        acquisition_function = BudgetedMultiStepExpectedImprovement(\n",
    "            model=model,\n",
    "            budget_plus_cumulative_cost=algo_params.get(\n",
    "                \"current_budget_plus_cumulative_cost\"\n",
    "            ),\n",
    "            batch_size=1,\n",
    "            lookahead_batch_sizes=[1 for _ in algo_params.get(\"lookahead_n_fantasies\")],\n",
    "            num_fantasies=algo_params.get(\"lookahead_n_fantasies\"),\n",
    "        )\n",
    "\n",
    "    elif algo == \"EI\":\n",
    "        # Model\n",
    "        model = fit_model(\n",
    "            X=X,\n",
    "            objective_X=objective_X,\n",
    "            cost_X=cost_X,\n",
    "            training_mode=\"objective\",\n",
    "            noiseless_obs=True,\n",
    "        )\n",
    "\n",
    "        acquisition_function = ExpectedImprovement(\n",
    "            model=model, best_f=objective_X.max().item()\n",
    "        )\n",
    "\n",
    "    elif algo == \"EI-PUC\":\n",
    "        # Model\n",
    "        model = fit_model(\n",
    "            X=X,\n",
    "            objective_X=objective_X,\n",
    "            cost_X=cost_X,\n",
    "            training_mode=\"objective_and_cost\",\n",
    "            noiseless_obs=True,\n",
    "        )\n",
    "\n",
    "        # Acquisition function\n",
    "        acquisition_function = ExpectedImprovementPerUnitOfCost(\n",
    "            model=model,\n",
    "            best_f=objective_X.max().item(),\n",
    "        )\n",
    "\n",
    "    elif algo == \"EI-PUC-CC\":\n",
    "        # Model\n",
    "        model = fit_model(\n",
    "            X=X,\n",
    "            objective_X=objective_X,\n",
    "            cost_X=cost_X,\n",
    "            training_mode=\"objective_and_cost\",\n",
    "            noiseless_obs=True,\n",
    "        )\n",
    "\n",
    "        # Acquisition function\n",
    "        acquisition_function = ExpectedImprovementPerUnitOfCost(\n",
    "            model=model,\n",
    "            best_f=objective_X.max().item(),\n",
    "            cost_exponent=budget_left / algo_params.get(\"init_budget\"),\n",
    "        )\n",
    "\n",
    "    standard_bounds = torch.tensor([[0.0] * input_dim, [1.0] * input_dim])\n",
    "\n",
    "    new_x = optimize_acqf_and_get_suggested_point(\n",
    "        acq_func=acquisition_function,\n",
    "        bounds=standard_bounds,\n",
    "        batch_size=1,\n",
    "        algo_params=algo_params,\n",
    "    )\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1665960575520,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "1hmNkiV8ujwx"
   },
   "outputs": [],
   "source": [
    "# @title experiment_manager.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from budgeted_bo.budgeted_bo_trial import budgeted_bo_trial\n",
    "\n",
    "\n",
    "def experiment_manager(\n",
    "    problem: str,\n",
    "    algo: str,\n",
    "    algo_params: Dict,\n",
    "    restart: bool,\n",
    "    first_trial: int,\n",
    "    last_trial: int,\n",
    "    get_objective_cost_function: Callable,\n",
    "    input_dim: int,\n",
    "    n_init_evals: int,\n",
    "    budget: float,\n",
    "    n_max_iter: int = 200,\n",
    "    ignore_failures: bool = False,\n",
    "):\n",
    "    Xs = []\n",
    "    objective_Xs = []\n",
    "    for trial in tqdm(range(first_trial, last_trial + 1)):\n",
    "        get_objective_cost_function_output = get_objective_cost_function(seed=trial)\n",
    "\n",
    "        if len(get_objective_cost_function_output) == 1:\n",
    "            objective_cost_function = get_objective_cost_function_output[0]\n",
    "            objective_function = None\n",
    "            cost_function = None\n",
    "        elif len(get_objective_cost_function_output) == 2:\n",
    "            objective_cost_function = None\n",
    "            objective_function = get_objective_cost_function_output[0]\n",
    "            cost_function = get_objective_cost_function_output[1]\n",
    "\n",
    "        X, objective_X = budgeted_bo_trial(\n",
    "            problem=problem,\n",
    "            algo=algo,\n",
    "            algo_params=algo_params,\n",
    "            trial=trial,\n",
    "            restart=restart,\n",
    "            objective_function=objective_function,\n",
    "            cost_function=cost_function,\n",
    "            objective_cost_function=objective_cost_function,\n",
    "            input_dim=input_dim,\n",
    "            n_init_evals=n_init_evals,\n",
    "            budget=budget,\n",
    "            n_max_iter=n_max_iter,\n",
    "            ignore_failures=ignore_failures,\n",
    "        )\n",
    "\n",
    "    return Xs, objective_Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1665960302843,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "VIniKVgqvLsw"
   },
   "outputs": [],
   "source": [
    "# @title Util.py\n",
    "\n",
    "import logging\n",
    "from copy import copy, deepcopy\n",
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.acquisition.multi_step_lookahead import (\n",
    "    qMultiStepLookahead,\n",
    "    warmstart_multistep,\n",
    ")\n",
    "from botorch.acquisition.objective import ScalarizedObjective\n",
    "from botorch.generation.gen import get_best_candidates\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.model import Model\n",
    "from botorch.models.transforms.outcome import (\n",
    "    ChainedOutcomeTransform,\n",
    "    Log,\n",
    "    Standardize,\n",
    ")\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.sampling.samplers import IIDNormalSampler\n",
    "from botorch.utils.transforms import normalize\n",
    "from gpytorch.constraints.constraints import Interval\n",
    "from gpytorch.likelihoods.gaussian_likelihood import GaussianLikelihood\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "\n",
    "# from budgeted_bo.acquisition_functions.ei_puc import ExpectedImprovementPerUnitOfCost\n",
    "# from budgeted_bo.acquisition_function_optimization.custom_warmstart_multistep import custom_warmstart_multistep\n",
    "\n",
    "\n",
    "def evaluate_obj_and_cost_at_X(\n",
    "    X: Tensor,\n",
    "    prev_X: Tensor,\n",
    "    objective_function: Optional[Callable],\n",
    "    cost_function: Optional[Callable],\n",
    "    objective_cost_function: Optional[Callable],\n",
    ") -> Tensor:\n",
    "    if (objective_cost_function is None) and (\n",
    "        objective_function is None or cost_function is None\n",
    "    ):\n",
    "        raise RuntimeError(\n",
    "            \"Both the objective and cost functions must be passed as inputs.\"\n",
    "        )\n",
    "\n",
    "    if objective_cost_function is not None:\n",
    "        objective_X, cost_X = objective_cost_function(X)\n",
    "    else:\n",
    "        objective_X = objective_function(X)\n",
    "        cost_X = cost_function(X, prev_X)\n",
    "\n",
    "    return objective_X, cost_X\n",
    "\n",
    "\n",
    "def fantasize_costs(\n",
    "    algo: str,\n",
    "    model: Model,\n",
    "    n_steps: int,\n",
    "    budget_left: float,\n",
    "    init_budget: float,\n",
    "    input_dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fantasizes the observed costs when following a specified sampling\n",
    "    policy for a given number of steps.\n",
    "    \"\"\"\n",
    "    standard_bounds = torch.tensor([[0.0] * input_dim, [1.0] * input_dim])\n",
    "\n",
    "    fantasy_costs = []\n",
    "    fantasy_optimizers = []\n",
    "\n",
    "    if algo == \"EI-PUC_CC\":\n",
    "        for _ in range(n_steps):\n",
    "            # Acquisition function\n",
    "            y = torch.transpose(model.train_targets, -2, -1)\n",
    "            y_original_scale = model.outcome_transform.untransform(y)[0]\n",
    "            obj_vals = y_original_scale[..., 0]\n",
    "            best_f = torch.max(obj_vals).item()\n",
    "            cost_exponent = budget_left / init_budget\n",
    "\n",
    "            aux_acq_func = ExpectedImprovementPerUnitOfCost(\n",
    "                model=model,\n",
    "                best_f=best_f,\n",
    "                cost_exponent=cost_exponent,\n",
    "            )\n",
    "\n",
    "            # Get new point\n",
    "            new_x, acq_value = optimize_acqf(\n",
    "                acq_function=aux_acq_func,\n",
    "                bounds=standard_bounds,\n",
    "                q=1,\n",
    "                num_restarts=5 * input_dim,\n",
    "                raw_samples=50 * input_dim,\n",
    "                options={\n",
    "                    \"batch_limit\": 5,\n",
    "                    \"maxiter\": 100,\n",
    "                    \"nonnegative\": True,\n",
    "                    \"method\": \"L-BFGS-B\",\n",
    "                },\n",
    "                return_best_only=True,\n",
    "            )\n",
    "\n",
    "            fantasy_optimizers.append(new_x.clone())\n",
    "\n",
    "            # Fantasize objective and cost values\n",
    "            sampler = IIDNormalSampler(\n",
    "                num_samples=1, resample=True, collapse_batch_dims=True\n",
    "            )\n",
    "            posterior_new_x = model.posterior(new_x, observation_noise=True)\n",
    "            fantasy_obs = sampler(posterior_new_x).squeeze(dim=0).detach()\n",
    "            fantasy_costs.append(torch.exp(fantasy_obs[0, 1]).item())\n",
    "            model = model.condition_on_observations(X=new_x, Y=fantasy_obs)\n",
    "\n",
    "            # Update remaining budget\n",
    "            budget_left -= fantasy_costs[-1]\n",
    "\n",
    "    print(\"Fantasy costs:\")\n",
    "    fantasy_costs = torch.tensor(fantasy_costs)\n",
    "    print(fantasy_costs)\n",
    "    return fantasy_costs, fantasy_optimizers\n",
    "\n",
    "\n",
    "def fit_model(\n",
    "    X: Tensor,\n",
    "    objective_X: Tensor,\n",
    "    cost_X: Tensor,\n",
    "    training_mode: str,\n",
    "    noiseless_obs: bool = False,\n",
    "):\n",
    "    if training_mode == \"objective\":\n",
    "        Y = objective_X\n",
    "    elif training_mode == \"cost\":\n",
    "        Y = cost_X\n",
    "    elif training_mode == \"objective_and_cost\":\n",
    "        Y = torch.cat(\n",
    "            (objective_X.unsqueeze(dim=-1), torch.log(cost_X).unsqueeze(dim=-1)), dim=-1\n",
    "        )\n",
    "\n",
    "    if Y.ndim == 1:\n",
    "        Y = Y.unsqueeze(-1)\n",
    "\n",
    "    # Outcome transform\n",
    "    standardize = Standardize(m=Y.shape[-1], batch_shape=Y.shape[:-2])\n",
    "    if training_mode == \"cost\":\n",
    "        log = Log()\n",
    "        outcome_transform = ChainedOutcomeTransform(log=log, standardize=standardize)\n",
    "    else:\n",
    "        outcome_transform = standardize\n",
    "\n",
    "    # Likelihood\n",
    "    if noiseless_obs:\n",
    "        _, aug_batch_shape = SingleTaskGP.get_batch_dimensions(\n",
    "            train_X=X,\n",
    "            train_Y=Y,\n",
    "        )\n",
    "        likelihood = GaussianLikelihood(\n",
    "            batch_shape=aug_batch_shape,\n",
    "            noise_constraint=Interval(lower_bound=1e-4, upper_bound=1e-3),\n",
    "        )\n",
    "    else:\n",
    "        likelihood = None\n",
    "\n",
    "    # Define model\n",
    "    model = SingleTaskGP(\n",
    "        train_X=X,\n",
    "        train_Y=Y,\n",
    "        likelihood=likelihood,\n",
    "        outcome_transform=outcome_transform,\n",
    "    )\n",
    "\n",
    "    model.outcome_transform.eval()\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_initial_design(\n",
    "    num_samples: int, input_dim: int, sobol: bool = True, seed: int = None\n",
    "):\n",
    "    # generate training data\n",
    "    if sobol:\n",
    "        soboleng = torch.quasirandom.SobolEngine(\n",
    "            dimension=input_dim, scramble=True, seed=seed\n",
    "        )\n",
    "        X = soboleng.draw(num_samples).to(dtype=torch.double)\n",
    "    else:\n",
    "        if seed is not None:\n",
    "            old_state = torch.random.get_rng_state()\n",
    "            torch.manual_seed(seed)\n",
    "            X = torch.rand([num_samples, input_dim])\n",
    "            torch.random.set_rng_state(old_state)\n",
    "        else:\n",
    "            X = torch.rand([num_samples, input_dim])\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_suggested_budget(\n",
    "    strategy: str,\n",
    "    refill_until_lower_bound_is_reached: bool,\n",
    "    budget_left: float,\n",
    "    model: Model,\n",
    "    n_lookahead_steps: int,\n",
    "    X: Tensor,\n",
    "    objective_X: Tensor,\n",
    "    cost_X: Tensor,\n",
    "    init_budget: float,\n",
    "    previous_budget: Optional[float] = None,\n",
    "    lower_bound: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the suggested budget to be used by the budgeted multi-step\n",
    "    expected improvement acquisition function.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        refill_until_lower_bound_is_reached\n",
    "        and (lower_bound is not None)\n",
    "        and (previous_budget - cost_X[-1] > lower_bound)\n",
    "    ):\n",
    "        suggested_budget = previous_budget - cost_X[-1].item()\n",
    "        return suggested_budget, lower_bound\n",
    "\n",
    "    if strategy == \"fantasy_costs_from_aux_policy\":\n",
    "        # Fantasize the observed costs following the auxiliary acquisition function\n",
    "        fantasy_costs, fantasy_optimizers = fantasize_costs(\n",
    "            algo=\"EI-PUC_CC\",\n",
    "            model=deepcopy(model),\n",
    "            n_steps=n_lookahead_steps,\n",
    "            budget_left=copy(budget_left),\n",
    "            init_budget=init_budget,\n",
    "            input_dim=X.shape[-1],\n",
    "        )\n",
    "\n",
    "        # Suggested budget is the minimum between the sum of the fantasy costs\n",
    "        # and the true remaining budget.\n",
    "        suggested_budget = fantasy_costs.sum().item()\n",
    "        lower_bound = fantasy_costs.min().item()\n",
    "    suggested_budget = min(suggested_budget, budget_left)\n",
    "    return suggested_budget, lower_bound\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_suggested_point(\n",
    "    acq_func: AcquisitionFunction,\n",
    "    bounds: Tensor,\n",
    "    batch_size: int,\n",
    "    algo_params: Dict,\n",
    ") -> Tensor:\n",
    "    \"\"\"Optimizes the acquisition function, and returns the candidate solution.\"\"\"\n",
    "    is_ms = isinstance(acq_func, qMultiStepLookahead)\n",
    "    input_dim = bounds.shape[1]\n",
    "    q = acq_func.get_augmented_q_batch_size(batch_size) if is_ms else batch_size\n",
    "    raw_samples = 200 * input_dim * batch_size\n",
    "    num_restarts = 10 * input_dim * batch_size\n",
    "\n",
    "    # if is_ms:\n",
    "    # raw_samples *= (len(algo_params.get(\"lookahead_n_fantasies\")) + 1)\n",
    "    # num_restarts *=  (len(algo_params.get(\"lookahead_n_fantasies\")) + 1)\n",
    "\n",
    "    # if algo_params.get(\"suggested_x_full_tree\") is not None:\n",
    "    #     batch_initial_conditions = custom_warmstart_multistep(\n",
    "    #         acq_function=acq_func,\n",
    "    #         bounds=bounds,\n",
    "    #         num_restarts=num_restarts,\n",
    "    #         raw_samples=raw_samples,\n",
    "    #         full_optimizer=algo_params.get(\"suggested_x_full_tree\"),\n",
    "    #         algo_params=algo_params,\n",
    "    #     )\n",
    "    # else:\n",
    "    batch_initial_conditions = None\n",
    "\n",
    "    # try:\n",
    "    candidates, acq_values = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=q,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options={\n",
    "            \"batch_limit\": 2,\n",
    "            \"maxiter\": 200,\n",
    "            \"nonnegative\": True,\n",
    "            \"method\": \"L-BFGS-B\",\n",
    "        },\n",
    "        batch_initial_conditions=batch_initial_conditions,\n",
    "        return_best_only=False,\n",
    "        return_full_tree=is_ms,\n",
    "    )\n",
    "    # except:\n",
    "    #     print(acq_func)\n",
    "    #     print('sang_______________________________ \\n')\n",
    "    #     sang\n",
    "\n",
    "    candidates = candidates.detach()\n",
    "    if is_ms:\n",
    "        # save all tree variables for multi-step initialization\n",
    "        algo_params[\"suggested_x_full_tree\"] = candidates.clone()\n",
    "        candidates = acq_func.extract_candidates(candidates)\n",
    "\n",
    "    acq_values_sorted, indices = torch.sort(acq_values.squeeze(), descending=True)\n",
    "    print(\"Acquisition values:\")\n",
    "    print(acq_values_sorted)\n",
    "    print(\"Candidates:\")\n",
    "    print(candidates[indices].squeeze())\n",
    "    print(candidates.squeeze())\n",
    "\n",
    "    new_x = get_best_candidates(batch_candidates=candidates, batch_values=acq_values)\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy5q11RivXUg"
   },
   "source": [
    "### acquisition_function_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665960303042,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "BFDS7DtavTTT"
   },
   "outputs": [],
   "source": [
    "# @title custom_warmstart_multistep.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from botorch import acquisition\n",
    "from botorch.acquisition.analytic import PosteriorMean\n",
    "from botorch.acquisition.multi_step_lookahead import (\n",
    "    qMultiStepLookahead,\n",
    "    warmstart_multistep,\n",
    ")\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.sampling.samplers import IIDNormalSampler\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "from typing import List, Dict\n",
    "\n",
    "# from budgeted_bo.acquisition_functions.budgeted_ei import BudgetedExpectedImprovement\n",
    "# from budgeted_bo.acquisition_functions.budgeted_multi_step_ei import BudgetedMultiStepExpectedImprovement\n",
    "\n",
    "\n",
    "def custom_warmstart_multistep(\n",
    "    acq_function: qMultiStepLookahead,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    full_optimizer: Tensor,\n",
    "    algo_params: Dict,\n",
    ") -> Tensor:\n",
    "    batch_initial_conditions = warmstart_multistep(\n",
    "        acq_function=acq_function,\n",
    "        bounds=bounds,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        full_optimizer=full_optimizer,\n",
    "    )\n",
    "\n",
    "    n_initial_points = batch_initial_conditions.shape[0]\n",
    "    random_index = random.randrange(n_initial_points)\n",
    "    print(random_index)\n",
    "    input_dim = batch_initial_conditions.shape[-1]\n",
    "    batch_shape, shapes, sizes = acq_function.get_split_shapes(\n",
    "        X=batch_initial_conditions\n",
    "    )\n",
    "\n",
    "    # Safe copy of model\n",
    "    model = deepcopy(acq_function.model)\n",
    "    obj_model = model.subset_output(idcs=[0])\n",
    "\n",
    "    # Define optimization domain\n",
    "    standard_bounds = torch.tensor([[0.0] * input_dim, [1.0] * input_dim])\n",
    "\n",
    "    #\n",
    "    aux_acq_func = PosteriorMean(model=obj_model)\n",
    "\n",
    "    new_x, acq_value = optimize_acqf(\n",
    "        acq_function=aux_acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=1,\n",
    "        num_restarts=5 * input_dim,\n",
    "        raw_samples=100 * input_dim,\n",
    "        options={\n",
    "            \"batch_limit\": 5,\n",
    "            \"maxiter\": 100,\n",
    "            \"method\": \"L-BFGS-B\",\n",
    "        },\n",
    "        return_best_only=True,\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    for _ in range(sizes[0]):\n",
    "        batch_initial_conditions[random_index, i, :] = new_x.clone().squeeze(0)\n",
    "        i += 1\n",
    "\n",
    "    # Fantasize objective and cost values\n",
    "    sampler = IIDNormalSampler(num_samples=1, resample=True, collapse_batch_dims=True)\n",
    "    posterior_new_x = model.posterior(new_x, observation_noise=True)\n",
    "    fantasy_obs = sampler(posterior_new_x).squeeze(dim=0).detach()\n",
    "    fantasy_cost = torch.exp(fantasy_obs[0, 1]).item()\n",
    "    model = model.condition_on_observations(X=new_x, Y=fantasy_obs)\n",
    "\n",
    "    n_lookahead_steps = len(algo_params.get(\"lookahead_n_fantasies\")) - 1\n",
    "\n",
    "    if n_lookahead_steps > 0:\n",
    "        aux_acq_func = BudgetedMultiStepExpectedImprovement(\n",
    "            model=model,\n",
    "            budget_plus_cumulative_cost=algo_params.get(\n",
    "                \"current_budget_plus_cumulative_cost\"\n",
    "            )\n",
    "            - fantasy_cost,\n",
    "            batch_size=1,\n",
    "            lookahead_batch_sizes=[1 for _ in range(n_lookahead_steps)],\n",
    "            num_fantasies=[1 for _ in range(n_lookahead_steps)],\n",
    "        )\n",
    "    else:\n",
    "        y = torch.transpose(model.train_targets, -2, -1)\n",
    "        y_original_scale = model.outcome_transform.untransform(y)[0]\n",
    "        obj_vals = y_original_scale[..., 0]\n",
    "        best_f = torch.max(obj_vals).item()\n",
    "\n",
    "        aux_acq_func = BudgetedExpectedImprovement(\n",
    "            model=model,\n",
    "            best_f=best_f,\n",
    "            budget=algo_params.get(\"current_budget\") - fantasy_cost,\n",
    "        )\n",
    "\n",
    "    new_x, acq_value = optimize_acqf(\n",
    "        acq_function=aux_acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=aux_acq_func.get_augmented_q_batch_size(1) if n_lookahead_steps > 0 else 1,\n",
    "        num_restarts=5 * input_dim,\n",
    "        raw_samples=100 * input_dim,\n",
    "        options={\n",
    "            \"batch_limit\": 1,\n",
    "            \"maxiter\": 100,\n",
    "            \"method\": \"L-BFGS-B\",\n",
    "        },\n",
    "        return_best_only=True,\n",
    "        return_full_tree=True,\n",
    "    )\n",
    "\n",
    "    for j, size in enumerate(sizes[1:]):\n",
    "        for _ in range(size):\n",
    "            batch_initial_conditions[random_index, i, :] = new_x[j, :].clone()\n",
    "            i += 1\n",
    "\n",
    "    return batch_initial_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S8W0QP9vhrZ"
   },
   "source": [
    "### acquisition_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1665960303260,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "QzFa9MxcvmFD"
   },
   "outputs": [],
   "source": [
    "# @title budgeted_ei.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.analytic import AnalyticAcquisitionFunction\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition.objective import MCAcquisitionObjective, ScalarizedObjective\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.samplers import MCSampler\n",
    "from botorch.utils.objective import soft_eval_constraint\n",
    "from botorch.utils.transforms import concatenate_pending_points, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class BudgetedExpectedImprovement(AnalyticAcquisitionFunction):\n",
    "    r\"\"\"Analytic Budgeted Expected Improvement.\n",
    "    Computes the analytic expected improvement weighted by a probability of\n",
    "    satisfying a budget constraint. The objective and (log-) cost are assumed\n",
    "    to be independent and have Gaussian posterior distributions. Only supports\n",
    "    the case `q=1`. The model should be two-outcome, with the first output\n",
    "    corresponding to the objective and the second output corresponding to the\n",
    "    log-cost.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        budget: Union[float, Tensor],\n",
    "        maximize: bool = True,\n",
    "        # objective: Optional[ScalarizedObjective] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Analytic Budgeted Expected Improvement.\n",
    "        Args:\n",
    "            model: A fitted two-outcome model, where the first output corresponds\n",
    "                to the objective and the second one to the log-cost.\n",
    "            best_f: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the best function value observed so far (assumed noiseless).\n",
    "            budget: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the budget constraint.\n",
    "            maximize: If True, consider the problem a maximization problem.\n",
    "        \"\"\"\n",
    "        # use AcquisitionFunction constructor to avoid check for objective\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model=model)\n",
    "        self.objective = None\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"budget\", torch.as_tensor(budget))\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Evaluate Constrained Expected Improvement on the candidate set X.\n",
    "        Args:\n",
    "            X: A `(b) x 1 x d`-dim Tensor of `(b)` t-batches of `d`-dim design\n",
    "                points each.\n",
    "        Returns:\n",
    "            A `(b)`-dim Tensor of Expected Improvement values at the given\n",
    "            design points `X`.\n",
    "        \"\"\"\n",
    "        posterior = self._get_posterior(X=X)\n",
    "        means = posterior.mean  # (b) x 2\n",
    "        sigmas = posterior.variance.sqrt().clamp_min(1e-6)  # (b) x 2\n",
    "\n",
    "        # (b) x 1\n",
    "        mean_obj = means[..., 0]\n",
    "        sigma_obj = sigmas[..., 0]\n",
    "        u = (mean_obj - self.best_f) / sigma_obj\n",
    "\n",
    "        if not self.maximize:\n",
    "            u = -u\n",
    "        standard_normal = Normal(\n",
    "            torch.zeros(1, device=u.device, dtype=u.dtype),\n",
    "            torch.ones(1, device=u.device, dtype=u.dtype),\n",
    "        )\n",
    "        pdf_u = torch.exp(standard_normal.log_prob(u))\n",
    "        cdf_u = standard_normal.cdf(u)\n",
    "        ei = sigma_obj * (pdf_u + u * cdf_u)  # (b) x 1\n",
    "        # (b) x 1\n",
    "        prob_feas = self._compute_prob_feas(means=means[..., 1], sigmas=sigmas[..., 1])\n",
    "        bc_ei = ei.mul(prob_feas)  # (b) x 1\n",
    "        return bc_ei.squeeze(dim=-1)\n",
    "\n",
    "    def _compute_prob_feas(self, means: Tensor, sigmas: Tensor) -> Tensor:\n",
    "        r\"\"\"Compute feasibility probability for each batch of X.\n",
    "        Args:\n",
    "            X: A `(b) x 1 x d`-dim Tensor of `(b)` t-batches of `d`-dim design\n",
    "                points each.\n",
    "            means: A `(b) x 1`-dim Tensor of means.\n",
    "            sigmas: A `(b) x 1`-dim Tensor of standard deviations.\n",
    "        Returns:\n",
    "            A `(b) x 1`-dim tensor of feasibility probabilities.\n",
    "        \"\"\"\n",
    "        standard_normal = Normal(\n",
    "            torch.zeros(1, device=means.device, dtype=means.dtype),\n",
    "            torch.ones(1, device=means.device, dtype=means.dtype),\n",
    "            validate_args=True,\n",
    "        )\n",
    "        prob_feas = standard_normal.cdf(\n",
    "            (torch.log(self.budget.clamp_min(1e-6)) - means) / sigmas\n",
    "        )\n",
    "        prob_feas = torch.where(\n",
    "            self.budget > 1e-6,\n",
    "            prob_feas,\n",
    "            torch.zeros(1, device=means.device, dtype=means.dtype),\n",
    "        )\n",
    "        return prob_feas\n",
    "\n",
    "\n",
    "class qBudgetedExpectedImprovement(MCAcquisitionFunction):\n",
    "    r\"\"\"Batch Budget-Constrained Expected Improvement.\n",
    "    Computes the expected improvement weighted by a probability of satisfying\n",
    "    a budget constraint. The model should be two-outcome, with the first output\n",
    "    corresponding to the objective and the second output corresponding to the\n",
    "    log-cost.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        budget: Union[float, Tensor],\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        X_pending: Optional[Tensor] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Batch Budgeted Expected Improvement.\n",
    "        Args:\n",
    "            model: A fitted two-outcome model, where the first output corresponds\n",
    "                to the objective and the second one to the log-cost.\n",
    "            best_f: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the best function value observed so far (assumed noiseless).\n",
    "            log_budget: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the budget constraint.\n",
    "            sampler: The sampler used to draw base samples. Defaults to\n",
    "                `SobolQMCNormalSampler(num_samples=512, collapse_batch_dims=True)`.\n",
    "            objective: The MCAcquisitionObjective under which the samples are\n",
    "                evaluated. Defaults to `IdentityMCObjective()`.\n",
    "            X_pending: A `batch_shape, m x d`-dim Tensor of `m` design points\n",
    "                that have points that have been submitted for function evaluation\n",
    "                but have not yet been evaluated.\n",
    "        \"\"\"\n",
    "        # use AcquisitionFunction constructor to avoid check for objective\n",
    "        super(MCAcquisitionFunction, self).__init__(model=model)\n",
    "        self.sampler = sampler\n",
    "        self.objective = None\n",
    "        self.X_pending = X_pending\n",
    "        self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"budget\", torch.as_tensor(budget))\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Evaluate Constrained Expected Improvement on the candidate set X.\n",
    "        Args:\n",
    "            X: A `(b) x q x d`-dim Tensor of `(b)` t-batches of `d`-dim design\n",
    "                points each.\n",
    "        Returns:\n",
    "            A `(b)`-dim Tensor of Expected Improvement values at the given\n",
    "            design points `X`.\n",
    "        \"\"\"\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        improvements = (samples[..., 0] - self.best_f).clamp_min(0)\n",
    "        max_improvement = improvements.max(dim=-1, keepdim=True)[0]\n",
    "        sum_costs = torch.exp(samples[..., 1]).sum(dim=-1, keepdim=True)\n",
    "        smooth_feas_ind = soft_eval_constraint(lhs=sum_costs - self.budget)\n",
    "        bc_ei = torch.mul(max_improvement, smooth_feas_ind).mean(dim=0)\n",
    "        return bc_ei.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1665960303260,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "rBdzK-auvzAb"
   },
   "outputs": [],
   "source": [
    "# @title budgeted_multi_step_ei.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.multi_step_lookahead import qMultiStepLookahead\n",
    "from botorch.acquisition.objective import LinearMCObjective, ScalarizedObjective\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.samplers import MCSampler, SobolQMCNormalSampler\n",
    "\n",
    "# from budgeted_bo.acquisition_functions.budgeted_ei import (\n",
    "#     BudgetedExpectedImprovement,\n",
    "#     qBudgetedExpectedImprovement,\n",
    "# )\n",
    "# from budgeted_bo.samplers import PosteriorMeanSampler\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class BudgetedMultiStepExpectedImprovement(qMultiStepLookahead):\n",
    "    r\"\"\"Budget-Constrained Multi-Step Look-Ahead Expected Improvement (one-shot optimization).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        budget_plus_cumulative_cost: Union[float, Tensor],\n",
    "        batch_size: int,\n",
    "        lookahead_batch_sizes: List[int],\n",
    "        num_fantasies: Optional[List[int]] = None,\n",
    "        samplers: Optional[List[MCSampler]] = None,\n",
    "        X_pending: Optional[Tensor] = None,\n",
    "        collapse_fantasy_base_samples: bool = True,\n",
    "    ) -> None:\n",
    "        r\"\"\"Budgeted Multi-Step Expected Improvement.\n",
    "        Args:\n",
    "            model: A fitted two-output model, where the first output corresponds to the\n",
    "                objective, and the second one to the log-cost.\n",
    "            budget: A value determining the budget constraint.\n",
    "            batch_size: Batch size of the current step.\n",
    "            lookahead_batch_sizes: A list `[q_1, ..., q_k]` containing the batch sizes for the\n",
    "            `k` look-ahead steps.\n",
    "            num_fantasies: A list `[f_1, ..., f_k]` containing the number of fantasy\n",
    "                for the `k` look-ahead steps.\n",
    "            samplers: A list of MCSampler objects to be used for sampling fantasies in\n",
    "                each stage.\n",
    "            X_pending: A `m x d`-dim Tensor of `m` design points that have points that\n",
    "                have been submitted for function evaluation but have not yet been\n",
    "                evaluated. Concatenated into `X` upon forward call. Copied and set to\n",
    "                have no gradient.\n",
    "            collapse_fantasy_base_samples: If True, collapse_batch_dims of the Samplers\n",
    "                will be applied on fantasy batch dimensions as well, meaning that base\n",
    "                samples are the same in all subtrees starting from the same level.\n",
    "        \"\"\"\n",
    "        self.budget_plus_cumulative_cost = budget_plus_cumulative_cost\n",
    "        self.batch_size = batch_size\n",
    "        batch_sizes = [batch_size] + lookahead_batch_sizes\n",
    "\n",
    "        # TODO: This objective is never really used.\n",
    "        weights = torch.zeros(model.num_outputs, dtype=torch.double)\n",
    "        weights[0] = 1.0\n",
    "\n",
    "        use_mc_val_funcs = any(bs != 1 for bs in batch_sizes)\n",
    "\n",
    "        if use_mc_val_funcs:\n",
    "            objective = LinearMCObjective(weights=weights)\n",
    "\n",
    "            valfunc_cls = [qBudgetedExpectedImprovement for _ in batch_sizes]\n",
    "\n",
    "            inner_mc_samples = [128 for bs in batch_sizes]\n",
    "        else:\n",
    "            objective = ScalarizedObjective(weights=weights)\n",
    "\n",
    "            valfunc_cls = [BudgetedExpectedImprovement for _ in batch_sizes]\n",
    "\n",
    "            inner_mc_samples = None\n",
    "\n",
    "        valfunc_argfacs = [\n",
    "            budgeted_ei_argfac(\n",
    "                budget_plus_cumulative_cost=self.budget_plus_cumulative_cost\n",
    "            )\n",
    "            for _ in batch_sizes\n",
    "        ]\n",
    "\n",
    "        # Set samplers\n",
    "        if samplers is None:\n",
    "            # The batch_range is not set here and left to sampler default of (0, -2),\n",
    "            # meaning that collapse_batch_dims will be applied on fantasy batch dimensions.\n",
    "            # If collapse_fantasy_base_samples is False, the batch_range is updated during\n",
    "            # the forward call.\n",
    "            samplers: List[MCSampler] = [\n",
    "                PosteriorMeanSampler(collapse_batch_dims=True)\n",
    "                if nf == 1\n",
    "                else SobolQMCNormalSampler(\n",
    "                    num_samples=nf, resample=False, collapse_batch_dims=True\n",
    "                )\n",
    "                for nf in num_fantasies\n",
    "            ]\n",
    "\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            batch_sizes=lookahead_batch_sizes,\n",
    "            samplers=samplers,\n",
    "            valfunc_cls=valfunc_cls,\n",
    "            valfunc_argfacs=valfunc_argfacs,\n",
    "            objective=objective,\n",
    "            inner_mc_samples=inner_mc_samples,\n",
    "            X_pending=X_pending,\n",
    "            collapse_fantasy_base_samples=collapse_fantasy_base_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "class budgeted_ei_argfac(Module):\n",
    "    r\"\"\"Extract the best observed value and reamaining budget from the model.\"\"\"\n",
    "\n",
    "    def __init__(self, budget_plus_cumulative_cost: Union[float, Tensor]) -> None:\n",
    "        super().__init__()\n",
    "        self.budget_plus_cumulative_cost = budget_plus_cumulative_cost\n",
    "\n",
    "    def forward(self, model: Model, X: Tensor) -> Dict[str, Any]:\n",
    "        y = torch.transpose(model.train_targets, -2, -1)\n",
    "        y_original_scale = model.outcome_transform.untransform(y)[0]\n",
    "        obj_vals = y_original_scale[..., 0]\n",
    "        log_costs = y_original_scale[..., 1]\n",
    "        costs = torch.exp(log_costs)\n",
    "        current_budget = self.budget_plus_cumulative_cost - costs.sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \"best_f\": obj_vals.max(dim=-1, keepdim=True).values,\n",
    "            \"budget\": current_budget,\n",
    "        }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1665960303261,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "1BC-J_p6xMWh"
   },
   "outputs": [],
   "source": [
    "# @title custom_warmstart_multistep_old.py\n",
    "from botorch import acquisition\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from botorch.acquisition.analytic import ExpectedImprovement, PosteriorMean\n",
    "from botorch.acquisition.multi_step_lookahead import (\n",
    "    qMultiStepLookahead,\n",
    "    warmstart_multistep,\n",
    ")\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.sampling.samplers import IIDNormalSampler\n",
    "from torch import Tensor\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def custom_warmstart_multistep(\n",
    "    acq_function: qMultiStepLookahead,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    full_optimizer: Tensor,\n",
    "    algo_params: Dict,\n",
    ") -> Tensor:\n",
    "    batch_initial_conditions = warmstart_multistep(\n",
    "        acq_function=acq_function,\n",
    "        bounds=bounds,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        full_optimizer=full_optimizer,\n",
    "    )\n",
    "\n",
    "    n_initial_points = batch_initial_conditions.shape[0]\n",
    "    random_index = random.randrange(n_initial_points)\n",
    "    input_dim = batch_initial_conditions.shape[-1]\n",
    "    batch_shape, shapes, sizes = acq_function.get_split_shapes(\n",
    "        X=batch_initial_conditions\n",
    "    )\n",
    "\n",
    "    # Copy of objective model\n",
    "    obj_model = acq_function.model.subset_output(idcs=[0])\n",
    "\n",
    "    # Define optimization domain\n",
    "    standard_bounds = torch.tensor([[0.0] * input_dim, [1.0] * input_dim])\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for size in sizes:\n",
    "        # Auxiliar acquisition fucntion\n",
    "        aux_acq_func_id = \"PM\"\n",
    "        if aux_acq_func_id == \"PM\":\n",
    "            aux_acq_func = PosteriorMean(model=obj_model)\n",
    "\n",
    "        elif aux_acq_func_id == \"EI\":\n",
    "            standardized_obj_vals = obj_model.train_targets\n",
    "            obj_vals = obj_model.outcome_transform.untransform(standardized_obj_vals)[0]\n",
    "            best_f = torch.max(obj_vals).item()\n",
    "\n",
    "            aux_acq_func = ExpectedImprovement(model=obj_model, best_f=best_f)\n",
    "\n",
    "        # Get new point\n",
    "        new_x, acq_value = optimize_acqf(\n",
    "            acq_function=aux_acq_func,\n",
    "            bounds=standard_bounds,\n",
    "            q=1,\n",
    "            num_restarts=5 * input_dim,\n",
    "            raw_samples=100 * input_dim,\n",
    "            options={\n",
    "                \"batch_limit\": 5,\n",
    "                \"maxiter\": 100,\n",
    "                \"method\": \"L-BFGS-B\",\n",
    "            },\n",
    "            return_best_only=True,\n",
    "        )\n",
    "\n",
    "        print(new_x)\n",
    "        for _ in range(size):\n",
    "            batch_initial_conditions[random_index, i, :] = new_x.clone().squeeze(0)\n",
    "            i += 1\n",
    "\n",
    "        # Fantasize objective and cost values\n",
    "        obj_sampler = IIDNormalSampler(\n",
    "            num_samples=1, resample=True, collapse_batch_dims=True\n",
    "        )\n",
    "        obj_post_X = obj_model.posterior(new_x, observation_noise=True)\n",
    "        fantasy_obj_val = obj_sampler(obj_post_X).squeeze(dim=0)\n",
    "        obj_model = obj_model.condition_on_observations(\n",
    "            X=new_x, Y=fantasy_obj_val.detach()\n",
    "        )\n",
    "\n",
    "    print(batch_initial_conditions[random_index])\n",
    "    return batch_initial_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1665960303450,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "hC37tyaTxTmD"
   },
   "outputs": [],
   "source": [
    "# @title ei_puc.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.analytic import AnalyticAcquisitionFunction\n",
    "from botorch.acquisition.objective import ScalarizedObjective\n",
    "from botorch.models.model import Model\n",
    "from botorch.utils.transforms import concatenate_pending_points, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class ExpectedImprovementPerUnitOfCost(AnalyticAcquisitionFunction):\n",
    "    r\"\"\"Expected Improvement Per Unit of Cost (analytic).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        cost_exponent: Union[float, Tensor] = 1.0,\n",
    "        maximize: bool = True,\n",
    "        objective: Optional[ScalarizedObjective] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Single-outcome Expected Improvement (analytic).\n",
    "        Args:\n",
    "            model: A fitted two-outcome model, where the first output corresponds\n",
    "                to the objective and the second one to the log-cost.\n",
    "            best_f: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the best function value observed so far (assumed noiseless).\n",
    "            maximize: If True, consider the problem a maximization problem.\n",
    "        \"\"\"\n",
    "        # use AcquisitionFunction constructor to avoid check for objective\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model=model)\n",
    "        self.objective = None\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"cost_exponent\", torch.as_tensor(cost_exponent))\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1, assert_output_shape=False)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Evaluate Expected Improvement Per Unit of Cost on the candidate set X.\n",
    "        Args:\n",
    "            X: A `b1 x ... bk x 1 x d`-dim batched tensor of `d`-dim design points.\n",
    "                Expected Improvement is computed for each point individually,\n",
    "                i.e., what is considered are the marginal posteriors, not the\n",
    "                joint.\n",
    "        Returns:\n",
    "            A `b1 x ... bk`-dim tensor of Expected Improvement Per Unit of Cost values\n",
    "            at the given design points `X`.\n",
    "        \"\"\"\n",
    "        posterior = self._get_posterior(X=X)\n",
    "        # posterior = self.model.posterior(X=X)\n",
    "        means = posterior.mean  # (b) x 2\n",
    "        vars = posterior.variance.clamp_min(1e-6)  # (b) x 2\n",
    "        stds = vars.sqrt()\n",
    "\n",
    "        # (b) x 1\n",
    "        mean_obj = means[..., 0]\n",
    "        std_obj = stds[..., 0]\n",
    "        u = (mean_obj - self.best_f) / std_obj\n",
    "\n",
    "        if not self.maximize:\n",
    "            u = -u\n",
    "        standard_normal = Normal(\n",
    "            torch.zeros(1, device=u.device, dtype=u.dtype),\n",
    "            torch.ones(1, device=u.device, dtype=u.dtype),\n",
    "        )\n",
    "        pdf_u = torch.exp(standard_normal.log_prob(u))\n",
    "        cdf_u = standard_normal.cdf(u)\n",
    "        ei = std_obj * (pdf_u + u * cdf_u)  # (b) x 1\n",
    "        # (b) x 1\n",
    "        eic = torch.exp(\n",
    "            -(self.cost_exponent * means[..., 1])\n",
    "            + 0.5 * (torch.square(self.cost_exponent) * vars[..., 1])\n",
    "        )\n",
    "        ei_puc = ei.mul(eic)  # (b) x 1\n",
    "        return ei_puc.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1665960303451,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "JTnvgx3txb5m"
   },
   "outputs": [],
   "source": [
    "# @title multi_step_ei.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.acquisition.multi_step_lookahead import qMultiStepLookahead\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.samplers import MCSampler, SobolQMCNormalSampler\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class MultiStepLookaheadEI(qMultiStepLookahead):\n",
    "    r\"\"\"Multi-Step Look-Ahead Expected Improvement (one-shot optimization).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        batch_size: int,\n",
    "        lookahead_batch_sizes: List[int],\n",
    "        num_fantasies: Optional[List[int]] = None,\n",
    "        samplers: Optional[List[MCSampler]] = None,\n",
    "        X_pending: Optional[Tensor] = None,\n",
    "        collapse_fantasy_base_samples: bool = True,\n",
    "    ) -> None:\n",
    "        r\"\"\"Multi-Step Look-Ahead Expected Improvement.\n",
    "        Args:\n",
    "            model: A single-output Model of appropriate batch size.\n",
    "            batch_size: Batch size for the current step.\n",
    "            lookahead_batch_sizes: A list `[q_1, ..., q_k]` containing the batch sizes for the\n",
    "                `k` look-ahead steps.\n",
    "            num_fantasies: A list `[f_1, ..., f_k]` containing the number of fantasy\n",
    "                points to use for the `k` look-ahead steps.\n",
    "            samplers: A list of MCSampler objects to be used for sampling fantasies in\n",
    "                each stage.\n",
    "            X_pending: A `m x d`-dim Tensor of `m` design points that have points that\n",
    "                have been submitted for function evaluation but have not yet been\n",
    "                evaluated. Concatenated into `X` upon forward call. Copied and set to\n",
    "                have no gradient.\n",
    "            collapse_fantasy_base_samples: If True, collapse_batch_dims of the Samplers\n",
    "                will be applied on fantasy batch dimensions as well, meaning that base\n",
    "                samples are the same in all subtrees starting from the same level.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        batch_sizes = [batch_size] + lookahead_batch_sizes\n",
    "\n",
    "        if any(bs != 1 for bs in batch_sizes):\n",
    "            valfunc_cls = [qExpectedImprovement for _ in batch_sizes]\n",
    "            inner_mc_samples = [512 for bs in batch_sizes]\n",
    "        else:\n",
    "            valfunc_cls = [ExpectedImprovement for _ in batch_sizes]\n",
    "            inner_mc_samples = None\n",
    "\n",
    "        valfunc_argfacs = [multi_step_ei_argfac for _ in batch_sizes]\n",
    "\n",
    "        # Set samplers\n",
    "        if samplers is None:\n",
    "            # The batch_range is not set here and left to sampler default of (0, -2),\n",
    "            # meaning that collapse_batch_dims will be applied on fantasy batch dimensions.\n",
    "            # If collapse_fantasy_base_samples is False, the batch_range is updated during\n",
    "            # the forward call.\n",
    "            samplers: List[MCSampler] = [\n",
    "                PosteriorMeanSampler(collapse_batch_dims=True)\n",
    "                if nf == 1\n",
    "                else SobolQMCNormalSampler(\n",
    "                    num_samples=nf, resample=False, collapse_batch_dims=True\n",
    "                )\n",
    "                for nf in num_fantasies\n",
    "            ]\n",
    "\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            batch_sizes=lookahead_batch_sizes,\n",
    "            samplers=samplers,\n",
    "            valfunc_cls=valfunc_cls,\n",
    "            valfunc_argfacs=valfunc_argfacs,\n",
    "            inner_mc_samples=inner_mc_samples,\n",
    "            X_pending=X_pending,\n",
    "            collapse_fantasy_base_samples=collapse_fantasy_base_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "def multi_step_ei_argfac(model: Model, X: Tensor) -> Dict[str, Any]:\n",
    "    y = model.train_targets\n",
    "    y_original_scale = model.outcome_transform.untransform(y)[0]\n",
    "    obj_vals = y_original_scale\n",
    "    params = {\n",
    "        \"best_f\": obj_vals.max(dim=-1).values,\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8n4iQI9xrSQ"
   },
   "source": [
    "### Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1665960303452,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "uu3z1Hyjxsx7"
   },
   "outputs": [],
   "source": [
    "# @title posterior_mean_sampler.py\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from botorch.posteriors import Posterior\n",
    "from botorch.sampling.samplers import MCSampler, _check_shape_changed\n",
    "from botorch.utils.sampling import manual_seed\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class PosteriorMeanSampler(MCSampler):\n",
    "    r\"\"\"Sampler for MC base samples using iid N(0,1) samples.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collapse_batch_dims: bool = True,\n",
    "        batch_range: Tuple[int, int] = (0, -2),\n",
    "    ) -> None:\n",
    "        r\"\"\"Sampler for MC base samples using iid `N(0,1)` samples.\n",
    "        Args:\n",
    "            num_samples: The number of samples to use.\n",
    "            resample: If `True`, re-draw samples in each `forward` evaluation -\n",
    "                this results in stochastic acquisition functions (and thus should\n",
    "                not be used with deterministic optimization algorithms).\n",
    "            seed: The seed for the RNG. If omitted, use a random seed.\n",
    "            collapse_batch_dims: If True, collapse the t-batch dimensions to\n",
    "                size 1. This is useful for preventing sampling variance across\n",
    "                t-batches.\n",
    "            batch_range: The range of t-batch dimensions in the `base_sample_shape`\n",
    "                used by `collapse_batch_dims`. The t-batch dims are\n",
    "                batch_range[0]:batch_range[1]. By default, this is (0, -2),\n",
    "                for the case where the non-batch dimensions are -2 (q) and\n",
    "                -1 (d) and all dims in the front are t-batch dims.\n",
    "        \"\"\"\n",
    "        super().__init__(batch_range=batch_range)\n",
    "        self._sample_shape = torch.Size([1])\n",
    "        self.collapse_batch_dims = collapse_batch_dims\n",
    "\n",
    "    def _construct_base_samples(self, posterior: Posterior, shape: torch.Size) -> None:\n",
    "        r\"\"\"Generate iid `N(0,1)` base samples (if necessary).\n",
    "        This function will generate a new set of base samples and set the\n",
    "        `base_samples` buffer if one of the following is true:\n",
    "        - `resample=True`\n",
    "        - the MCSampler has no `base_samples` attribute.\n",
    "        - `shape` is different than `self.base_samples.shape` (if\n",
    "            `collapse_batch_dims=True`, then batch dimensions of will be\n",
    "            automatically broadcasted as necessary). This shape is expected to\n",
    "            be `sample_shape + base_sample_shape`, where `base_sample_shape` has been\n",
    "            adjusted to account for `collapse_batch_dims` (i.e., the output\n",
    "            of the function `_get_base_sample_shape`).\n",
    "        Args:\n",
    "            posterior: The Posterior for which to generate base samples.\n",
    "            shape: The shape of the base samples to construct.\n",
    "        \"\"\"\n",
    "        if _check_shape_changed(self.base_samples, self.batch_range, shape) or (\n",
    "            not self.collapse_batch_dims and shape != self.base_samples.shape\n",
    "        ):\n",
    "            base_samples = torch.zeros(\n",
    "                shape, device=posterior.device, dtype=posterior.dtype\n",
    "            )\n",
    "            self.register_buffer(\"base_samples\", base_samples)\n",
    "        elif self.collapse_batch_dims and shape != self.base_samples.shape:\n",
    "            self.base_samples = self.base_samples.view(shape)\n",
    "        if self.base_samples.device != posterior.device:\n",
    "            self.to(device=posterior.device)  # pragma: nocover\n",
    "        if self.base_samples.dtype != posterior.dtype:\n",
    "            self.to(dtype=posterior.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1665960303453,
     "user": {
      "displayName": "Sang T. Truong",
      "userId": "11064366708249050654"
     },
     "user_tz": 420
    },
    "id": "kYhy6JuXV8y8"
   },
   "outputs": [],
   "source": [
    "# @title parametric_cost_function.py\n",
    "import torch\n",
    "from botorch.utils.sampling import manual_seed\n",
    "from math import pi\n",
    "\n",
    "\n",
    "def get_cost_function_parameters(seed):\n",
    "    with manual_seed(seed=seed):\n",
    "        a = torch.rand(1)\n",
    "        a = 0.75 * a + 0.75\n",
    "\n",
    "        b = torch.rand(1)\n",
    "        b = 2.0 * b + 1.0\n",
    "\n",
    "        c = torch.rand(1)\n",
    "        c = 2 * pi * c\n",
    "    return a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4JXv67JHgGE"
   },
   "source": [
    "# Cost-constraint BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15794,
     "status": "ok",
     "timestamp": 1665955731600,
     "user": {
      "displayName": "Hieu Tran",
      "userId": "10537577983681808764"
     },
     "user_tz": 240
    },
    "id": "nsz8at-ZS8Wy",
    "outputId": "b0d1ad42-dfea-4eb0-a086-c6e8bc7761be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting qmcpy==0.7\n",
      "  Downloading qmcpy-0.7.tar.gz (4.0 MB)\n",
      "\u001b[K     || 4.0 MB 25.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from qmcpy==0.7) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from qmcpy==0.7) (1.21.6)\n",
      "Building wheels for collected packages: qmcpy\n",
      "  Building wheel for qmcpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for qmcpy: filename=qmcpy-0.7-cp37-cp37m-linux_x86_64.whl size=4159290 sha256=f44dc4ace8bdbf4edc75cb9955f9d7e308d0d89469b401dfbdb1e94a0e12cb7e\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/51/2b/bb041d71a5c4e01ddad2c3a569c74746c3c2adb55aaf1e2b7b\n",
      "Successfully built qmcpy\n",
      "Installing collected packages: qmcpy\n",
      "Successfully installed qmcpy-0.7\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericlee0803/lookahead_release\n",
    "!pip install qmcpy==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u90Za1JDTSUv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qmcpy as qp\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from abc import ABCMeta\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from scipy.linalg import solve_triangular, cho_factor, cho_solve, cholesky\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "\n",
    "import scipy as sp\n",
    "import pprint\n",
    "import scipy.linalg\n",
    "import scipy.optimize\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUKH5pJlI3Ap"
   },
   "source": [
    "## lookahead/acquisitions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wHJ6oF_DHkuB"
   },
   "outputs": [],
   "source": [
    "# @title acquisition_function_interface.py\n",
    "# Super simple acquisition function interface to be used\n",
    "\n",
    "\n",
    "class AcquisitionFunctionInterface(object):\n",
    "    def __init__(self, gaussian_process, opt_domain, **kwargs):\n",
    "        self.gaussian_process = gaussian_process\n",
    "        self.opt_domain = opt_domain\n",
    "        self.opt_domain_constraints = np.array(\n",
    "            [\n",
    "                (interval.min, interval.max)\n",
    "                for interval in self.opt_domain.get_bounding_box()\n",
    "            ]\n",
    "        )\n",
    "        self.horizon = 1\n",
    "\n",
    "    # There are some seeds that could be problems for this, I think (if the point [0, 0] is in)\n",
    "    def low_discrepancy_points(self, num, seed=1234):\n",
    "        measure = qp.Gaussian(qp.Sobol(dimension=self.horizon, seed=seed))\n",
    "        points = measure.gen_samples(n=2 ** np.ceil(np.log2(num)))[:num]\n",
    "        points = np.nan_to_num(points)\n",
    "        assert not np.any(np.isnan(points))\n",
    "        return points\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        pass\n",
    "\n",
    "    def joint_function_gradient_eval(self, points_to_evaluate):\n",
    "        pass\n",
    "\n",
    "    def next_point_grid(self, num_grid_points=400):\n",
    "        grid_points = self.opt_domain.generate_quasi_random_points_in_domain(\n",
    "            num_grid_points\n",
    "        )\n",
    "        vals = self.evaluate_at_point_list(grid_points)\n",
    "        idx = np.argmax(vals)\n",
    "        return grid_points[[idx], :]\n",
    "\n",
    "    def next_point_grad(self, num_restarts=5):\n",
    "        ymin = np.inf\n",
    "        num_guess = 10 * self.opt_domain_constraints.shape[0]\n",
    "        init_guesses = self.opt_domain.generate_quasi_random_points_in_domain(num_guess)\n",
    "        guesses = self.evaluate_at_point_list(init_guesses)\n",
    "        idx_guesses = np.argsort(guesses)\n",
    "        for i in range(num_restarts):\n",
    "            x0 = init_guesses[[idx_guesses[-i]], :]\n",
    "            opt_result = minimize(\n",
    "                fun=self.joint_function_gradient_eval,\n",
    "                method=\"L-BFGS-B\",\n",
    "                x0=x0,\n",
    "                jac=True,\n",
    "                bounds=self.opt_domain_constraints,\n",
    "                options={\"maxiter\": 40},\n",
    "            )\n",
    "            if opt_result.fun < ymin:\n",
    "                x = opt_result.x\n",
    "                ymin = opt_result.fun\n",
    "        return x[np.newaxis, :]\n",
    "\n",
    "    def next_point(self):\n",
    "        if self.opt_domain_constraints.shape[0] > 2:\n",
    "            return self.next_point_grad()\n",
    "        else:\n",
    "            return self.next_point_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TVMNV9x_KOuw"
   },
   "outputs": [],
   "source": [
    "# @title acquisition_optimizer_bo.py\n",
    "\n",
    "\n",
    "class BayesOptAcquisitionOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizes acquisition function w/ bayesian optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gaussian_process, acquisition_handle, opt_domain):\n",
    "        self.gaussian_process = gaussian_process\n",
    "        self.acquisition_handle = acquisition_handle\n",
    "        self.opt_domain = opt_domain\n",
    "\n",
    "    def get_initial_design(self):\n",
    "        # maximize the expected improvement first\n",
    "        ei = ExpectedImprovement(self.gaussian_process, self.opt_domain)\n",
    "        x_ei_opt = ei.next_point()\n",
    "        X = self.opt_domain.generate_quasi_random_points_in_domain(5)\n",
    "        X = np.vstack([X, x_ei_opt])\n",
    "        return X\n",
    "\n",
    "    def get_sample_point(self):\n",
    "        \"\"\"\n",
    "        Get next sample point through BO, using 100d BO iterations. We want to maximize, meaning we have to multiple by\n",
    "        negative when calling the handle\n",
    "        \"\"\"\n",
    "        budget = self.gaussian_process.d * 5\n",
    "        initial_design = self.get_initial_design()\n",
    "        X = initial_design\n",
    "        Y = -1 * self.acquisition_handle(X)\n",
    "\n",
    "        for i in range(budget):\n",
    "            # Normalize Y\n",
    "            Y_train = np.copy(Y)\n",
    "            Y_train = (Y_train - np.mean(Y_train)) / np.std(Y_train)\n",
    "            gp = GaussianProcessSimple(X, Y_train)\n",
    "            gp.train()\n",
    "\n",
    "            ei = ExpectedImprovement(gp, self.opt_domain)\n",
    "            xx = ei.next_point()\n",
    "            yy = -1 * self.acquisition_handle(xx)\n",
    "            X = np.vstack([X, xx])\n",
    "            Y = np.concatenate([Y, yy])\n",
    "\n",
    "        idx = np.argmin(Y)\n",
    "        return X[[idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mpa5EdRtKCgm"
   },
   "outputs": [],
   "source": [
    "# @title expected_improvement.py\n",
    "\n",
    "\n",
    "class ExpectedImprovement(AcquisitionFunctionInterface):\n",
    "    def __init__(self, gaussian_process, opt_domain):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        mu = self.gaussian_process.mean(points_to_evaluate)\n",
    "        sigma2 = self.gaussian_process.variance(points_to_evaluate)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "        fmin = np.min(yhist)\n",
    "        z = (fmin - mu) / sigma\n",
    "        cdf_z = norm.cdf(z)\n",
    "        pdf_z = norm.pdf(z)\n",
    "        return sigma * np.fmax(0.0, z * cdf_z + pdf_z)\n",
    "\n",
    "    def evaluate_grad_at_point_list(self, points_to_evaluate):\n",
    "        return self._evaluate_grad_at_point_list_core(\n",
    "            self.gaussian_process.components(points_to_evaluate)\n",
    "        )\n",
    "\n",
    "    def _evaluate_at_point_list_core(self, core_components):\n",
    "        z, sqrt_var, cdf_z, pdf_z, _, _, _ = core_components\n",
    "        return sqrt_var * np.fmax(0.0, z * cdf_z + pdf_z)\n",
    "\n",
    "    def _evaluate_grad_at_point_list_core(self, core_components):\n",
    "        _, _, cdf_z, pdf_z, grad_mean, _, grad_sqrt_var = core_components\n",
    "        return grad_sqrt_var * pdf_z[:, np.newaxis] - grad_mean * cdf_z[:, np.newaxis]\n",
    "\n",
    "    def joint_function_gradient_eval(self, points_to_evaluate):\n",
    "        points_to_evaluate = np.atleast_2d(points_to_evaluate)\n",
    "        core_components = self.gaussian_process.components(points_to_evaluate)\n",
    "        ei = self._evaluate_at_point_list_core(core_components)\n",
    "        ei_grad = self._evaluate_grad_at_point_list_core(core_components)[0, :]\n",
    "        return -ei, -ei_grad\n",
    "\n",
    "    def next_point(self):\n",
    "        return self.next_point_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SooPhtx-Kgg2"
   },
   "outputs": [],
   "source": [
    "# @title knowledge_gradient.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "MINIMUM_VARIANCE_KG = 2.2250738585072014e-308  # np.finfo(np.float64).tiny\n",
    "\n",
    "KG_MC_ITERATIONS = 1000\n",
    "KG_DOMAIN_NUM_SAMPLES = 666\n",
    "\n",
    "\n",
    "class KnowledgeGradient(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    Implementation of Knowledge Gradient (see, e.g., Frazier et al 2008)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gaussian_process, opt_domain):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        eval_domain_points = opt_domain.generate_quasi_random_points_in_domain(\n",
    "            KG_DOMAIN_NUM_SAMPLES\n",
    "        )\n",
    "        self.num_mc_iterations = eval_domain_points.shape[0]\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "        self.best_value = np.min(yhist)\n",
    "        self.eval_domain_points = eval_domain_points\n",
    "        self.eval_domain_points_mean_estimate = (\n",
    "            self.gaussian_process.compute_mean_of_points(self.eval_domain_points)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def differentiable(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        num_points = len(points_to_evaluate)\n",
    "        sigma_star = np.sqrt(\n",
    "            self.gaussian_process.compute_variance_of_points(points_to_evaluate)\n",
    "        )\n",
    "        pred_stdev = self.gaussian_process.cross_correlation_for_samples(\n",
    "            points_to_evaluate, self.eval_domain_points\n",
    "        )\n",
    "        curr_best = self.best_value\n",
    "        std_norm_rv = np.random.normal(size=(self.num_mc_iterations, num_points))\n",
    "\n",
    "        scaled_norm_rvs = std_norm_rv[:, np.newaxis] * (pred_stdev / sigma_star)\n",
    "        post_estimate = (\n",
    "            self.eval_domain_points_mean_estimate[:, np.newaxis] + scaled_norm_rvs\n",
    "        )\n",
    "        # min applied for each mc sample over the eval_domain_points, then mean over all MC samples\n",
    "        value_kg = curr_best - np.mean(np.min(post_estimate, axis=1), axis=0)\n",
    "        return value_kg\n",
    "\n",
    "    def compute_acquisition_function(self, point_to_sample):\n",
    "        return self.evaluate_at_point_list(np.atleast_2d(point_to_sample))[0]\n",
    "\n",
    "    def compute_grad_acquisition_function(self, point_to_sample):\n",
    "        raise NotImplementedError(\"The gradient of the KG is not implemented yet\")\n",
    "\n",
    "    def next_point(self):\n",
    "        return self.next_point_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EwuFk8F-KpKx"
   },
   "outputs": [],
   "source": [
    "# @title probability_improvement.py\n",
    "\n",
    "\n",
    "class ProbabilityImprovement(AcquisitionFunctionInterface):\n",
    "    def __init__(self, gaussian_process, opt_domain):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        mu = self.gaussian_process.mean(points_to_evaluate)\n",
    "        sigma2 = self.gaussian_process.variance(points_to_evaluate)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "        fmin = np.min(yhist)\n",
    "        z = (fmin - mu) / sigma\n",
    "        cdf_z = norm.cdf(z)\n",
    "        return cdf_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WW9Xn69RKshX"
   },
   "outputs": [],
   "source": [
    "# @title rollout_ei.py\n",
    "\n",
    "\n",
    "class RolloutEI(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    A class for calculating the sequential rewards of MDP BayesOpt\n",
    "    via parallel MC. The code is threaded, and uses the max number of processors available\n",
    "    :gaussian_process: A GaussianProcess object\n",
    "    :domain: The domain to optimize the inner acquisition function over\n",
    "    :horizon: The time horizon\n",
    "    :mc_iters: Number of Monte Carlo iterations\n",
    "    :opt_mode: either 'grad' (default) or 'grid' for rollout acquisition optimization. 'grid' is faster for low-d problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gaussian_process, opt_domain, horizon, mc_iters=int(1e2), opt_mode=\"grad\"\n",
    "    ):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.gaussian_process = gaussian_process\n",
    "        self.opt_domain = opt_domain\n",
    "        self.horizon = horizon\n",
    "        self.mc_iters = mc_iters\n",
    "        self.numthreads = int(mp.cpu_count() / 2)\n",
    "        self.opt_mode = opt_mode\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        num_points = points_to_evaluate.shape[0]\n",
    "        rollout_values = np.zeros(num_points)\n",
    "        for i in range(0, num_points):\n",
    "            rollout_values[i] = self._evaluate_at_point_list(points_to_evaluate[[i], :])\n",
    "        return rollout_values\n",
    "\n",
    "    # Iterate through point list, parallelizing MC\n",
    "    def _evaluate_at_point_list(self, point_to_evaluate):\n",
    "        self.point_current = point_to_evaluate\n",
    "\n",
    "        if self.numthreads > 1:\n",
    "            serial_mc_iters = [int(self.mc_iters / self.numthreads)] * self.numthreads\n",
    "            pool = Pool(processes=self.numthreads)\n",
    "            rewards = pool.map(self._evaluate_point_at_list_serial, serial_mc_iters)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            rewards = self._evaluate_point_at_list_serial(self.mc_iters)\n",
    "\n",
    "        return np.sum(rewards) / self.numthreads\n",
    "\n",
    "    def _evaluate_point_at_list_serial(self, mc_iters):\n",
    "        reward = 0\n",
    "        for iters in range(0, mc_iters):\n",
    "            r = self.draw_from_policy(self.point_current)\n",
    "            reward += r\n",
    "        return reward / mc_iters\n",
    "\n",
    "    # Execute policy once using sequential draws from GPs\n",
    "    def draw_from_policy(self, point_to_evaluate):\n",
    "        reward = 0\n",
    "        gp_temp = copy.deepcopy(self.gaussian_process)\n",
    "        xi = point_to_evaluate\n",
    "        h = self.horizon\n",
    "        i = 0\n",
    "\n",
    "        # Optimize acquisition function and set new sample point\n",
    "        acquisition = ExpectedImprovement(gp_temp, self.opt_domain)\n",
    "\n",
    "        while True:\n",
    "            np.random.seed(int(time.time()))\n",
    "            fi = gp_temp.sample_single(xi)\n",
    "            ri = self.reward(gp_temp, fi)\n",
    "            reward += ri\n",
    "            i = i + 1\n",
    "            h = h - 1\n",
    "            if h <= 0:\n",
    "                break\n",
    "\n",
    "            gp_temp.chol_update(xi, fi)\n",
    "            if self.opt_mode == \"grad\":\n",
    "                xi = acquisition.next_point_grad()\n",
    "            else:\n",
    "                xi = acquisition.next_point_grid()\n",
    "        return reward\n",
    "\n",
    "    def reward(self, gp, fi):\n",
    "        _, ytrain = gp.get_historical_data()\n",
    "        ymin = np.min(ytrain)\n",
    "        r = max(ymin - float(fi), 0)\n",
    "        return r\n",
    "\n",
    "    def next_point(self):\n",
    "        optimizer = BayesOptAcquisitionOptimizer(\n",
    "            self.gaussian_process, self.evaluate_at_point_list, self.opt_domain\n",
    "        )\n",
    "        return optimizer.get_sample_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89sEzQHhK5du"
   },
   "outputs": [],
   "source": [
    "# @title rollout_ei_vr.py\n",
    "\n",
    "\n",
    "class RolloutEI_VR(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    A class for calculating the sequential rewards of MDP BayesOpt\n",
    "    via parallel MC. The code is threaded, and uses the max number of processors available\n",
    "    Variance reduction techniques used:\n",
    "    QMC, CRN, EI, PI Control Variates\n",
    "    :gaussian_process: A GaussianProcess object\n",
    "    :domain: The domain to optimize the inner acquisition function over\n",
    "    :horizon: The time horizon\n",
    "    :mc_iters: Number of Monte Carlo iterations\n",
    "    :opt_mode: either 'grad' (default) or 'grid' for rollout acquisition optimization. 'grid' is faster for low-d problems.\n",
    "    :grid_size: size of rollout grid, not used if opt_mode is 'grad'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gaussian_process,\n",
    "        opt_domain,\n",
    "        horizon,\n",
    "        mc_iters=int(1e3),\n",
    "        opt_mode=\"grad\",\n",
    "        grid_size=400,\n",
    "        seed=0,\n",
    "    ):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.gaussian_process = gaussian_process\n",
    "        self.opt_domain = opt_domain\n",
    "        self.horizon = horizon\n",
    "        self.mc_iters = mc_iters\n",
    "        self.numthreads = int(mp.cpu_count() / 2)\n",
    "        self.opt_mode = opt_mode\n",
    "        self.grid_size = grid_size\n",
    "        self.seed = seed\n",
    "\n",
    "        # Control Variates\n",
    "        self.ei = ExpectedImprovement(gaussian_process, opt_domain)\n",
    "        self.pi = ProbabilityImprovement(gaussian_process, opt_domain)\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        num_points = points_to_evaluate.shape[0]\n",
    "        rollout_values = np.zeros(num_points)\n",
    "        for i in range(0, num_points):\n",
    "            rollout_values[i] = self._evaluate_at_point_list(points_to_evaluate[[i], :])\n",
    "        return rollout_values\n",
    "\n",
    "    # Iterate through point list, parallelizing MC\n",
    "    def _evaluate_at_point_list(self, point_to_evaluate):\n",
    "        self.point_current = point_to_evaluate\n",
    "        random_number_stream = self.low_discrepancy_points(\n",
    "            self.mc_iters, seed=self.seed\n",
    "        )\n",
    "        pool = Pool(processes=self.numthreads)\n",
    "        rewards = pool.map(self.draw_from_policy, random_number_stream)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Sum up samples to get estimate\n",
    "        value_variates = np.sum(rewards) / self.mc_iters\n",
    "\n",
    "        # Remove effects of control variates\n",
    "        ei_value = self.ei.evaluate_at_point_list(point_to_evaluate)\n",
    "        pi_value = self.pi.evaluate_at_point_list(point_to_evaluate)\n",
    "        value_corrected = value_variates + ei_value + pi_value\n",
    "        return value_corrected\n",
    "\n",
    "    # Execute policy once using sequential draws from GPs\n",
    "    def draw_from_policy(self, random_number_list):\n",
    "        \"\"\"\n",
    "        Performs a draw from the policy, given a fixed random series of numbers\n",
    "        :param random_number_list: fixed random series of numbers, of size h, assumed unit normal distributed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.horizon > 1, \"Horizon must be greater than 1 to calculate MC\"\n",
    "\n",
    "        gp_temp = copy.deepcopy(self.gaussian_process)\n",
    "        xi = self.point_current\n",
    "        h = self.horizon\n",
    "        rewards = np.zeros(h)\n",
    "\n",
    "        # Optimize acquisition function and set new sample point\n",
    "        acquisition = ExpectedImprovement(gp_temp, self.opt_domain)\n",
    "\n",
    "        for i in range(self.horizon):\n",
    "            fi = gp_temp.sample_single(xi, random_number_list[i])\n",
    "\n",
    "            if i < self.horizon - 1:\n",
    "                rewards[i] = self.reward(gp_temp, fi)\n",
    "            else:\n",
    "                rewards[i] = acquisition.evaluate_at_point_list(xi)\n",
    "\n",
    "            gp_temp.chol_update(xi, fi)\n",
    "            if self.opt_mode == \"grad\":\n",
    "                xi = acquisition.next_point_grad()\n",
    "            else:\n",
    "                xi = acquisition.next_point_grid(self.grid_size)\n",
    "\n",
    "        # Add in control variates (EI, PI)... in this case, an EI control variate\n",
    "        # removes the first term, and a PI control variate adds 1 if there is\n",
    "        # improvement in the first sample\n",
    "        r = np.sum(rewards)\n",
    "        r -= rewards[0]  # EI Control Variate\n",
    "        if rewards[0] > 0:\n",
    "            r -= 1\n",
    "        return r\n",
    "\n",
    "    def reward(self, gp, fi):\n",
    "        _, ytrain = gp.get_historical_data()\n",
    "        ymin = np.min(ytrain)\n",
    "        r = max(ymin - float(fi), 0)\n",
    "        return r\n",
    "\n",
    "    def next_point(self):\n",
    "        \"\"\"\n",
    "        If self horizon is 1, just return the max of regular EI\n",
    "        \"\"\"\n",
    "        if self.horizon == 1:\n",
    "            ei = ExpectedImprovement(self.gaussian_process, self.opt_domain)\n",
    "            return ei.next_point_grid()\n",
    "        else:\n",
    "            optimizer = BayesOptAcquisitionOptimizer(\n",
    "                self.gaussian_process, self.evaluate_at_point_list, self.opt_domain\n",
    "            )\n",
    "            return optimizer.get_sample_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "v9BnaJEwLBtl"
   },
   "outputs": [],
   "source": [
    "# @title rollout_ei_vr_ablation.py\n",
    "\n",
    "\n",
    "class RolloutEI_VR_AB(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    A class for calculating the sequential rewards of MDP BayesOpt\n",
    "    via parallel MC. The code is threaded, and uses the max number of processors available\n",
    "    **ONLY TO BE USED FOR ABLATION STUDY**\n",
    "    Variance reduction techniques used:\n",
    "    QMC, CRN, Control Variates (optional)\n",
    "    :gaussian_process: A GaussianProcess object\n",
    "    :domain: The domain to optimize the inner acquisition function over\n",
    "    :horizon: The time horizon\n",
    "    :mc_iters: Number of Monte Carlo iterations\n",
    "    :opt_mode: either 'grad' (default) or 'grid' for rollout acquisition optimization. 'grid' is faster for low-d problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gaussian_process,\n",
    "        opt_domain,\n",
    "        horizon,\n",
    "        mc_iters=int(1e3),\n",
    "        control_variate=True,\n",
    "    ):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.gaussian_process = gaussian_process\n",
    "        self.opt_domain = opt_domain\n",
    "        self.horizon = horizon\n",
    "        self.mc_iters = mc_iters\n",
    "        self.numthreads = int(mp.cpu_count() / 2)\n",
    "        self.random_number_stream = self.low_discrepancy_points(\n",
    "            self.mc_iters\n",
    "        )  # For QMC\n",
    "\n",
    "        # Control Variates\n",
    "        self.ei = ExpectedImprovement(gaussian_process, opt_domain)\n",
    "        self.pi = ProbabilityImprovement(gaussian_process, opt_domain)\n",
    "        self.control_variate = control_variate\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        # Run on each point separately\n",
    "\n",
    "        num_points = points_to_evaluate.shape[0]\n",
    "        rollout_values = np.zeros(num_points)\n",
    "        for i in range(0, num_points):\n",
    "            rollout_values[i] = self._evaluate_at_point_list(points_to_evaluate[[i], :])\n",
    "        return rollout_values\n",
    "\n",
    "    # Iterate through point list, parallelizing MC\n",
    "    def _evaluate_at_point_list(self, point_to_evaluate):\n",
    "        \"\"\"\n",
    "        Does this with a fixed sequence of random numbers (to decrease variance via common random numbers)\n",
    "        :param point_to_evaluate:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.point_current = point_to_evaluate\n",
    "        pool = Pool(processes=self.numthreads)\n",
    "        rewards = pool.map(self.draw_from_policy, self.random_number_stream)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Sum up samples to get estimate\n",
    "        value_variates = np.sum(rewards) / self.mc_iters\n",
    "\n",
    "        # Remove effects of control variates\n",
    "        ei_value = self.ei.evaluate_at_point_list(point_to_evaluate)\n",
    "        pi_value = self.pi.evaluate_at_point_list(point_to_evaluate)\n",
    "        if self.control_variate:\n",
    "            value_corrected = value_variates + ei_value\n",
    "        else:\n",
    "            value_corrected = value_variates\n",
    "        return value_corrected\n",
    "\n",
    "    # Execute policy once using sequential draws from GPs\n",
    "    def draw_from_policy(self, random_number_list):\n",
    "        \"\"\"\n",
    "        Performs a draw from the policy, given a fixed random series of numbers\n",
    "        :param random_number_list: fixed random series of numbers, of size h, assumed unit normal distributed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.horizon > 1, \"Horizon must be greater than 1 to calculate MC\"\n",
    "\n",
    "        gp_temp = copy.deepcopy(self.gaussian_process)\n",
    "        xi = self.point_current\n",
    "        h = self.horizon\n",
    "        rewards = np.zeros(h)\n",
    "\n",
    "        # Optimize acquisition function and set new sample point\n",
    "        acquisition = ExpectedImprovement(gp_temp, self.opt_domain)\n",
    "\n",
    "        for i in range(self.horizon):\n",
    "            fi = gp_temp.sample_single(xi, random_number_list[i])\n",
    "\n",
    "            if i < self.horizon - 1:\n",
    "                rewards[i] = self.reward(gp_temp, fi)\n",
    "            else:\n",
    "                if self.control_variate:\n",
    "                    rewards[i] = acquisition.evaluate_at_point_list(xi)\n",
    "                else:\n",
    "                    rewards[i] = self.reward(gp_temp, fi)\n",
    "\n",
    "            gp_temp.chol_update(xi, fi)\n",
    "            xi = acquisition.next_point_grid(num_grid_points=40)\n",
    "\n",
    "        # Add in control variates (EI, PI)... in this case, an EI control variate\n",
    "        # removes the first term, and a PI control variate adds 1 if there is\n",
    "        # improvement in the first sample\n",
    "        r = np.sum(rewards)\n",
    "        if self.control_variate:\n",
    "            r -= rewards[0]  # EI Control Variate\n",
    "        # if rewards[0] > 0:\n",
    "        #     r -= 1\n",
    "        return r\n",
    "\n",
    "    def reward(self, gp, fi):\n",
    "        _, ytrain = gp.get_historical_data()\n",
    "        ymin = np.min(ytrain)\n",
    "        r = max(ymin - float(fi), 0)\n",
    "        return r\n",
    "\n",
    "    def next_point(self):\n",
    "        \"\"\"\n",
    "        If self horizon is 1, just return the max of regular EI\n",
    "        \"\"\"\n",
    "        if self.horizon == 1:\n",
    "            ei = ExpectedImprovement(self.gaussian_process, self.opt_domain)\n",
    "            return ei.next_point_grid()\n",
    "        else:\n",
    "            optimizer = BayesOptAcquisitionOptimizer(\n",
    "                self.gaussian_process, self.evaluate_at_point_list, self.opt_domain\n",
    "            )\n",
    "            return optimizer.get_sample_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SMuvwh-eLc7M"
   },
   "outputs": [],
   "source": [
    "# @title ucb.py\n",
    "BASE_KAPPA = 1\n",
    "\n",
    "\n",
    "class UpperConfidenceBound(AcquisitionFunctionInterface):\n",
    "    def __init__(self, gaussian_process, opt_domain, **kwargs):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.kappa = kwargs.get(\"base_kappa\", BASE_KAPPA)\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        mean, variance = self.gaussian_process.compute_mean_and_variance_of_points(\n",
    "            points_to_evaluate\n",
    "        )\n",
    "        return -mean - self.kappa * np.sqrt(variance)\n",
    "\n",
    "    def joint_function_gradient_eval(self, points_to_evaluate):\n",
    "        points_to_evaluate = np.atleast_2d(points_to_evaluate)\n",
    "        (\n",
    "            mean,\n",
    "            sqrt_var,\n",
    "            _,\n",
    "            _,\n",
    "            grad_mean,\n",
    "            _,\n",
    "            grad_sqrt_var,\n",
    "        ) = self.gaussian_process.components(points_to_evaluate)\n",
    "        ucb = -mean + self.kappa * sqrt_var\n",
    "        ucb_grad = -grad_mean - self.kappa * grad_sqrt_var\n",
    "        return -ucb, -ucb_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zlsex43SLKmK"
   },
   "outputs": [],
   "source": [
    "# @title rollout_portfolio.py\n",
    "MC_ITERS = 200\n",
    "\n",
    "# The portfolio of acquisiton functions is a dict mapping acquisitions to a list of kwargs to consider. We consider\n",
    "# EI and the UCB class of acquisition functions.\n",
    "PORTFOLIO = {\n",
    "    ExpectedImprovement: [{}],\n",
    "    UpperConfidenceBound: [\n",
    "        {\"base_kappa\": 0},\n",
    "        {\"base_kappa\": 1},\n",
    "        {\"base_kappa\": 2},\n",
    "        {\"base_kappa\": 4},\n",
    "        {\"base_kappa\": 8},\n",
    "    ],\n",
    "    KnowledgeGradient: [{}],\n",
    "}\n",
    "\n",
    "\n",
    "class RolloutPortfolio(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    Choosing an acquisition function with Rollout. Consider: UCB for constants 1 through 5, EI, KG\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gaussian_process, opt_domain, horizon):\n",
    "        assert horizon > 0, \"Horizon must be greater than 0\"\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.horizon = horizon\n",
    "        self.numthreads = int(mp.cpu_count() / 2)\n",
    "\n",
    "    def next_point(self):\n",
    "        x_next = None\n",
    "        acquisition_chosen = None\n",
    "        reward_max = -np.inf\n",
    "        for portfolio_element, kwargs_list in PORTFOLIO.items():\n",
    "            for kwargs in kwargs_list:\n",
    "                acquisition = portfolio_element(\n",
    "                    self.gaussian_process, self.opt_domain, **kwargs\n",
    "                )\n",
    "                x_opt = acquisition.next_point()\n",
    "\n",
    "                # Roll out EI on KG argmax first to see what happens!\n",
    "                if portfolio_element is KnowledgeGradient:\n",
    "                    acquisition_rollout = RolloutPortfolioElement(\n",
    "                        self.gaussian_process,\n",
    "                        self.opt_domain,\n",
    "                        self.horizon,\n",
    "                        ExpectedImprovement,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                else:\n",
    "                    acquisition_rollout = RolloutPortfolioElement(\n",
    "                        self.gaussian_process,\n",
    "                        self.opt_domain,\n",
    "                        self.horizon,\n",
    "                        portfolio_element,\n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "                reward = acquisition_rollout.evaluate_at_point_list(x_opt)\n",
    "                if reward > reward_max:\n",
    "                    acquisition_chosen = str(portfolio_element.__name__)\n",
    "                    if kwargs:\n",
    "                        acquisition_chosen += str(kwargs[\"base_kappa\"])\n",
    "                    x_next = x_opt\n",
    "                    reward_max = reward\n",
    "        return x_next, acquisition_chosen\n",
    "\n",
    "\n",
    "class RolloutPortfolioEI(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    Chooses an acquisition function with Rollout. Consider: UCB for constants 1 through 5, EI, KG\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gaussian_process, opt_domain, horizon):\n",
    "        assert horizon > 0, \"Horizon must be greater than 0\"\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        self.horizon = horizon\n",
    "        self.numthreads = int(mp.cpu_count() / 2)\n",
    "\n",
    "    def next_point(self):\n",
    "        x_next = None\n",
    "        acquisition_chosen = None\n",
    "        reward_max = -np.inf\n",
    "        for portfolio_element, kwargs_list in PORTFOLIO.items():\n",
    "            for kwargs in kwargs_list:\n",
    "                acquisition = portfolio_element(\n",
    "                    self.gaussian_process, self.opt_domain, **kwargs\n",
    "                )\n",
    "                x_opt = acquisition.next_point()\n",
    "\n",
    "                # Roll out EI on KG argmax first to see what happens!\n",
    "                acquisition_rollout = RolloutPortfolioElement(\n",
    "                    self.gaussian_process,\n",
    "                    self.opt_domain,\n",
    "                    self.horizon,\n",
    "                    ExpectedImprovement,\n",
    "                    **kwargs\n",
    "                )\n",
    "                reward = acquisition_rollout.evaluate_at_point_list(x_opt)\n",
    "                if reward > reward_max:\n",
    "                    acquisition_chosen = str(portfolio_element.__name__)\n",
    "                    if kwargs:\n",
    "                        acquisition_chosen += str(kwargs[\"base_kappa\"])\n",
    "                    x_next = x_opt\n",
    "                    reward_max = reward\n",
    "        return x_next, acquisition_chosen\n",
    "\n",
    "\n",
    "class RolloutPortfolioElement(AcquisitionFunctionInterface):\n",
    "    \"\"\"\n",
    "    Rolls out a specific acquisition function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gaussian_process, opt_domain, horizon, acquisition, **kwargs):\n",
    "        super().__init__(gaussian_process, opt_domain)\n",
    "        assert horizon > 0, \"Horizon must be greater than 0\"\n",
    "        self.horizon = horizon\n",
    "        self.mc_iters = MC_ITERS\n",
    "        self.numthreads = 4\n",
    "        self.acquisition = acquisition\n",
    "\n",
    "    def evaluate_at_point_list(self, points_to_evaluate):\n",
    "        num_points = points_to_evaluate.shape[0]\n",
    "        rollout_values = np.zeros(num_points)\n",
    "        for i in range(0, num_points):\n",
    "            rollout_values[i] = self._evaluate_at_point_list(points_to_evaluate[[i], :])\n",
    "        return rollout_values\n",
    "\n",
    "    # Iterate through point list, parallelizing MC\n",
    "    def _evaluate_at_point_list(self, point_to_evaluate):\n",
    "        self.point_current = point_to_evaluate\n",
    "        random_number_stream = self.low_discrepancy_points(self.mc_iters)\n",
    "        pool = Pool(processes=self.numthreads)\n",
    "        rewards = pool.map(self.draw_from_policy, random_number_stream)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Sum up samples to get estimate\n",
    "        value_variates = np.sum(rewards) / self.mc_iters\n",
    "        return value_variates\n",
    "\n",
    "    def random_points(self, num):\n",
    "        \"\"\"\n",
    "        Random points distributed w.r.t. unit gaussian, fixed according to a seed.\n",
    "        \"\"\"\n",
    "        np.random.seed(1234)\n",
    "        return list(np.random.randn(num, self.horizon))\n",
    "\n",
    "    # Execute policy once using sequential draws from GPs\n",
    "    def draw_from_policy(self, random_number_list):\n",
    "        \"\"\"\n",
    "        Performs a draw from the policy, given a fixed random series of numbers\n",
    "        :param random_number_list: fixed random series of numbers, of size h, assumed unit normal distributed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.horizon > 1, \"Horizon must be greater than 1 to calculate MC\"\n",
    "\n",
    "        gp_temp = copy.deepcopy(self.gaussian_process)\n",
    "        xi = self.point_current\n",
    "        h = self.horizon\n",
    "        rewards = np.zeros(h)\n",
    "\n",
    "        # Optimize acquisition function and set new sample point\n",
    "        acquisition_ei = ExpectedImprovement(gp_temp, self.opt_domain)\n",
    "        acquisition = self.acquisition(gp_temp, self.opt_domain)\n",
    "\n",
    "        for i in range(self.horizon):\n",
    "            fi = gp_temp.sample_single(xi, random_number_list[i])\n",
    "\n",
    "            if i < self.horizon - 1:\n",
    "                rewards[i] = self.reward(gp_temp, fi)\n",
    "            else:\n",
    "                rewards[i] = acquisition_ei.evaluate_at_point_list(xi)\n",
    "            gp_temp.chol_update(xi, fi)\n",
    "            xi = acquisition.next_point()\n",
    "\n",
    "        r = np.sum(rewards)\n",
    "        return r\n",
    "\n",
    "    def reward(self, gp, fi):\n",
    "        _, ytrain = gp.get_historical_data()\n",
    "        ymin = np.min(ytrain)\n",
    "        r = max(ymin - float(fi), 0)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "phcSusKrLgqJ"
   },
   "outputs": [],
   "source": [
    "# @title utils.py\n",
    "\n",
    "\n",
    "# Takes min of run information\n",
    "def process_run(yhist):\n",
    "    for i in range(1, yhist.shape[0]):\n",
    "        yhist[i] = np.min(yhist[0 : i + 1])\n",
    "    return yhist\n",
    "\n",
    "\n",
    "def filter_cand_points(acq_function, cand_points):\n",
    "    vals = acq_function.evaluate_at_point_list(cand_points)\n",
    "\n",
    "    # calculate particular information about vals\n",
    "    median_vals = np.median(vals)\n",
    "    return cand_points[vals > median_vals, :]\n",
    "\n",
    "\n",
    "# creates a reshaped meshgrid matrix of size n^d x d\n",
    "# where n = size_per_dimension\n",
    "def unit_grid_points(size_per_dimension, d):\n",
    "    a = np.linspace(0, 1, size_per_dimension)\n",
    "    A = (a,) * d\n",
    "    return np.dstack(np.meshgrid(*A)).ravel(\"F\").reshape(len(A), -1).T\n",
    "\n",
    "\n",
    "# Generates uniform grid of size_per_dimension x size_per_dimesion,\n",
    "# reshaped into proper dimensions\n",
    "def unit_grid_points2(size_per_dimension):\n",
    "    x = np.linspace(0, 1, size_per_dimension)\n",
    "    X1, X2 = np.meshgrid(x, x)\n",
    "    X1vec = np.ndarray.flatten(X1)\n",
    "    X2vec = np.ndarray.flatten(X2)\n",
    "    X = np.stack((X1vec, X2vec), axis=1)\n",
    "    return X, X1, X2\n",
    "\n",
    "\n",
    "# A check if points are inside unit cube or not\n",
    "def in_unit_cube(points):\n",
    "    if points.shape[1] > 2:\n",
    "        raise Exception(\"Only used when d < 3\")\n",
    "\n",
    "    if points.shape[1] == 2:\n",
    "        return (\n",
    "            all(points[:, 0] >= 0)\n",
    "            and all(points[:, 0] <= 1)\n",
    "            and all(points[:, 1] >= 0)\n",
    "            and all(points[:, 1] <= 1)\n",
    "        )\n",
    "    elif points.shape[1] == 1:\n",
    "        return all(points[:, 0] >= 0) and all(points[:, 0] <= 1)\n",
    "\n",
    "\n",
    "# A simple normpdf function if\n",
    "# we don't want to use the scipy one\n",
    "def normpdf(x, mean, sd):\n",
    "    var = float(sd) ** 2\n",
    "    denom = (2 * np.pi * var) ** 0.5\n",
    "    num = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "def slhd(num_pts, dim, seed=None):\n",
    "    # Symmetric Latin Hypercube initial design\n",
    "    # Implementation is found in PySOT:\n",
    "    #\n",
    "    # D. Eriksson and D. Bindel, PySOT,\n",
    "    # https://github.com/dme65/pySOT\n",
    "    # Fix the seed if necessary\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Generate a one-dimensional array based on sample number\n",
    "    points = np.zeros([num_pts, dim])\n",
    "    points[:, 0] = np.arange(1, num_pts + 1)\n",
    "\n",
    "    # Get the last index of the row in the top half of the hypercube\n",
    "    middleind = num_pts // 2\n",
    "\n",
    "    # special manipulation if odd number of rows\n",
    "    if num_pts % 2 == 1:\n",
    "        points[middleind, :] = middleind + 1\n",
    "\n",
    "    # Generate the top half of the hypercube matrix\n",
    "    for j in range(1, dim):\n",
    "        for i in range(middleind):\n",
    "            if np.random.random() < 0.5:\n",
    "                points[i, j] = num_pts - i\n",
    "            else:\n",
    "                points[i, j] = i + 1\n",
    "        np.random.shuffle(points[:middleind, j])\n",
    "\n",
    "    # Generate the bottom half of the hypercube matrix\n",
    "    for i in range(middleind, num_pts):\n",
    "        points[i, :] = num_pts + 1 - points[num_pts - 1 - i, :]\n",
    "\n",
    "    # Get new random seed:\n",
    "    t = 1000 * time.time()  # current time in milliseconds\n",
    "    np.random.seed(int(t) % 2**32)\n",
    "\n",
    "    return (points - 1) / (num_pts - 1)  # Map to [0, 1]^d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqAWyNBLPIeA"
   },
   "source": [
    "## lookahead/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2QLEevzDPU-3"
   },
   "outputs": [],
   "source": [
    "# @title _gaussian_process.py\n",
    "MINIMUM_STD_DEV_GRAD_CHOLESKY = np.finfo(np.float64).eps\n",
    "MINIMUM_VARIANCE_GRAD_EI = 150 * MINIMUM_STD_DEV_GRAD_CHOLESKY**2\n",
    "MINIMUM_KRIGING_VARIANCE = 1e-100  # Just something really small\n",
    "\n",
    "\n",
    "class GaussianProcessDataInterface(object):\n",
    "    def __init__(self, covariance, historical_data):\n",
    "        assert isinstance(covariance, CovarianceInterface)\n",
    "        assert isinstance(historical_data, HistoricalData)\n",
    "        assert covariance.dim == historical_data.dim\n",
    "        self.covariance = copy.deepcopy(covariance)\n",
    "        self.historical_data = copy.deepcopy(historical_data)\n",
    "        assert self.num_sampled > 0\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.historical_data.dim\n",
    "\n",
    "    @property\n",
    "    def num_sampled(self):\n",
    "        return self.historical_data.num_sampled\n",
    "\n",
    "    @property\n",
    "    def differentiable(self):\n",
    "        return isinstance(self.covariance, DifferentiableCovariance)\n",
    "\n",
    "    @property\n",
    "    def points_sampled(self):\n",
    "        return self.historical_data.points_sampled\n",
    "\n",
    "    @property\n",
    "    def points_sampled_value(self):\n",
    "        return self.historical_data.points_sampled_value\n",
    "\n",
    "    @property\n",
    "    def points_sampled_noise_variance(self):\n",
    "        return self.historical_data.points_sampled_noise_variance\n",
    "\n",
    "\n",
    "class GaussianProcess(GaussianProcessDataInterface):\n",
    "    def __init__(self, covariance, historical_data, tikhonov_param=None):\n",
    "        super().__init__(\n",
    "            covariance=covariance,\n",
    "            historical_data=historical_data,\n",
    "        )\n",
    "        self.tikhonov_param = tikhonov_param\n",
    "\n",
    "        self.K_chol = None\n",
    "        self.K_inv_y = None\n",
    "\n",
    "        self.build_precomputed_data()\n",
    "\n",
    "    def build_precomputed_data(self):\n",
    "        if self.num_sampled == 0:\n",
    "            self.K_chol = np.array([])\n",
    "            self.K_inv_y = np.array([])\n",
    "        else:\n",
    "            if self.tikhonov_param is not None:\n",
    "                noise_diag_vector = np.full(self.num_sampled, self.tikhonov_param)\n",
    "            else:\n",
    "                noise_diag_vector = self.points_sampled_noise_variance\n",
    "            kernel_matrix = self.covariance.build_kernel_matrix(\n",
    "                self.points_sampled,\n",
    "                noise_variance=noise_diag_vector,\n",
    "            )\n",
    "            self.K_chol = cho_factor(kernel_matrix, lower=True, overwrite_a=True)\n",
    "            self.K_inv_y = cho_solve(self.K_chol, self.points_sampled_value)\n",
    "\n",
    "    def _compute_core_posterior_components(self, points_to_sample, option):\n",
    "        K_eval = grad_K_eval = cardinal_functions_at_points_to_sample = None\n",
    "        if option in (\"K_eval\", \"all\"):\n",
    "            K_eval = self.covariance.build_kernel_matrix(\n",
    "                self.points_sampled, points_to_sample=points_to_sample\n",
    "            )\n",
    "        if option in (\"grad_K_eval\", \"all\"):\n",
    "            grad_K_eval = self.covariance.build_kernel_grad_tensor(\n",
    "                self.points_sampled, points_to_sample=points_to_sample\n",
    "            )\n",
    "        if option == \"all\":\n",
    "            cardinal_functions_at_points_to_sample = cho_solve(self.K_chol, K_eval.T).T\n",
    "        return K_eval, grad_K_eval, cardinal_functions_at_points_to_sample\n",
    "\n",
    "    def compute_mean_of_points(self, points_to_sample):\n",
    "        K_eval, _, _ = self._compute_core_posterior_components(\n",
    "            points_to_sample, \"K_eval\"\n",
    "        )\n",
    "        return self._compute_mean_of_points(points_to_sample, K_eval)\n",
    "\n",
    "    def _compute_mean_of_points(self, points_to_sample, K_eval):\n",
    "        return np.dot(K_eval, self.K_inv_y)\n",
    "\n",
    "    def compute_variance_of_points(self, points_to_sample):\n",
    "        K_eval, _, _ = self._compute_core_posterior_components(\n",
    "            points_to_sample, \"K_eval\"\n",
    "        )\n",
    "        return self._compute_variance_of_points(points_to_sample, K_eval)\n",
    "\n",
    "    def _compute_variance_of_points(\n",
    "        self, points_to_sample, K_eval, cardinal_functions_at_points_to_sample=None\n",
    "    ):\n",
    "        K_x_x_array = self.covariance.covariance(points_to_sample, points_to_sample)\n",
    "        if cardinal_functions_at_points_to_sample is None:\n",
    "            V = solve_triangular(\n",
    "                self.K_chol[0],\n",
    "                K_eval.T,\n",
    "                lower=self.K_chol[1],\n",
    "                overwrite_b=True,\n",
    "            )\n",
    "            schur_complement_component = np.sum(V**2, axis=0)\n",
    "        else:\n",
    "            schur_complement_component = np.sum(\n",
    "                K_eval * cardinal_functions_at_points_to_sample, axis=1\n",
    "            )\n",
    "        return np.fmax(\n",
    "            MINIMUM_KRIGING_VARIANCE, K_x_x_array - schur_complement_component\n",
    "        )\n",
    "\n",
    "    def compute_mean_and_variance_of_points(self, points_to_sample):\n",
    "        K_eval, _, _ = self._compute_core_posterior_components(\n",
    "            points_to_sample, \"K_eval\"\n",
    "        )\n",
    "        mean = self._compute_mean_of_points(points_to_sample, K_eval)\n",
    "        var = self._compute_variance_of_points(points_to_sample, K_eval)\n",
    "        return mean, var\n",
    "\n",
    "    def compute_grad_mean_of_points(self, points_to_sample):\n",
    "        _, grad_K_eval, _ = self._compute_core_posterior_components(\n",
    "            points_to_sample, \"grad_K_eval\"\n",
    "        )\n",
    "        return self._compute_grad_mean_of_points(points_to_sample, grad_K_eval)\n",
    "\n",
    "    def _compute_grad_mean_of_points(self, points_to_sample, grad_K_eval):\n",
    "        return numpy.einsum(\"ijk, j\", grad_K_eval, self.K_inv_y)\n",
    "\n",
    "    def compute_grad_variance_of_points(self, points_to_sample):\n",
    "        _, grad_K_eval, cardinal_functions = self._compute_core_posterior_components(\n",
    "            points_to_sample, \"all\"\n",
    "        )\n",
    "        return self._compute_grad_variance_of_points(grad_K_eval, cardinal_functions)\n",
    "\n",
    "    def _compute_grad_variance_of_points(\n",
    "        self, grad_K_eval, cardinal_functions_at_points_to_sample\n",
    "    ):\n",
    "        if not self.covariance.translation_invariant:\n",
    "            raise NotImplementedError(\"Not yet ready for general kernels.\")\n",
    "\n",
    "        return -2 * np.sum(\n",
    "            grad_K_eval * cardinal_functions_at_points_to_sample[:, :, np.newaxis],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    def compute_mean_variance_grad_of_points(self, points_to_sample):\n",
    "        (\n",
    "            K_eval,\n",
    "            grad_K_eval,\n",
    "            cardinal_functions,\n",
    "        ) = self._compute_core_posterior_components(points_to_sample, \"all\")\n",
    "\n",
    "        mean = self._compute_mean_of_points(points_to_sample, K_eval)\n",
    "        var = self._compute_variance_of_points(\n",
    "            points_to_sample, K_eval, cardinal_functions\n",
    "        )\n",
    "        grad_mean = self._compute_grad_mean_of_points(points_to_sample, grad_K_eval)\n",
    "        grad_var = self._compute_grad_variance_of_points(\n",
    "            grad_K_eval, cardinal_functions\n",
    "        )\n",
    "\n",
    "        return mean, var, grad_mean, grad_var\n",
    "\n",
    "    def compute_covariance_of_points(self, points_to_sample):\n",
    "        K_eval_var = self.covariance.build_kernel_matrix(points_to_sample)\n",
    "        if self.num_sampled == 0:\n",
    "            return np.diag(np.diag(K_eval_var))\n",
    "\n",
    "        K_eval = self.covariance.build_kernel_matrix(\n",
    "            self.points_sampled, points_to_sample=points_to_sample\n",
    "        )\n",
    "        V = solve_triangular(\n",
    "            self.K_chol[0],\n",
    "            K_eval.T,\n",
    "            lower=self.K_chol[1],\n",
    "            overwrite_b=True,\n",
    "        )\n",
    "\n",
    "        return K_eval_var - np.dot(V.T, V)\n",
    "\n",
    "    def cross_correlation_for_samples(self, points_being_sampled, eval_domain_points):\n",
    "        all_k_x_vectors = self.covariance.build_kernel_matrix(\n",
    "            self.points_sampled, points_being_sampled\n",
    "        )\n",
    "        all_k_xp_vectors = self.covariance.build_kernel_matrix(\n",
    "            self.points_sampled, eval_domain_points\n",
    "        )\n",
    "\n",
    "        V_k = solve_triangular(\n",
    "            self.K_chol[0],\n",
    "            all_k_x_vectors.T,\n",
    "            lower=self.K_chol[1],\n",
    "            overwrite_b=True,\n",
    "        )\n",
    "        V_kprime = solve_triangular(\n",
    "            self.K_chol[0],\n",
    "            all_k_xp_vectors.T,\n",
    "            lower=self.K_chol[1],\n",
    "            overwrite_b=True,\n",
    "        )\n",
    "        K_x_xprime = self.covariance.build_kernel_matrix(\n",
    "            points_being_sampled, eval_domain_points\n",
    "        )\n",
    "        cross_corr = K_x_xprime - np.dot(V_kprime.T, V_k)\n",
    "\n",
    "        return cross_corr\n",
    "\n",
    "    def draw_posterior_samples_of_points(self, num_samples, points_to_sample):\n",
    "        mean = self.compute_mean_of_points(points_to_sample)\n",
    "        cov = self.compute_covariance_of_points(points_to_sample)\n",
    "        L = compute_cholesky_for_gp_sampling(cov)\n",
    "\n",
    "        # z_samples is an array with shape (num_points, num_samples)\n",
    "        z_samples = np.atleast_2d(np.random.normal(size=(len(mean), num_samples)))\n",
    "        return mean[np.newaxis, :] + np.transpose(np.dot(L, z_samples))\n",
    "\n",
    "    def draw_posterior_samples(self, num_samples):\n",
    "        return self.draw_posterior_samples_of_points(num_samples, self.points_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1IJcHvTTPpWe"
   },
   "outputs": [],
   "source": [
    "# @title covariance.py\n",
    "\n",
    "\n",
    "class CovarianceInterface(object, metaclass=ABCMeta):\n",
    "    process_variance = None\n",
    "\n",
    "    @property\n",
    "    def num_hyperparameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def translation_invariant(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    def get_hyperparameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def set_hyperparameters(self, hyperparameters):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    hyperparameters = property(get_hyperparameters, set_hyperparameters)\n",
    "\n",
    "    def _covariance(self, x, z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def covariance(self, x, z):\n",
    "        assert len(x.shape) == len(z.shape) == 2\n",
    "        n, d = x.shape\n",
    "        assert n == z.shape[0]\n",
    "        assert self.dim == d == z.shape[1]\n",
    "\n",
    "        covariance_vector = self._covariance(x, z)\n",
    "        assert len(covariance_vector) == n\n",
    "\n",
    "        return self.process_variance * covariance_vector\n",
    "\n",
    "    def _build_kernel_matrix(self, points_sampled, points_to_sample=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_kernel_matrix(\n",
    "        self, points_sampled, points_to_sample=None, noise_variance=None\n",
    "    ):\n",
    "        kernel_matrix = self.process_variance * self._build_kernel_matrix(\n",
    "            points_sampled, points_to_sample\n",
    "        )\n",
    "\n",
    "        if noise_variance is not None:\n",
    "            nx, nz = kernel_matrix.shape\n",
    "            assert (\n",
    "                nx == nz\n",
    "            )  # Or else there should be no noise_variance term because it would be meaningless\n",
    "            kernel_matrix.flat[:: nx + 1] += noise_variance\n",
    "        return kernel_matrix\n",
    "\n",
    "\n",
    "class DifferentiableCovariance(CovarianceInterface):\n",
    "    def _grad_covariance(self, x, z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad_covariance(self, x, z):\n",
    "        assert len(x.shape) == len(z.shape) == 2\n",
    "        n, d = x.shape\n",
    "        assert n == z.shape[0]\n",
    "        assert self.dim == d == z.shape[1]\n",
    "\n",
    "        grad_covariance_vector = self._grad_covariance(x, z)\n",
    "        assert grad_covariance_vector.shape == (n, d)\n",
    "\n",
    "        return self.process_variance * grad_covariance_vector\n",
    "\n",
    "    def _hyperparameter_grad_covariance_without_process_variance(self, x, z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def hyperparameter_grad_covariance(self, x, z):\n",
    "        assert len(x.shape) == len(z.shape) == 2\n",
    "        n, d = x.shape\n",
    "        assert n == z.shape[0]\n",
    "        assert self.dim == d == z.shape[1]\n",
    "\n",
    "        hyperparameter_grad_covariance = np.empty((n, self.num_hyperparameters))\n",
    "        hyperparameter_grad_covariance[:, 0] = self._covariance(x, z)\n",
    "        hyperparameter_grad_covariance[:, 1:] = (\n",
    "            self.process_variance\n",
    "            * self._hyperparameter_grad_covariance_without_process_variance(x, z)\n",
    "        )\n",
    "\n",
    "        return hyperparameter_grad_covariance\n",
    "\n",
    "    def _build_kernel_grad_tensor(self, points_sampled, points_to_sample=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build_kernel_grad_tensor(self, points_sampled, points_to_sample=None):\n",
    "        return self.process_variance * self._build_kernel_grad_tensor(\n",
    "            points_sampled, points_to_sample\n",
    "        )\n",
    "\n",
    "    def _build_kernel_hparam_grad_tensor_without_process_variance(\n",
    "        self, points_sampled, points_to_sample=None\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build_kernel_hparam_grad_tensor(self, points_sampled, points_to_sample=None):\n",
    "        n_cols, _ = points_sampled.shape\n",
    "        n_rows = n_cols if points_to_sample is None else len(points_to_sample)\n",
    "\n",
    "        kg_tensor = np.empty((n_rows, n_cols, self.num_hyperparameters))\n",
    "        kg_tensor[:, :, 0] = self._build_kernel_matrix(points_sampled, points_to_sample)\n",
    "        kg_tensor[:, :, 1:] = (\n",
    "            self.process_variance\n",
    "            * self._build_kernel_hparam_grad_tensor_without_process_variance(\n",
    "                points_sampled, points_to_sample\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return kg_tensor\n",
    "\n",
    "\n",
    "class RadialCovariance(CovarianceInterface):\n",
    "    def __init__(self, hyperparameters):\n",
    "        self._hyperparameters = None\n",
    "        self._length_scales = None\n",
    "        self._length_scales_squared = None\n",
    "        self._length_scales_cubed = None\n",
    "        self.process_variance = None\n",
    "        self.set_hyperparameters(hyperparameters)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__}_{self.dim}({self.hyperparameters})\"\n",
    "\n",
    "    def check_hyperparameters_are_valid(self, new_hyperparameters):\n",
    "        new_hyperparameters = np.asarray(new_hyperparameters, dtype=float)\n",
    "        assert (\n",
    "            len(new_hyperparameters.shape) == 1\n",
    "        ), f\"Hyperparameters should be in 1D array, not {new_hyperparameters}\"\n",
    "        assert np.all(\n",
    "            new_hyperparameters > 0\n",
    "        ), f\"For {self.__class__.__name__}, all hyperparameters must be positive\"\n",
    "        return new_hyperparameters\n",
    "\n",
    "    @property\n",
    "    def num_hyperparameters(self):\n",
    "        return self._hyperparameters.size\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return len(self._length_scales)\n",
    "\n",
    "    @property\n",
    "    def translation_invariant(self):\n",
    "        return True\n",
    "\n",
    "    def get_hyperparameters(self):\n",
    "        return np.copy(self._hyperparameters)\n",
    "\n",
    "    def set_hyperparameters(self, hyperparameters):\n",
    "        self._hyperparameters = self.check_hyperparameters_are_valid(hyperparameters)\n",
    "        self.process_variance = self._hyperparameters[0]\n",
    "        self._length_scales = np.copy(self._hyperparameters[1:])\n",
    "        self._length_scales_squared = self._length_scales**2\n",
    "        self._length_scales_cubed = self._length_scales**3\n",
    "\n",
    "    hyperparameters = property(get_hyperparameters, set_hyperparameters)\n",
    "\n",
    "    def eval_radial_kernel(self, distance_matrix_squared):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _distance_between_points(self, data, eval_points):\n",
    "        data_shape = data.shape\n",
    "        eval_shape = eval_points.shape\n",
    "        if len(data_shape) != 2 or len(eval_shape) != 2:\n",
    "            raise ValueError(\n",
    "                f\"Points must be a 2D array: data.shape = {data_shape}, eval_points.shape = {eval_shape}\"\n",
    "            )\n",
    "        elif data_shape != eval_shape:\n",
    "            raise ValueError(f\"Data size {data_shape}, Eval size {eval_shape}\")\n",
    "        elif data_shape[1] != self.dim:\n",
    "            raise ValueError(\n",
    "                f\"Points dimension {data_shape[1]}, Covariance dimension {self.dim}\"\n",
    "            )\n",
    "\n",
    "        diff_vecs = eval_points - data\n",
    "        r = np.sqrt(np.sum(np.power(diff_vecs / self._length_scales, 2), axis=1))\n",
    "        return r, diff_vecs\n",
    "\n",
    "    def _build_distance_matrix_squared(\n",
    "        self,\n",
    "        data,\n",
    "        eval_points=None,\n",
    "        build_diff_matrices=False,\n",
    "    ):\n",
    "        if eval_points is None:\n",
    "            return self._build_symmetric_distance_matrix_squared(\n",
    "                data, build_diff_matrices\n",
    "            )\n",
    "        else:\n",
    "            return self._build_nonsymmetric_distance_matrix_squared(\n",
    "                data, eval_points, build_diff_matrices\n",
    "            )\n",
    "\n",
    "    def _build_symmetric_distance_matrix_squared(self, data, build_diff_matrices):\n",
    "        diff_mats = None\n",
    "        dist_mat_sq = squareform(\n",
    "            pdist(data / self._length_scales[np.newaxis, :], \"sqeuclidean\")\n",
    "        )\n",
    "        if build_diff_matrices:\n",
    "            diff_mats = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n",
    "        return dist_mat_sq, diff_mats\n",
    "\n",
    "    def _build_nonsymmetric_distance_matrix_squared(\n",
    "        self, data, eval_points, build_diff_matrices\n",
    "    ):\n",
    "        diff_mats = None\n",
    "        x = eval_points / self._length_scales[np.newaxis, :]\n",
    "        z = data / self._length_scales[np.newaxis, :]\n",
    "        dist_mat_sq = cdist(x, z) ** 2\n",
    "        if build_diff_matrices:\n",
    "            diff_mats = eval_points[:, np.newaxis, :] - data[np.newaxis, :, :]\n",
    "        return dist_mat_sq, diff_mats\n",
    "\n",
    "    def _build_kernel_matrix(self, points_sampled, points_to_sample=None):\n",
    "        return self.eval_radial_kernel(\n",
    "            self._build_distance_matrix_squared(points_sampled, points_to_sample)[0]\n",
    "        )\n",
    "\n",
    "\n",
    "class DifferentiableRadialCovariance(DifferentiableCovariance, RadialCovariance):\n",
    "    def eval_radial_kernel_grad(self, distance_matrix_squared, difference_matrix):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def eval_radial_kernel_hparam_grad(\n",
    "        self, distance_matrix_squared, difference_matrix\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _build_kernel_grad_tensor(self, points_sampled, points_to_sample=None):\n",
    "        dm_sq, diff_mats = self._build_distance_matrix_squared(\n",
    "            points_sampled, points_to_sample, True\n",
    "        )\n",
    "        return self.eval_radial_kernel_grad(dm_sq, diff_mats)\n",
    "\n",
    "    def _build_kernel_hparam_grad_tensor_without_process_variance(\n",
    "        self, points_sampled, points_to_sample=None\n",
    "    ):\n",
    "        dm_sq, diff_mats = self._build_distance_matrix_squared(\n",
    "            points_sampled, points_to_sample, True\n",
    "        )\n",
    "        return self.eval_radial_kernel_hparam_grad(dm_sq, diff_mats)\n",
    "\n",
    "\n",
    "def _scale_difference_matrix(scale, difference_matrix):\n",
    "    return scale[:, :, np.newaxis] * difference_matrix\n",
    "\n",
    "\n",
    "class C4RadialMatern(DifferentiableRadialCovariance):\n",
    "    def __init__(self, hyperparameters):\n",
    "        super().__init__(hyperparameters)\n",
    "\n",
    "    def eval_radial_kernel(self, distance_matrix_squared):\n",
    "        r = np.sqrt(distance_matrix_squared)\n",
    "        return (1 + r + 1.0 / 3.0 * distance_matrix_squared) * np.exp(-r)\n",
    "\n",
    "    def eval_radial_kernel_grad(self, distance_matrix_squared, difference_matrix):\n",
    "        r = np.sqrt(distance_matrix_squared)\n",
    "        return (\n",
    "            _scale_difference_matrix(\n",
    "                -(1.0 / 3.0) * (1 + r) * np.exp(-r), difference_matrix\n",
    "            )\n",
    "            / self._length_scales_squared\n",
    "        )\n",
    "\n",
    "    def eval_radial_kernel_hparam_grad(\n",
    "        self, distance_matrix_squared, difference_matrix\n",
    "    ):\n",
    "        r = np.sqrt(distance_matrix_squared)\n",
    "        return (\n",
    "            _scale_difference_matrix(\n",
    "                (1.0 / 3.0) * (1 + r) * np.exp(-r), (difference_matrix**2)\n",
    "            )\n",
    "            / self._length_scales_cubed\n",
    "        )\n",
    "\n",
    "    def _covariance(self, x, z):\n",
    "        r, _ = self._distance_between_points(z, x)\n",
    "        return (1 + r + 1.0 / 3.0 * r**2) * np.exp(-r)\n",
    "\n",
    "    def _grad_covariance(self, x, z):\n",
    "        r, dm = self._distance_between_points(z, x)\n",
    "        r_2d = r[:, np.newaxis]\n",
    "        return (\n",
    "            -(1.0 / 3.0) * (1 + r_2d) * np.exp(-r_2d) * dm / self._length_scales_squared\n",
    "        )\n",
    "\n",
    "    def _hyperparameter_grad_covariance_without_process_variance(self, x, z):\n",
    "        r, dm = self._distance_between_points(z, x)\n",
    "        r_2d = r[:, np.newaxis]\n",
    "        return (\n",
    "            (1.0 / 3.0)\n",
    "            * (1 + r_2d)\n",
    "            * np.exp(-r_2d)\n",
    "            * (dm**2)\n",
    "            / self._length_scales_cubed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wsg1rZJFPy9c"
   },
   "outputs": [],
   "source": [
    "# @title domain.py\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "import numpy\n",
    "import qmcpy\n",
    "\n",
    "\n",
    "def generate_sobol_points(num_points, domain_bounds):\n",
    "    distribution = qmcpy.Sobol(dimension=len(domain_bounds))\n",
    "    pts01 = distribution.gen_samples(n=2 ** numpy.ceil(numpy.log2(num_points)))[\n",
    "        :num_points\n",
    "    ]\n",
    "    pts_scale = numpy.array([domain.length for domain in domain_bounds])\n",
    "    pts_min = numpy.array([domain.min for domain in domain_bounds])\n",
    "    return pts_min + pts_scale * pts01\n",
    "\n",
    "\n",
    "class ClosedInterval(namedtuple(\"BaseInterval\", (\"min\", \"max\"))):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.min},{self.max})\"\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return self.min <= key <= self.max\n",
    "\n",
    "    def __nonzero__(self):\n",
    "        return self.min <= self.max\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.__nonzero__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.__class__ == other.__class__\n",
    "            and self.min == other.min\n",
    "            and self.max == other.max\n",
    "        )\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.min, self.max))\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.max - self.min\n",
    "\n",
    "    def is_inside(self, value):\n",
    "        return self.__contains__(value)\n",
    "\n",
    "    def is_valid(self):\n",
    "        return self.max >= self.min\n",
    "\n",
    "\n",
    "class TensorProductDomain(object):\n",
    "    def __init__(self, domain_bounds):\n",
    "        \"\"\"Construct a TensorProductDomain with the specified bounds defined using\n",
    "        a list of ClosedInterval objects.\n",
    "        \"\"\"\n",
    "        self.domain_bounds = copy.deepcopy(domain_bounds)\n",
    "\n",
    "        for interval in self.domain_bounds:\n",
    "            if not interval.is_valid():\n",
    "                raise ValueError(\"Tensor product region is EMPTY.\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TensorProductDomain({self.domain_bounds})\"\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return len(self.domain_bounds)\n",
    "\n",
    "    def check_point_inside(self, point):\n",
    "        return all(\n",
    "            [\n",
    "                interval.is_inside(point[i])\n",
    "                for i, interval in enumerate(self.domain_bounds)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def check_point_acceptable(self, point):\n",
    "        assert len(point) == self.dim\n",
    "        return self.check_point_inside(point)\n",
    "\n",
    "    def get_bounding_box(self):\n",
    "        return copy.copy(self.domain_bounds)\n",
    "\n",
    "    def generate_quasi_random_points_in_domain(self, num_points, log_sample=False):\n",
    "        r\"\"\"Generate quasi-random points in the domain.\n",
    "        :param num_points: max number of points to generate\n",
    "        :type num_points: int >= 0\n",
    "        :param log_sample: sample logarithmically spaced points\n",
    "        :type log_sample: bool\n",
    "        :return: uniform random sampling of points from the domain\n",
    "        :rtype: array of float64 with shape (num_points, dim)\n",
    "        \"\"\"\n",
    "        domain_bounds = self.domain_bounds\n",
    "        if log_sample:\n",
    "            domain_bounds = [\n",
    "                ClosedInterval(numpy.log(a), numpy.log(b)) for (a, b) in domain_bounds\n",
    "            ]\n",
    "\n",
    "        points = generate_sobol_points(num_points, domain_bounds)\n",
    "        return numpy.exp(points) if log_sample else points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wUZlKBneQSC2"
   },
   "outputs": [],
   "source": [
    "# @title parametrization.py\n",
    "\n",
    "DEFAULT_TIKHONOV_PARAMETER = 1.0e-10\n",
    "\n",
    "\n",
    "class GaussianProcessLogMarginalLikelihood(GaussianProcessDataInterface):\n",
    "    def __init__(\n",
    "        self,\n",
    "        covariance,\n",
    "        historical_data,\n",
    "        log_domain=False,\n",
    "        tikhonov_parameter=DEFAULT_TIKHONOV_PARAMETER,\n",
    "    ):\n",
    "        super().__init__(covariance, historical_data)\n",
    "        self.log_domain = log_domain\n",
    "        self.tikhonov_parameter = tikhonov_parameter\n",
    "        self.gp = GaussianProcess(\n",
    "            self.covariance,\n",
    "            self.historical_data,\n",
    "            tikhonov_param=self.tikhonov_parameter,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_hyperparameters(self):\n",
    "        return self.covariance.num_hyperparameters\n",
    "\n",
    "    @property\n",
    "    def tikhonov_param(self):\n",
    "        return self.gp.tikhonov_param\n",
    "\n",
    "    def get_hyperparameters(self):\n",
    "        hyperparameters = self.covariance.hyperparameters\n",
    "        return np.log(hyperparameters) if self.log_domain else hyperparameters\n",
    "\n",
    "    def set_hyperparameters(self, hyperparameters):\n",
    "        hp_linear_domain = (\n",
    "            np.exp(hyperparameters) if self.log_domain else hyperparameters\n",
    "        )\n",
    "        # Don't pass the noise term in covariance.hyperparameters since we pass it as a separate param\n",
    "        self.covariance.hyperparameters = hp_linear_domain[: self.dim + 1]\n",
    "        self.gp = GaussianProcess(\n",
    "            self.covariance, self.historical_data, self.tikhonov_parameter\n",
    "        )\n",
    "\n",
    "    hyperparameters = property(get_hyperparameters, set_hyperparameters)\n",
    "    current_point = hyperparameters\n",
    "\n",
    "    def compute_objective_function(self):\n",
    "        y_Pb = self.gp.points_sampled_value\n",
    "        Kinvy_Pb = self.gp.K_inv_y\n",
    "        L = self.gp.K_chol[0]\n",
    "        log_likelihood = np.dot(y_Pb, Kinvy_Pb) + 2 * np.sum(numpy.log(L.diagonal()))\n",
    "        return -log_likelihood\n",
    "\n",
    "    def compute_grad_objective_function(self):\n",
    "        grad_hyperparameter_cov_tensor = (\n",
    "            self.covariance.build_kernel_hparam_grad_tensor(self.points_sampled)\n",
    "        )\n",
    "        grad_log_marginal = np.empty(self.num_hyperparameters)\n",
    "        Kinvy_Pb = self.gp.K_inv_y\n",
    "        K_chol = self.gp.K_chol\n",
    "        for k in range(self.num_hyperparameters):\n",
    "            dK = grad_hyperparameter_cov_tensor[:, :, k]\n",
    "            dKKinvy_Pb = np.dot(dK, Kinvy_Pb)\n",
    "            grad_log_marginal[k] = -np.dot(Kinvy_Pb, dKKinvy_Pb)\n",
    "            grad_log_marginal[k] += np.trace(\n",
    "                scipy.linalg.cho_solve(K_chol, dK, overwrite_b=True)\n",
    "            )\n",
    "\n",
    "        log_scaling = np.exp(self.hyperparameters) if self.log_domain else 1.0\n",
    "        return -grad_log_marginal * log_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4OizEwWjQaoH"
   },
   "outputs": [],
   "source": [
    "# @title scalar_optimization.py\n",
    "\n",
    "MAXIMUM_REPRESENTABLE_FINITE_FLOAT = np.finfo(np.float64).max\n",
    "\n",
    "\n",
    "class MultistartMaximizer(object):\n",
    "    def __init__(self, optimizer, num_multistarts=1, log_sample=False):\n",
    "        assert not isinstance(optimizer, MultistartMaximizer)\n",
    "        self.optimizer = optimizer\n",
    "        assert num_multistarts >= 1\n",
    "        self.num_multistarts = num_multistarts\n",
    "        self.log_sample = log_sample\n",
    "\n",
    "    def optimize(self, **kwargs):\n",
    "        all_starts = self.optimizer.domain.generate_quasi_random_points_in_domain(\n",
    "            self.num_multistarts, self.log_sample\n",
    "        )\n",
    "\n",
    "        best_point = None\n",
    "        best_function_value = -np.inf\n",
    "        for point in all_starts:\n",
    "            try:\n",
    "                self.optimizer.objective_function.current_point = point\n",
    "                self.optimizer.optimize(**kwargs)\n",
    "            except np.linalg.LinAlgError:\n",
    "                function_value = float(\"nan\")\n",
    "                success = False\n",
    "            else:\n",
    "                # The negation here is required because the optimizer decorator has already negated the value\n",
    "                function_value = -self.optimizer.optimization_results.fun\n",
    "                success = self.optimizer.optimization_results.success\n",
    "\n",
    "            end_point = self.optimizer.objective_function.current_point\n",
    "            if not self.optimizer.domain.check_point_acceptable(end_point):\n",
    "                function_value = float(\"nan\")\n",
    "                success = False\n",
    "\n",
    "            if best_point is None or (success and function_value > best_function_value):\n",
    "                if best_point is None and not success:\n",
    "                    best_point = point\n",
    "                    continue\n",
    "                best_point = end_point\n",
    "                best_function_value = (\n",
    "                    function_value\n",
    "                    if not np.isnan(function_value)\n",
    "                    else best_function_value\n",
    "                )\n",
    "\n",
    "        return best_point\n",
    "\n",
    "\n",
    "class LBFGSBOptimizer(object):\n",
    "    def __init__(self, domain, optimizable, approx_grad=False):\n",
    "        self.domain = domain\n",
    "        self.objective_function = optimizable\n",
    "        self.optimization_results = None\n",
    "        self.approx_grad = approx_grad\n",
    "        assert self.objective_function.differentiable or self.approx_grad\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.domain.dim\n",
    "\n",
    "    def _domain_as_array(self):\n",
    "        return np.array(\n",
    "            [\n",
    "                (interval.min, interval.max)\n",
    "                for interval in self.domain.get_bounding_box()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def joint_function_gradient_eval(self, **kwargs):\n",
    "        def decorated(point):\n",
    "            if np.any(np.isnan(point)):\n",
    "                return np.inf, np.zeros((self.dim,))\n",
    "\n",
    "            self.objective_function.current_point = point\n",
    "            value = -self.objective_function.compute_objective_function(**kwargs)\n",
    "            gradient = -self.objective_function.compute_grad_objective_function(\n",
    "                **kwargs\n",
    "            )\n",
    "            assert np.isfinite(value) and gradient.shape == (self.dim,)\n",
    "            return value, gradient\n",
    "\n",
    "        return decorated\n",
    "\n",
    "    def _scipy_decorator(self, func, **kwargs):\n",
    "        def decorated(point):\n",
    "            self.objective_function.current_point = point\n",
    "            return -func(**kwargs)\n",
    "\n",
    "        return decorated\n",
    "\n",
    "    def optimize(self, **kwargs):\n",
    "        self.optimization_results = self._optimize_core(**kwargs)\n",
    "        point = self.optimization_results.x\n",
    "        self.objective_function.current_point = point\n",
    "\n",
    "    def _optimize_core(self, **kwargs):\n",
    "        options = {\n",
    "            \"eps\": 1.0e-8,\n",
    "            \"gtol\": 1.0e-4,\n",
    "            \"maxcor\": 10,\n",
    "            \"maxfun\": 15000,\n",
    "            \"ftol\": 1e-4,\n",
    "        }\n",
    "        if self.approx_grad:\n",
    "            return scipy.optimize.minimize(\n",
    "                fun=self._scipy_decorator(\n",
    "                    self.objective_function.compute_objective_function, **kwargs\n",
    "                ),\n",
    "                x0=self.objective_function.current_point.flatten(),\n",
    "                method=\"L-BFGS-B\",\n",
    "                bounds=self._domain_as_array(),\n",
    "                options=options,\n",
    "            )\n",
    "        else:\n",
    "            options.pop(\"eps\")\n",
    "            return scipy.optimize.minimize(\n",
    "                fun=self.joint_function_gradient_eval(**kwargs),\n",
    "                x0=self.objective_function.current_point.flatten(),\n",
    "                method=\"L-BFGS-B\",\n",
    "                jac=True,\n",
    "                bounds=self._domain_as_array(),\n",
    "                options=options,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7xJPRAOGP8QM"
   },
   "outputs": [],
   "source": [
    "# @title gaussian_process.py\n",
    "# Separate wrapper for Gaussian process using\n",
    "# MOE's GaussianProcess class as the base class (for internal use).\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from lookahead.model.domain import ClosedInterval\n",
    "# from lookahead.model.historical_data import HistoricalData\n",
    "# from lookahead.model.domain import TensorProductDomain\n",
    "# from lookahead.model.covariance import C4RadialMatern as Matern\n",
    "# from lookahead.model._gaussian_process import GaussianProcess\n",
    "# from lookahead.model.parametrization import GaussianProcessLogMarginalLikelihood as GPlml\n",
    "# from lookahead.model.scalar_optimization import LBFGSBOptimizer as lbfgs_opt\n",
    "# from lookahead.model.scalar_optimization import MultistartMaximizer as ms_opt\n",
    "\n",
    "GPlml = GaussianProcessLogMarginalLikelihood\n",
    "lbfgs_opt = LBFGSBOptimizer\n",
    "ms_opt = MultistartMaximizer\n",
    "\n",
    "\n",
    "# Updates Cholesky by adding a\n",
    "# set of rows/columns, with\n",
    "# data in c12, c22\n",
    "def cho_addcol(L1, c12, c22):\n",
    "    # Assume L1 is in scipy-compliant format\n",
    "    L1 = L1[0]\n",
    "    n = L1.shape[0]\n",
    "    arg_bool = L1[1]\n",
    "    k = c12.shape[1]\n",
    "    L = np.zeros((n + k, n + k))\n",
    "    L12 = solve_triangular(L1, c12, lower=True)\n",
    "    if k == 1:\n",
    "        L22 = np.sqrt(c22 - sp.linalg.norm(L12) ** 2)\n",
    "    else:\n",
    "        L22 = cholesky(c22 - np.dot(L12.T, L12))\n",
    "    L[0:n, 0:n] = L1\n",
    "    if k > 1:\n",
    "        L[n:, 0:n] = L12.T\n",
    "        L[n:, n:] = L22.T\n",
    "    else:\n",
    "        L[n, 0:n] = L12[:, 0]\n",
    "        L[n, n] = L22\n",
    "    return [L, True]\n",
    "\n",
    "\n",
    "class GaussianProcessSimple(GaussianProcess):\n",
    "    \"\"\"\n",
    "    A less complicated implementation of a Gaussian process with a Matern 5/2 kernel, to be used for BO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.ymean = np.mean(ytrain)\n",
    "        self.tikh = 1e-6\n",
    "        ytrain = np.copy(ytrain) - self.ymean\n",
    "        self.best_value = np.min(ytrain)  # update best value\n",
    "        _, self.d = xtrain.shape\n",
    "        hd = HistoricalData(dim=self.d)\n",
    "        hd.append_historical_data(\n",
    "            points_sampled=xtrain,\n",
    "            points_sampled_value=ytrain,\n",
    "            points_sampled_noise_variance=self.tikh * np.ones(ytrain.shape),\n",
    "        )\n",
    "        params_init = np.ones((self.d + 1))\n",
    "        cov = C4RadialMatern(params_init)\n",
    "\n",
    "        super().__init__(\n",
    "            covariance=cov,\n",
    "            historical_data=hd,\n",
    "            tikhonov_param=self.tikh,\n",
    "        )\n",
    "\n",
    "    def set_hypers(self, params):\n",
    "        self.covariance.set_hyperparameters(params)\n",
    "        super().build_precomputed_data()\n",
    "\n",
    "    # Train hyperparameters\n",
    "    def train(self):\n",
    "        log_marginal_likelihood = GPlml(\n",
    "            covariance=self.covariance,\n",
    "            historical_data=self.historical_data,\n",
    "            log_domain=True,\n",
    "        )\n",
    "        hp_domain = TensorProductDomain(\n",
    "            [ClosedInterval(-7, 3)] + [ClosedInterval(-3, 4)] * self.d\n",
    "        )\n",
    "        solver = ms_opt(\n",
    "            lbfgs_opt(hp_domain, log_marginal_likelihood), num_multistarts=4\n",
    "        )\n",
    "        self.covariance.set_hyperparameters(np.exp(solver.optimize()))\n",
    "        super().build_precomputed_data()\n",
    "\n",
    "    def mean(self, xx):\n",
    "        return self.ymean + self.compute_mean_of_points(xx)\n",
    "\n",
    "    # Only difference between predict and mean is that\n",
    "    # predict returns an (n,1) array instead of (n) array\n",
    "    def predict(self, xx):\n",
    "        temp = self.ymean + self.compute_mean_of_points(xx)\n",
    "        temp = temp[:, np.newaxis]\n",
    "        return temp\n",
    "\n",
    "    def variance(self, xx):\n",
    "        return self.compute_variance_of_points(xx)\n",
    "\n",
    "    def add_points(self, XX, YY, retrain=False):\n",
    "        YY = YY - self.ymean\n",
    "        self.best_value = min(np.min(YY), self.best_value)  # update best value\n",
    "        num_points = XX.shape[0]\n",
    "\n",
    "        for i in range(num_points):\n",
    "            self.historical_data.append_historical_data(\n",
    "                points_sampled=XX[[i], :],\n",
    "                points_sampled_value=YY[[i]],\n",
    "                points_sampled_noise_variance=np.array([1e-6]),\n",
    "            )\n",
    "        if retrain:\n",
    "            self.train()\n",
    "        else:\n",
    "            super().build_precomputed_data()\n",
    "\n",
    "    # Sample from posterior\n",
    "    def sample(self, num_samples, points_to_sample):\n",
    "        return (\n",
    "            super().draw_posterior_samples_of_points(num_samples, points_to_sample)\n",
    "            + self.ymean\n",
    "        )\n",
    "\n",
    "    # Update Cholesky factorization with a row/column\n",
    "    def chol_update(self, XX, YY):\n",
    "        K12 = self.covariance.build_kernel_matrix(XX, self.points_sampled)\n",
    "        K22 = self.covariance.build_kernel_matrix(XX, XX) + self.tikh * np.identity(\n",
    "            XX.shape[0]\n",
    "        )\n",
    "        YY = YY - self.ymean\n",
    "        self.best_value = min(np.min(YY), self.best_value)  # update best value\n",
    "        self.historical_data.append_historical_data(\n",
    "            points_sampled=XX,\n",
    "            points_sampled_value=YY,\n",
    "            points_sampled_noise_variance=1e-6 * np.ones(YY.shape),\n",
    "        )\n",
    "        self.K_chol = cho_addcol(self.K_chol, K12, K22)\n",
    "        self.K_inv_y = cho_solve(self.K_chol, self.points_sampled_value)\n",
    "\n",
    "    # Return historical data in the form of two arrays xtrain, ytrain\n",
    "    def get_historical_data(self):\n",
    "        return (\n",
    "            self.historical_data.points_sampled,\n",
    "            self.ymean + self.historical_data.points_sampled_value,\n",
    "        )\n",
    "\n",
    "    # Sample single point, should be faster than self.sample()\n",
    "    def sample_single(self, point_to_sample, random_gaussian_normal_sample=None):\n",
    "        mu, var = self.mean_variance_single(point_to_sample)\n",
    "        if random_gaussian_normal_sample is None:\n",
    "            return np.random.normal(mu, np.sqrt(var), 1)\n",
    "        else:\n",
    "            mu, var = self.mean_variance_single(point_to_sample)\n",
    "            return np.atleast_1d(mu + np.sqrt(var) * random_gaussian_normal_sample)\n",
    "\n",
    "    # Compute means and variance at single point, should be vaster than self.variance()\n",
    "    # returns in order mu, sigma (mean and variance respectively)\n",
    "    def mean_variance_single(self, point_to_sample):\n",
    "        Kx = self.covariance.build_kernel_matrix(\n",
    "            self.historical_data.points_sampled, points_to_sample=point_to_sample\n",
    "        )\n",
    "        Kxx = self.covariance.build_kernel_matrix(\n",
    "            point_to_sample, points_to_sample=point_to_sample\n",
    "        )\n",
    "        mu = np.dot(Kx, self.K_inv_y) + self.ymean\n",
    "        Q = solve_triangular(\n",
    "            self.K_chol[0],\n",
    "            Kx.T,\n",
    "            lower=self.K_chol[1],\n",
    "            overwrite_b=True,\n",
    "        )\n",
    "        return mu.item(), Kxx.item() - np.sum(Q**2, axis=0).item()\n",
    "\n",
    "    def components(self, points_to_evaluate):\n",
    "        mean, var, grad_mean, grad_var = self.compute_mean_variance_grad_of_points(\n",
    "            points_to_evaluate\n",
    "        )\n",
    "        sqrt_var = np.sqrt(var)\n",
    "        grad_sqrt_var = 0.5 * grad_var / sqrt_var[:, np.newaxis]\n",
    "        z = (self.best_value - mean) / sqrt_var\n",
    "        cdf_z = sp.stats.norm.cdf(z)\n",
    "        pdf_z = sp.stats.norm.pdf(z)\n",
    "        return z, sqrt_var, cdf_z, pdf_z, grad_mean, grad_var, grad_sqrt_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dC8gSdAeQKti"
   },
   "outputs": [],
   "source": [
    "# @title historical_data.py\n",
    "\n",
    "\n",
    "class SamplePoint(object):\n",
    "    def __init__(self, point, value=None, noise_variance=0.0):\n",
    "        if not (noise_variance >= 0.0 and np.isfinite(noise_variance)):\n",
    "            raise ValueError(\n",
    "                f\"noise_variance = {noise_variance} must be non-negative and finite!\"\n",
    "            )\n",
    "        if value is None or np.isinf(value):\n",
    "            raise ValueError(f\"value = {value} must be finite (nan allowed)!\")\n",
    "        if any(~np.isfinite(point)):\n",
    "            raise ValueError(f\"point = {point} must be finite!\")\n",
    "\n",
    "        self.point = np.copy(point)\n",
    "        self.value = value\n",
    "        self.noise_variance = noise_variance\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return SamplePoint(d[\"point\"], d[\"value\"], d[\"value_var\"])\n",
    "\n",
    "    @property\n",
    "    def as_tuple(self):\n",
    "        return self.point, self.value, self.noise_variance\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item == \"point\":\n",
    "            return self.point\n",
    "        elif item == \"value\":\n",
    "            return self.value\n",
    "        elif item == \"value_var\":\n",
    "            return self.noise_variance\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"key {item} not recognized, must be one of ('point', 'value', 'value_var')\"\n",
    "            )\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Pretty print this object as a dict.\"\"\"\n",
    "        return pprint.pformat(dict(zip((\"point\", \"value\", \"value_var\"), self.as_tuple)))\n",
    "\n",
    "    def json_payload(self):\n",
    "        return {\n",
    "            \"point\": list(self.point),  # json needs a list\n",
    "            \"value\": self.value,\n",
    "            \"value_var\": self.noise_variance,\n",
    "        }\n",
    "\n",
    "\n",
    "class HistoricalData(object):\n",
    "    @staticmethod\n",
    "    def convert_list_of_sample_points_to_arrays(sample_points):\n",
    "        if len(sample_points) == 0:\n",
    "            return np.array([[]]), np.array([]), np.array([])\n",
    "        return tuple(np.array(v) for v in zip(*[p.as_tuple for p in sample_points]))\n",
    "\n",
    "    def __init__(self, dim, sample_points=None):\n",
    "        if sample_points is None:\n",
    "            sample_points = []\n",
    "\n",
    "        self.dim = dim\n",
    "        self.points_sampled = np.empty((0, self.dim))\n",
    "        self.points_sampled_value = np.empty(0)\n",
    "        self.points_sampled_noise_variance = np.empty(0)\n",
    "\n",
    "        self.append_sample_points(sample_points)\n",
    "\n",
    "    def __str__(self, pretty_print=True):\n",
    "        \"\"\"String representation of this HistoricalData object.\"\"\"\n",
    "        if pretty_print:\n",
    "            return pprint.pformat(self.to_list_of_sample_points())\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                repr(self.points_sampled),\n",
    "                repr(self.points_sampled_value),\n",
    "                repr(self.points_sampled_noise_variance),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def append_lies(self, points_being_sampled, lie_value, lie_value_var):\n",
    "        self.append_sample_points(\n",
    "            [\n",
    "                SamplePoint(point, lie_value, lie_value_var)\n",
    "                for point in points_being_sampled\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def append_historical_data(\n",
    "        self, points_sampled, points_sampled_value, points_sampled_noise_variance\n",
    "    ):\n",
    "        \"\"\"Append lists of points_sampled, their values, and their noise variances to the data members of this class.\"\"\"\n",
    "        if points_sampled.size == 0:\n",
    "            return\n",
    "\n",
    "        assert len(points_sampled.shape) == 2\n",
    "        assert (\n",
    "            len(points_sampled_value.shape)\n",
    "            == len(points_sampled_noise_variance.shape)\n",
    "            == 1\n",
    "        )\n",
    "        assert (\n",
    "            len(points_sampled)\n",
    "            == len(points_sampled_value)\n",
    "            == len(points_sampled_noise_variance)\n",
    "        )\n",
    "        assert points_sampled.shape[1] == self.dim\n",
    "\n",
    "        self.points_sampled = np.append(self.points_sampled, points_sampled, axis=0)\n",
    "        self.points_sampled_value = np.append(\n",
    "            self.points_sampled_value, points_sampled_value\n",
    "        )\n",
    "        self.points_sampled_noise_variance = np.append(\n",
    "            self.points_sampled_noise_variance, points_sampled_noise_variance\n",
    "        )\n",
    "\n",
    "    def append_sample_points(self, sample_points):\n",
    "        self.append_historical_data(\n",
    "            *self.convert_list_of_sample_points_to_arrays(sample_points)\n",
    "        )\n",
    "\n",
    "    def to_list_of_sample_points(self):\n",
    "        return [\n",
    "            SamplePoint(*a)\n",
    "            for a in zip(\n",
    "                self.points_sampled,\n",
    "                self.points_sampled_value,\n",
    "                self.points_sampled_noise_variance,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def num_sampled(self):\n",
    "        \"\"\"Return the number of sampled points.\"\"\"\n",
    "        return self.points_sampled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gJcTYErNQqpR"
   },
   "outputs": [],
   "source": [
    "# @title utils.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "def compute_cholesky_for_gp_sampling(covariance_matrix):\n",
    "    try:\n",
    "        chol_cov = -scipy.linalg.cholesky(\n",
    "            covariance_matrix, lower=True, overwrite_a=True, check_finite=False\n",
    "        )\n",
    "    except scipy.linalg.LinAlgError:\n",
    "        U, E, _ = scipy.linalg.svd(\n",
    "            covariance_matrix, overwrite_a=True, check_finite=False\n",
    "        )\n",
    "        chol_cov = U * np.sqrt(E)[np.newaxis, :]\n",
    "        chol_cov = -scipy.linalg.qr(\n",
    "            chol_cov.T, mode=\"r\", overwrite_a=True, check_finite=False\n",
    "        )[0].T\n",
    "    return chol_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt5d-q6rRAI1"
   },
   "source": [
    "## lookahead/runners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OCgId5lvQ_rn"
   },
   "outputs": [],
   "source": [
    "# @title bayesian_optimization.py\n",
    "# import numpy as np\n",
    "# from lookahead.model.gaussian_process import GaussianProcessSimple as GaussianProcess\n",
    "# import os\n",
    "\n",
    "GaussianProcess = GaussianProcessSimple\n",
    "\n",
    "\n",
    "class BayesianOptimization(object):\n",
    "    def __init__(self, search_space):\n",
    "        self.opt_name = \"abstract\"\n",
    "        self.gaussian_process = None\n",
    "        self.search_space = search_space\n",
    "\n",
    "    def run(self, f, seed, budget_minus_initialization, initialization_duration=5):\n",
    "        # Warm start with 5 points, with fixed random seed\n",
    "        np.random.seed(seed)\n",
    "        d = len(self.search_space.domain_bounds)\n",
    "        xhist = np.random.rand(initialization_duration, d)\n",
    "        yhist = f(xhist)\n",
    "        self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "        self.gaussian_process.train()\n",
    "\n",
    "        while budget_minus_initialization > 0:\n",
    "            # Get next sample point\n",
    "            xsample = self.get_next_point()\n",
    "            ysample = f(xsample)\n",
    "            xhist = np.vstack((xhist, xsample))\n",
    "            yhist = np.append(yhist, ysample)\n",
    "            self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "            self.gaussian_process.train()\n",
    "            budget_minus_initialization -= 1\n",
    "\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "        self.save_bo_run(yhist, str(f.__name__), seed)\n",
    "\n",
    "    def get_next_point(self):\n",
    "        # To be implemented by each acquisition function\n",
    "        pass\n",
    "\n",
    "    def save_bo_run(self, yhist, objective_name, seed):\n",
    "        seed = str(seed)\n",
    "        \"\"\"\n",
    "        Saves run to the folder ~/Look-Ahead/results/optimizer_name/objective_name/seed.csv\n",
    "        \"\"\"\n",
    "        base_path = os.path.expanduser(\"~\") + \"/Look-Ahead/results/\"\n",
    "        # Make paths if necessary\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "        path = base_path + self.opt_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        path = path + objective_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        run_name = path + str(seed) + \".csv\"\n",
    "\n",
    "        # Save data as csv to path\n",
    "        np.savetxt(run_name, yhist, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8AhrXg3IRcE6"
   },
   "outputs": [],
   "source": [
    "# @title portfolio_runners.py\n",
    "# import numpy as np\n",
    "# from lookahead.runners.bayesian_optimization import BayesianOptimization\n",
    "# from lookahead.acquisitions.rollout_portfolio import RolloutPortfolio, RolloutPortfolioEI\n",
    "# from lookahead.model.gaussian_process import GaussianProcessSimple as GaussianProcess\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "GaussianProcess = GaussianProcessSimple\n",
    "\n",
    "\n",
    "class PortfolioRunner(BayesianOptimization):\n",
    "    \"\"\"\n",
    "    A  runner class for Portfolio, Diagnostic because it outputs auxillary data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, search_space, horizon):\n",
    "        super().__init__(search_space)\n",
    "        self.horizon = horizon\n",
    "        self.opt_name = \"portfolio\" + str(horizon)\n",
    "\n",
    "    def run(self, f, seed, budget_minus_initialization, initialization_duration=5):\n",
    "        acquisition_chosen_all = []\n",
    "\n",
    "        # Warm start with 5 points, with fixed random seed\n",
    "        np.random.seed(seed)\n",
    "        d = len(self.search_space.domain_bounds)\n",
    "        xhist = np.random.rand(5, d)\n",
    "        yhist = f(xhist)\n",
    "        self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "        self.gaussian_process.train()\n",
    "\n",
    "        while budget_minus_initialization > 0:\n",
    "            # Get next sample point\n",
    "            xsample, acquisition_chosen = self.get_next_point()\n",
    "            acquisition_chosen_all.append(acquisition_chosen)\n",
    "            ysample = f(xsample)\n",
    "            xhist = np.vstack((xhist, xsample))\n",
    "            yhist = np.append(yhist, ysample)\n",
    "            self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "            self.gaussian_process.train()\n",
    "            budget_minus_initialization -= 1\n",
    "\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "\n",
    "        # Save BOTH acquisitions chosen as well as run information\n",
    "        self.save_auxillary_data(acquisition_chosen_all, str(f.__name__), seed)\n",
    "        self.save_bo_run(yhist, str(f.__name__), seed)\n",
    "\n",
    "    def get_next_point(self):\n",
    "        # To be implemented by each acquisition function\n",
    "        pr = RolloutPortfolio(self.gaussian_process, self.search_space, self.horizon)\n",
    "        return pr.next_point()\n",
    "\n",
    "    def save_auxillary_data(self, acquisition_chosen_all, objective_name, seed):\n",
    "        seed = str(seed)\n",
    "        \"\"\"\n",
    "        Saves run to the folder ~/Look-Ahead/results/optimizer_name/objective_name/seed.csv\n",
    "        \"\"\"\n",
    "        base_path = os.path.expanduser(\"~\") + \"/Look-Ahead/results/\"\n",
    "        # Make paths if necessary\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "        path = base_path + self.opt_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        path = path + objective_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        run_name = path + \"portfolio_aux\" + str(seed) + \".csv\"\n",
    "\n",
    "        # Save data as csv to path\n",
    "        with open(run_name, \"w\") as file:\n",
    "            writer = csv.writer(file, delimiter=\"\\n\")\n",
    "            writer.writerow(acquisition_chosen_all)\n",
    "\n",
    "\n",
    "class PortfolioEIRunner(BayesianOptimization):\n",
    "    \"\"\"\n",
    "    A  runner class for Portfolio, Diagnostic because it outputs auxillary data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, search_space, horizon):\n",
    "        super().__init__(search_space)\n",
    "        self.horizon = horizon\n",
    "        self.opt_name = \"portfolio_ei\" + str(horizon)\n",
    "\n",
    "    def run(self, f, seed, budget_minus_initialization, initialization_duration=5):\n",
    "        acquisition_chosen_all = []\n",
    "\n",
    "        # Warm start with 5 points, with fixed random seed\n",
    "        np.random.seed(seed)\n",
    "        d = len(self.search_space.domain_bounds)\n",
    "        xhist = np.random.rand(5, d)\n",
    "        yhist = f(xhist)\n",
    "        self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "        self.gaussian_process.train()\n",
    "\n",
    "        while budget_minus_initialization > 0:\n",
    "            # Get next sample point\n",
    "            xsample, acquisition_chosen = self.get_next_point()\n",
    "            acquisition_chosen_all.append(acquisition_chosen)\n",
    "            ysample = f(xsample)\n",
    "            xhist = np.vstack((xhist, xsample))\n",
    "            yhist = np.append(yhist, ysample)\n",
    "            self.gaussian_process = GaussianProcess(xhist, yhist)\n",
    "            self.gaussian_process.train()\n",
    "            budget_minus_initialization -= 1\n",
    "\n",
    "        # Save BOTH acquisitions chosen as well as run information\n",
    "        xhist, yhist = self.gaussian_process.get_historical_data()\n",
    "        self.save_auxillary_data(acquisition_chosen_all, str(f.__name__), seed)\n",
    "        self.save_bo_run(yhist, str(f.__name__), seed)\n",
    "\n",
    "    def get_next_point(self):\n",
    "        # To be implemented by each acquisition function\n",
    "        pr = RolloutPortfolioEI(self.gaussian_process, self.search_space, self.horizon)\n",
    "        return pr.next_point()\n",
    "\n",
    "    def save_auxillary_data(self, acquisition_chosen_all, objective_name, seed):\n",
    "        seed = str(seed)\n",
    "        \"\"\"\n",
    "        Saves run to the folder ~/Look-Ahead/results/optimizer_name/objective_name/seed.csv\n",
    "        \"\"\"\n",
    "        base_path = os.path.expanduser(\"~\") + \"/Look-Ahead/results/\"\n",
    "        # Make paths if necessary\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "        path = base_path + self.opt_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        path = path + objective_name + \"/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        run_name = path + \"portfolio_aux\" + str(seed) + \".csv\"\n",
    "\n",
    "        # Save data as csv to path\n",
    "        with open(run_name, \"w\") as file:\n",
    "            writer = csv.writer(file, delimiter=\"\\n\")\n",
    "            writer.writerow(acquisition_chosen_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2HdJBIyZSAsh"
   },
   "outputs": [],
   "source": [
    "# @title runners.py\n",
    "# import numpy as np\n",
    "# from lookahead.runners.bayesian_optimization import BayesianOptimization\n",
    "# from lookahead.acquisitions.expected_improvement import ExpectedImprovement\n",
    "# from lookahead.acquisitions.knowledge_gradient import KnowledgeGradient\n",
    "# from lookahead.acquisitions.ucb import UpperConfidenceBound\n",
    "# from lookahead.acquisitions.rollout_ei_vr import RolloutEI_VR\n",
    "\n",
    "\n",
    "class RandomRunner(BayesianOptimization):\n",
    "    def __init__(self, search_space):\n",
    "        super().__init__(search_space)\n",
    "        self.opt_name = \"random\"\n",
    "\n",
    "    def get_next_point(self):\n",
    "        return self.search_space.generate_quasi_random_points_in_domain(1)\n",
    "\n",
    "\n",
    "class ExpectedImprovementRunner(BayesianOptimization):\n",
    "    def __init__(self, search_space):\n",
    "        super().__init__(search_space)\n",
    "        self.opt_name = \"ei\"\n",
    "\n",
    "    def get_next_point(self):\n",
    "        ei = ExpectedImprovement(self.gaussian_process, self.search_space)\n",
    "        return ei.next_point_grad()\n",
    "\n",
    "\n",
    "class UpperConfidenceBoundRunner(BayesianOptimization):\n",
    "    def __init__(self, search_space, base_kappa=1):\n",
    "        super().__init__(search_space)\n",
    "        self.opt_name = \"ucb\" + str(base_kappa)\n",
    "        self.base_kappa = base_kappa\n",
    "\n",
    "    def get_next_point(self):\n",
    "        kwargs = {\"base_kappa\": self.base_kappa}\n",
    "        ucb = UpperConfidenceBound(self.gaussian_process, self.search_space, **kwargs)\n",
    "        return ucb.next_point_grad()\n",
    "\n",
    "\n",
    "class KnowledgeGradientRunner(BayesianOptimization):\n",
    "    def __init__(self, search_space):\n",
    "        super().__init__(search_space)\n",
    "        self.opt_name = \"kg\"\n",
    "\n",
    "    def get_next_point(self):\n",
    "        kg = KnowledgeGradient(self.gaussian_process, self.search_space)\n",
    "        return kg.next_point_grid()\n",
    "\n",
    "\n",
    "class RolloutRunner(BayesianOptimization):\n",
    "    def __init__(self, search_space, horizon):\n",
    "        super().__init__(search_space)\n",
    "        self.horizon = horizon\n",
    "        self.opt_name = \"ei\" + str(horizon)\n",
    "\n",
    "    def get_next_point(self):\n",
    "        eih = RolloutEI_VR(self.gaussian_process, self.search_space, self.horizon)\n",
    "        return eih.next_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qm4Psb4tSVej"
   },
   "source": [
    "## lookahead/test_problems/unconstrained.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMjF-UMkSbpb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE 1: all test functions designed to be evaluated on cube [0.1]^d\n",
    "Make sure domains are slighly uneven (b/c the minimum is often in\n",
    "the exact middle of the domain for these test functions)\n",
    "NOTE 2: most test functions' implementations copied directly either from\n",
    "SFU Virtual Library of Simulation Experiments:\n",
    "https://www.sfu.ca/~ssurjano/index.html\n",
    "or from pySOT:\n",
    "https://github.com/dme65/pySOT/blob/master/pySOT/optimization_problems.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def from_unit_box(x, lb, ub):\n",
    "    return lb + (ub - lb) * x\n",
    "\n",
    "\n",
    "# Evaluated from [-5, 10]^2\n",
    "def rosenbrock(x):\n",
    "    lb = np.full((2,), -5)\n",
    "    ub = np.full((2,), 10)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    a = 1.0 - x1\n",
    "    b = x2 - x1 * x1\n",
    "    return a * a + b * b * 100.0\n",
    "\n",
    "\n",
    "# Evaluated by default from [-4, 5]^d\n",
    "def ackley(x, lb=None, ub=None):\n",
    "    n, d = x.shape\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), -4)\n",
    "        ub = np.full((d,), 5)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    return (\n",
    "        -20.0 * np.exp(-0.2 * np.sqrt(np.sum(x**2, axis=1) / d))\n",
    "        - np.exp(np.sum(np.cos(2.0 * np.pi * x), axis=1) / d)\n",
    "        + 20\n",
    "        + np.exp(1)\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluated by default from [-2.5, 3]^d\n",
    "def rastrigin(x, lb=None, ub=None):\n",
    "    n, d = x.shape\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), -2.5)\n",
    "        ub = np.full((d,), 3)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    return 10 * d + np.sum(x**2 - 10 * np.cos(2 * np.pi * x), axis=1)\n",
    "\n",
    "\n",
    "# Evaluated by default from [-1.8, 2.2]^2\n",
    "def sixhump(x, lb=None, ub=None):\n",
    "    if x.shape[1] != 2:\n",
    "        raise Exception(\"Dimension must be 2\")\n",
    "    d = 2\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), -1.8)\n",
    "        ub = np.full((d,), 2.2)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    return (\n",
    "        (4.0 - 2.1 * x[:, 0] ** 2 + (x[:, 0] ** 4) / 3.0) * x[:, 0] ** 2\n",
    "        + x[:, 0] * x[:, 1]\n",
    "        + (-4 + 4 * x[:, 1] ** 2) * x[:, 1] ** 2\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluated from [-5, 10] x [0, 15]\n",
    "def branin(x, lb=None, ub=None):\n",
    "    if x.shape[1] != 2:\n",
    "        raise Exception(\"Dimension must be 2\")\n",
    "    d = 2\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), 0)\n",
    "        ub = np.full((d,), 0)\n",
    "        lb[0] = -5\n",
    "        lb[1] = 0\n",
    "        ub[0] = 10\n",
    "        ub[1] = 15\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    t = 1 / (8 * np.pi)\n",
    "    s = 10\n",
    "    r = 6\n",
    "    c = 5 / np.pi\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    a = 1\n",
    "    term1 = a * (x2 - b * x1**2 + c * x1 - r) ** 2\n",
    "    term2 = s * (1 - t) * np.cos(x1)\n",
    "    return term1 + term2 + s\n",
    "\n",
    "\n",
    "def hartman3(x, lb=None, ub=None):\n",
    "    if x.shape[1] != 3:\n",
    "        raise Exception(\"Dimension must be 3\")\n",
    "    d = 3\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), 0)\n",
    "        ub = np.full((d,), 1)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    alpha = np.array([1, 1.2, 3, 3.2])\n",
    "    A = np.array(\n",
    "        [[3.0, 10.0, 30.0], [0.1, 10.0, 35.0], [3.0, 10.0, 30.0], [0.1, 10.0, 35.0]]\n",
    "    )\n",
    "    P = np.array(\n",
    "        [\n",
    "            [0.3689, 0.1170, 0.2673],\n",
    "            [0.4699, 0.4387, 0.747],\n",
    "            [0.1091, 0.8732, 0.5547],\n",
    "            [0.0381, 0.5743, 0.8828],\n",
    "        ]\n",
    "    )\n",
    "    outer = 0\n",
    "    for ii in range(4):\n",
    "        inner = 0\n",
    "        for jj in range(3):\n",
    "            xj = x[:, jj]\n",
    "            Aij = A[ii, jj]\n",
    "            Pij = P[ii, jj]\n",
    "            inner += Aij * ((xj - Pij) ** 2)\n",
    "        outer += alpha[ii] * np.exp(-inner)\n",
    "    return -outer\n",
    "\n",
    "\n",
    "def hartman4(x, lb=None, ub=None):\n",
    "    if x.shape[1] != 4:\n",
    "        raise Exception(\"Dimension must be 4\")\n",
    "    d = 4\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), 0)\n",
    "        ub = np.full((d,), 1)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "    A = np.array(\n",
    "        [\n",
    "            [10, 3, 17, 3.5, 1.7, 8],\n",
    "            [0.05, 10, 17, 0.1, 8, 14],\n",
    "            [3, 3.5, 1.7, 10, 17, 8],\n",
    "            [17, 8, 0.05, 10, 0.1, 14],\n",
    "        ]\n",
    "    )\n",
    "    P = 1e-4 * np.array(\n",
    "        [\n",
    "            [1312, 1696, 5569, 124, 8283, 5886],\n",
    "            [2329, 4135, 8307, 3736, 1004, 9991],\n",
    "            [2348, 1451, 3522, 2883, 3047, 6650],\n",
    "            [4047, 8828, 8732, 5743, 1091, 381],\n",
    "        ]\n",
    "    )\n",
    "    outer = 0\n",
    "    for ii in range(4):\n",
    "        inner = 0\n",
    "        for jj in range(4):\n",
    "            xj = x[:, jj]\n",
    "            Aij = A[ii, jj]\n",
    "            Pij = P[ii, jj]\n",
    "            inner = inner + Aij * ((xj - Pij) ** 2)\n",
    "        outer += alpha[ii] * np.exp(-inner)\n",
    "    return (1.1 - outer) / 0.839\n",
    "\n",
    "\n",
    "def levy(x, lb=None, ub=None):\n",
    "    d = x.shape[1]\n",
    "    if lb is None or ub is None:\n",
    "        lb = -5 * np.full((d,), 1)\n",
    "        ub = 5 * np.full((d,), 1)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    w = 1 + (x - 1.0) / 4.0\n",
    "    return (\n",
    "        np.sin(np.pi * w[:, 0]) ** 2\n",
    "        + np.sum(\n",
    "            (w[:, 1 : d - 1] - 1) ** 2\n",
    "            * (1 + 10 * np.sin(np.pi * w[:, 1 : d - 1] + 1) ** 2),\n",
    "            axis=1,\n",
    "        )\n",
    "        + (w[:, d - 1] - 1) ** 2 * (1 + np.sin(2 * np.pi * w[:, d - 1]) ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "def hartman6(x, lb=None, ub=None):\n",
    "    if x.shape[1] != 6:\n",
    "        raise Exception(\"Dimension must be 6\")\n",
    "    d = 6\n",
    "    if lb is None or ub is None:\n",
    "        lb = np.full((d,), 0)\n",
    "        ub = np.full((d,), 1)\n",
    "    x = from_unit_box(x, lb, ub)\n",
    "    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "    A = np.array(\n",
    "        [\n",
    "            [10.0, 3.0, 17.0, 3.5, 1.7, 8.0],\n",
    "            [0.05, 10.0, 17.0, 0.1, 8.0, 14.0],\n",
    "            [3.0, 3.5, 1.7, 10.0, 17.0, 8.0],\n",
    "            [17.0, 8.0, 0.05, 10.0, 0.1, 14.0],\n",
    "        ]\n",
    "    )\n",
    "    P = 1e-4 * np.array(\n",
    "        [\n",
    "            [1312.0, 1696.0, 5569.0, 124.0, 8283.0, 5886.0],\n",
    "            [2329.0, 4135.0, 8307.0, 3736.0, 1004.0, 9991.0],\n",
    "            [2348.0, 1451.0, 3522.0, 2883.0, 3047.0, 6650.0],\n",
    "            [4047.0, 8828.0, 8732.0, 5743.0, 1091.0, 381.0],\n",
    "        ]\n",
    "    )\n",
    "    outer = 0\n",
    "    for ii in range(4):\n",
    "        inner = 0\n",
    "        for jj in range(6):\n",
    "            xj = x[:, jj]\n",
    "            Aij = A[ii, jj]\n",
    "            Pij = P[ii, jj]\n",
    "            inner += Aij * ((xj - Pij) ** 2)\n",
    "        outer += alpha[ii] * np.exp(-inner)\n",
    "    return -outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mel2sfi-C4bz"
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NV0tbEZZGEc"
   },
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    opt_domain = TensorProductDomain([ClosedInterval(0, 1)])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    xtrain = np.linspace(0.15, 0.85, 2)[:, None]\n",
    "    ytrain = np.array([0, 0])\n",
    "    gp = GaussianProcessSimple(xtrain, ytrain)\n",
    "    gp.set_hypers([5, 0.25])\n",
    "    x = np.linspace(0, 1, 40)[:, None]\n",
    "    y_gp = gp.mean(x)\n",
    "    y_var = np.sqrt(gp.variance(x))\n",
    "    _ = ax.plot(x, y_gp, color=\"r\")\n",
    "    _ = ax.plot(xtrain, ytrain, \"k.\", markersize=15)\n",
    "    _ = ax.fill_between(x[:, 0], y_gp - y_var, y_gp + y_var, color=\"m\", alpha=0.25)\n",
    "    _ = ax.legend([\"GP mean\", \"Observations\", \"GP uncertainty\"])\n",
    "\n",
    "    (fig, ax) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ei = ExpectedImprovement(gp, opt_domain)\n",
    "    ei_vals = ei.evaluate_at_point_list(x)\n",
    "\n",
    "    # We estimate EI2 using 100 MC iterations, using grid search of size 50 to maximize the inner EI\n",
    "    ei2 = RolloutEI_VR(\n",
    "        gp, opt_domain, horizon=2, opt_mode=\"grid\", mc_iters=20, grid_size=100\n",
    "    )\n",
    "    ei2_vals = ei2.evaluate_at_point_list(x)\n",
    "\n",
    "    _ = ax[0].plot(x, ei_vals, \"--g\")\n",
    "    _ = ax[1].plot(x, ei2_vals, \"--g\")\n",
    "    _ = ax[0].set_title(\"EI Acquisition\")\n",
    "    _ = ax[1].set_title(\"EI2 Acquisition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqLp37xE2xI6"
   },
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lbv6gVrQEoYF"
   },
   "outputs": [],
   "source": [
    "# @title HES\n",
    "import copy\n",
    "\n",
    "n_dim = \"1\"  # @param [1, 2]\n",
    "n_dim = int(n_dim)\n",
    "nonmyopic = True  # @param {type:\"boolean\"}\n",
    "budget = 20  # @param {type:\"integer\"}\n",
    "\n",
    "save = True\n",
    "draw_legend = False\n",
    "\n",
    "if n_dim == 1:\n",
    "    dim_xi = [n_dim]\n",
    "    prev_x = torch.tensor([0.3])\n",
    "    prev_x_str = \"0.3\"\n",
    "elif n_dim == 2:\n",
    "    dim_xi = [1, n_dim]\n",
    "    prev_x = torch.tensor([[0.4, 0.4]])\n",
    "    prev_x_str = \"0.4_0.4\"\n",
    "\n",
    "neighbor_size = 1 / budget  # 0.02\n",
    "n_init = 0\n",
    "ehig_opt_epoch = 50\n",
    "ehig_opt_lr = 0.1\n",
    "use_lr_schedule = False\n",
    "train = False\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "optimal_loss_results = []\n",
    "\n",
    "exp_name = f\"{n_dim}D_nonmyopic{nonmyopic}_budget{budget}_\\\n",
    "neighbor_size{neighbor_size}_prev_x{prev_x_str}\"\n",
    "\n",
    "# create folder\n",
    "if not os.path.exists(f\"{path}/{exp_name}\"):\n",
    "    os.makedirs(f\"{path}/{exp_name}\")\n",
    "    print(f\"Saving to: {path}/{exp_name}\")\n",
    "\n",
    "for seed in seeds:\n",
    "    if n_dim == 1:\n",
    "        dim_xi = [n_dim]\n",
    "        prev_x = torch.tensor([0.3])\n",
    "    elif n_dim == 2:\n",
    "        dim_xi = [1, n_dim]\n",
    "        prev_x = torch.tensor([[0.4, 0.4]])\n",
    "\n",
    "    if n_dim == 2:\n",
    "        print(\"-\" * 20, \"Ground truth surface\", \"-\" * 20)\n",
    "        ground_truth(draw_true_model=True, n_dim=n_dim)\n",
    "        plt.show()\n",
    "\n",
    "    train_x, train_y = init_data(n_init=n_init)\n",
    "\n",
    "    model = init_model(\n",
    "        n_dim=n_dim, prev_x=prev_x, train_x=train_x, train_y=train_y, train=train\n",
    "    )\n",
    "\n",
    "    if nonmyopic:\n",
    "        horizon = budget\n",
    "    else:\n",
    "        horizon = 1\n",
    "    optimal_loss = []\n",
    "    for step in range(budget):\n",
    "        best_result = argmax_ehig(\n",
    "            model,\n",
    "            prev_x,\n",
    "            horizon=horizon,\n",
    "            ehig_opt_epoch=ehig_opt_epoch,\n",
    "            ehig_opt_lr=ehig_opt_lr,\n",
    "            neighbor_size=neighbor_size,\n",
    "            use_lr_schedule=use_lr_schedule,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        if nonmyopic:\n",
    "            horizon = horizon - 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_x = best_result[0]\n",
    "            next_y = func(next_x.reshape(-1, n_dim))\n",
    "\n",
    "            # draw_posterior(n_dim=n_dim, model=model)\n",
    "            # if n_dim == 2: plt.show()\n",
    "\n",
    "            # update data and model\n",
    "            train_x = torch.cat([train_x, next_x.reshape(-1, n_dim)])\n",
    "            train_y = torch.cat([train_y, next_y])\n",
    "\n",
    "            model = init_model(\n",
    "                n_dim=n_dim,\n",
    "                prev_x=prev_x,\n",
    "                train_x=train_x,\n",
    "                train_y=train_y,\n",
    "                train=train,\n",
    "            )\n",
    "\n",
    "        # eval\n",
    "        a = (torch.rand([n_dim] if n_dim == 1 else [1, n_dim])).requires_grad_(True)\n",
    "        optimizer = optim.Adam([a], lr=ehig_opt_lr)\n",
    "\n",
    "        loss = []\n",
    "        for epoch in range(10):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            a = torch.sigmoid(a) * (neighbor_size * 2) + (next_x - neighbor_size)\n",
    "\n",
    "            p_y_on_a_D = model.posterior(a)\n",
    "            ell = p_y_on_a_D.mean.mean()\n",
    "            ell.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            loss.append(ell.clone().detach())\n",
    "\n",
    "        loss = torch.stack(loss).squeeze()\n",
    "        print(f\"Current optimal loss {loss.min().item()};\" f\"optimal action: {a}\")\n",
    "        optimal_loss.append(loss.min())\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dim_xi = [n_dim]\n",
    "        elif n_dim == 2:\n",
    "            dim_xi = [1, n_dim]\n",
    "\n",
    "        # plot & update prev_x\n",
    "        if n_dim == 1:\n",
    "            ground_truth(draw_true_model=True, n_dim=n_dim)\n",
    "\n",
    "        plt.title(f\"Step {step} with horizon of {horizon}\")\n",
    "\n",
    "        # draw_posterior(n_dim=n_dim, model=model)\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dmin = -4\n",
    "            dmax = 4\n",
    "            plt.vlines(prev_x, dmin, dmax, color=color[\"C5\"], label=\"Current location\")\n",
    "            plt.vlines(\n",
    "                prev_x - neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                prev_x + neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.plot(\n",
    "                train_x.squeeze().cpu().numpy(), train_y.squeeze().cpu().numpy(), \"k*\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[0], dmin, dmax, color=color[\"C3\"], label=\"Optimal query\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[-1], dmin, dmax, color=color[\"C4\"], label=\"Optimal action\"\n",
    "            )\n",
    "            plt.ylim(dmin, dmax)\n",
    "\n",
    "        elif n_dim == 2:\n",
    "            plt.hlines(prev_x[0, 1] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.hlines(prev_x[0, 1] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.scatter(\n",
    "                prev_x[0, 0], prev_x[0, 1], color=color[\"C5\"], label=\"Current location\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.scatter(train_x[:, 0], train_x[:, 1], marker=\"*\", color=\"black\")\n",
    "            plt.scatter(\n",
    "                best_result[0][0, 0],\n",
    "                best_result[0][0, 1],\n",
    "                color=color[\"C3\"],\n",
    "                label=\"Optimal query\",\n",
    "            )\n",
    "            plt.scatter(\n",
    "                best_result[-1][..., 0],\n",
    "                best_result[-1][..., 1],\n",
    "                color=color[\"C4\"],\n",
    "                label=\"Optimal action\",\n",
    "            )\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if draw_legend:\n",
    "            plt.legend()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(\n",
    "                f\"{path}/{exp_name}/seed{seed}_step{step}.{imgtype}\",\n",
    "                dpi=dpi,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # plot optimal loss over query\n",
    "    optimal_loss = torch.stack(optimal_loss).numpy()\n",
    "\n",
    "    optimal_loss_results.append(optimal_loss)\n",
    "\n",
    "optimal_loss_results = np.array(optimal_loss_results)\n",
    "mean = optimal_loss_results.mean(0)\n",
    "std = optimal_loss_results.std(0)\n",
    "\n",
    "plt.plot(-mean, label=color[\"C1\"])\n",
    "plt.fill_between(\n",
    "    np.arange(len(mean)),\n",
    "    -(mean - 2 * std),\n",
    "    -(mean + 2 * std),\n",
    "    color=color[\"C2\"],\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.save(f\"{path}/{exp_name}/data.npy\", optimal_loss_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KDvh0ZwkWgWp"
   },
   "outputs": [],
   "source": [
    "# @title Random\n",
    "import copy\n",
    "\n",
    "n_dim = \"2\"  # @param [1, 2]\n",
    "n_dim = int(n_dim)\n",
    "nonmyopic = True  # @param {type:\"boolean\"}\n",
    "budget = 20  # @param {type:\"integer\"}\n",
    "\n",
    "save = True\n",
    "draw_legend = False\n",
    "\n",
    "if n_dim == 1:\n",
    "    dim_xi = [n_dim]\n",
    "    prev_x = torch.tensor([0.3])\n",
    "    prev_x_str = \"0.3\"\n",
    "elif n_dim == 2:\n",
    "    dim_xi = [1, n_dim]\n",
    "    prev_x = torch.tensor([[0.4, 0.4]])\n",
    "    prev_x_str = \"0.4_0.4\"\n",
    "\n",
    "neighbor_size = 1 / budget  # 0.02\n",
    "n_init = 0\n",
    "ehig_opt_epoch = 50\n",
    "ehig_opt_lr = 0.1\n",
    "use_lr_schedule = False\n",
    "train = False\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "optimal_loss_results = []\n",
    "\n",
    "exp_name = f\"{n_dim}D_nonmyopic{nonmyopic}_budget{budget}_\\\n",
    "neighbor_size{neighbor_size}_prev_x{prev_x_str}\"\n",
    "\n",
    "# create folder\n",
    "if not os.path.exists(f\"{path}/{exp_name}\"):\n",
    "    os.makedirs(f\"{path}/{exp_name}\")\n",
    "    print(f\"Saving to: {path}/{exp_name}\")\n",
    "\n",
    "for seed in seeds:\n",
    "    if n_dim == 1:\n",
    "        dim_xi = [n_dim]\n",
    "        prev_x = torch.tensor([0.3])\n",
    "    elif n_dim == 2:\n",
    "        dim_xi = [1, n_dim]\n",
    "        prev_x = torch.tensor([[0.4, 0.4]])\n",
    "\n",
    "    if n_dim == 2:\n",
    "        print(\"-\" * 20, \"Ground truth surface\", \"-\" * 20)\n",
    "        ground_truth(draw_true_model=True, n_dim=n_dim)\n",
    "        plt.show()\n",
    "\n",
    "    train_x, train_y = init_data(n_init=n_init)\n",
    "\n",
    "    model = init_model(\n",
    "        n_dim=n_dim, prev_x=prev_x, train_x=train_x, train_y=train_y, train=train\n",
    "    )\n",
    "\n",
    "    if nonmyopic:\n",
    "        horizon = budget\n",
    "    else:\n",
    "        horizon = 1\n",
    "    optimal_loss = []\n",
    "    for step in range(budget):\n",
    "        with torch.no_grad():\n",
    "            if n_dim == 1:\n",
    "                low = (prev_x - neighbor_size).item()\n",
    "                high = (prev_x + neighbor_size).item()\n",
    "                next_x = torch.FloatTensor(1).uniform_(low, high)\n",
    "\n",
    "                print(next_x)\n",
    "            elif n_dim == 2:\n",
    "                low0 = (prev_x[0, 0] - neighbor_size).item()\n",
    "                high0 = (prev_x[0, 0] + neighbor_size).item()\n",
    "                next_x0 = torch.FloatTensor(1, 1).uniform_(low0, high0)\n",
    "                low1 = (prev_x[0, 1] - neighbor_size).item()\n",
    "                high1 = (prev_x[0, 1] + neighbor_size).item()\n",
    "                next_x1 = torch.FloatTensor(1, 1).uniform_(low1, high1)\n",
    "                next_x = torch.cat([next_x0, next_x1], dim=1)\n",
    "\n",
    "            next_y = func(next_x.reshape(-1, n_dim))\n",
    "\n",
    "            # draw_posterior(n_dim=n_dim, model=model)\n",
    "            # if n_dim == 2: plt.show()\n",
    "\n",
    "            # update data and model\n",
    "            train_x = torch.cat([train_x, next_x.reshape(-1, n_dim)])\n",
    "            train_y = torch.cat([train_y, next_y])\n",
    "\n",
    "            model = init_model(\n",
    "                n_dim=n_dim,\n",
    "                prev_x=prev_x,\n",
    "                train_x=train_x,\n",
    "                train_y=train_y,\n",
    "                train=train,\n",
    "            )\n",
    "\n",
    "        # eval\n",
    "        a = (torch.rand([n_dim] if n_dim == 1 else [1, n_dim])).requires_grad_(True)\n",
    "        optimizer = optim.Adam([a], lr=ehig_opt_lr)\n",
    "\n",
    "        loss = []\n",
    "        for epoch in range(10):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            a = torch.sigmoid(a) * (neighbor_size * 2) + (next_x - neighbor_size)\n",
    "\n",
    "            p_y_on_a_D = model.posterior(a)\n",
    "            ell = p_y_on_a_D.mean.mean()\n",
    "            ell.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            loss.append(ell.clone().detach())\n",
    "\n",
    "        loss = torch.stack(loss).squeeze()\n",
    "        print(f\"Current optimal loss {loss.min().item()};\" f\"optimal action: {a}\")\n",
    "        optimal_loss.append(loss.min())\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dim_xi = [n_dim]\n",
    "        elif n_dim == 2:\n",
    "            dim_xi = [1, n_dim]\n",
    "\n",
    "        # plot & update prev_x\n",
    "        if n_dim == 1:\n",
    "            ground_truth(draw_true_model=True, n_dim=n_dim)\n",
    "\n",
    "        plt.title(f\"Step {step} with horizon of {horizon}\")\n",
    "\n",
    "        # draw_posterior(n_dim=n_dim, model=model)\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dmin = -4\n",
    "            dmax = 4\n",
    "            plt.vlines(prev_x, dmin, dmax, color=color[\"C5\"], label=\"Current location\")\n",
    "            plt.vlines(\n",
    "                prev_x - neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                prev_x + neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.plot(\n",
    "                train_x.squeeze().cpu().numpy(), train_y.squeeze().cpu().numpy(), \"k*\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[0], dmin, dmax, color=color[\"C3\"], label=\"Optimal query\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[-1], dmin, dmax, color=color[\"C4\"], label=\"Optimal action\"\n",
    "            )\n",
    "            plt.ylim(dmin, dmax)\n",
    "\n",
    "        elif n_dim == 2:\n",
    "            plt.hlines(prev_x[0, 1] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.hlines(prev_x[0, 1] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.scatter(\n",
    "                prev_x[0, 0], prev_x[0, 1], color=color[\"C5\"], label=\"Current location\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.scatter(train_x[:, 0], train_x[:, 1], marker=\"*\", color=\"black\")\n",
    "            plt.scatter(\n",
    "                best_result[0][0, 0],\n",
    "                best_result[0][0, 1],\n",
    "                color=color[\"C3\"],\n",
    "                label=\"Optimal query\",\n",
    "            )\n",
    "            plt.scatter(\n",
    "                best_result[-1][..., 0],\n",
    "                best_result[-1][..., 1],\n",
    "                color=color[\"C4\"],\n",
    "                label=\"Optimal action\",\n",
    "            )\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if draw_legend:\n",
    "            plt.legend()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(\n",
    "                f\"{path}/{exp_name}/seed{seed}_step{step}.{imgtype}\",\n",
    "                dpi=dpi,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # plot optimal loss over query\n",
    "    optimal_loss = torch.stack(optimal_loss).numpy()\n",
    "\n",
    "    optimal_loss_results.append(optimal_loss)\n",
    "\n",
    "optimal_loss_results = np.array(optimal_loss_results)\n",
    "mean = optimal_loss_results.mean(0)\n",
    "std = optimal_loss_results.std(0)\n",
    "\n",
    "plt.plot(-mean, label=color[\"C1\"])\n",
    "plt.fill_between(\n",
    "    np.arange(len(mean)),\n",
    "    -(mean - 2 * std),\n",
    "    -(mean + 2 * std),\n",
    "    color=color[\"C2\"],\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.save(f\"{path}/{exp_name}/data.npy\", optimal_loss_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IlchvM39U1ta"
   },
   "outputs": [],
   "source": [
    "# @title DDPG\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Reinformcement learning baseline for non-myopic\n",
    "Bayesian optimization\n",
    "\"\"\"\n",
    "\n",
    "# from synthfunc import SynthFunc\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from argparse import Namespace, ArgumentParser\n",
    "\n",
    "__author__ = \"Sang T. Truong, Willie Neiswanger, Shengjia Zhao, Stefano Ermon\"\n",
    "__copyright__ = \"Copyright 2022, Stanford University\"\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={\"float_kind\": float_formatter})\n",
    "\n",
    "n_dim = 2\n",
    "\n",
    "\n",
    "class SynthFuncEnv(gym.Env):\n",
    "    def __init__(self, n_dim):\n",
    "        self.T = 100\n",
    "        self.state_dim = 1\n",
    "        self.n_dim = n_dim\n",
    "        hypers = {\"ls\": 1.0, \"alpha\": 2.0, \"sigma\": 1e-2, \"n_dimx\": self.n_dim}\n",
    "        self.sf_bounds = [0, 10]\n",
    "        sf_domain = [self.sf_bounds] * self.n_dim\n",
    "        self.lower_bounds = [self.sf_bounds[0]] * self.n_dim\n",
    "        self.upper_bounds = [self.sf_bounds[1]] * self.n_dim\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array(self.lower_bounds, dtype=np.float32),\n",
    "            high=np.array(self.upper_bounds, dtype=np.float32),\n",
    "        )\n",
    "        self.state = torch.tensor([])\n",
    "        self.sf = func\n",
    "\n",
    "    def step(self, action):\n",
    "        act_input = action.reshape(-1, n_dim)\n",
    "        reward = func(act_input)\n",
    "\n",
    "        self.state = torch.cat([self.state, action, torch.tensor([reward])])\n",
    "        self.state_dim = self.state_dim + self.n_dim + 1 if self.state_dim > 1 else 3\n",
    "        self.T = self.T - 1\n",
    "        done = False if self.T > 0 else True\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Return the initial state\n",
    "        \"\"\"\n",
    "        return torch.tensor(0)\n",
    "\n",
    "    def update_action_bound(self, action, r):\n",
    "        self.lower_bounds = []\n",
    "        self.upper_bounds = []\n",
    "        for i in range(self.n_dim):\n",
    "            lower_bound = action[i].item() - r\n",
    "            lower_bound = (\n",
    "                lower_bound if lower_bound >= self.sf_bounds[0] else self.sf_bounds[0]\n",
    "            )\n",
    "            self.lower_bounds.append(lower_bound)\n",
    "\n",
    "            upper_bound = action[i].item() + r\n",
    "            upper_bound = (\n",
    "                upper_bound if upper_bound <= self.sf_bounds[1] else self.sf_bounds[1]\n",
    "            )\n",
    "            self.upper_bounds.append(upper_bound)\n",
    "\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array(self.lower_bounds, dtype=np.float32),\n",
    "            high=np.array(self.upper_bounds, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "        self.lower_bounds = torch.tensor(self.lower_bounds)\n",
    "        self.upper_bounds = torch.tensor(self.upper_bounds)\n",
    "\n",
    "        return self.lower_bounds, self.upper_bounds\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.transition = namedtuple(\n",
    "            \"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\")\n",
    "        )\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = self.transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Noise:\n",
    "    def __init__(self, params):\n",
    "        self.mu = params.mu\n",
    "        self.theta = params.theta\n",
    "        self.sigma = params.sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.full(params.action_dim, self.mu)\n",
    "\n",
    "    def make_noise(self):\n",
    "        state = self.state\n",
    "        delta = self.theta * (self.mu - state) + self.sigma * np.random.randn(\n",
    "            len(state)\n",
    "        )\n",
    "        self.state = state + delta\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(params.state_dim, params.act_hid_1)\n",
    "        self.fc2 = nn.Linear(params.act_hid_1, params.act_hid_2)\n",
    "        self.fc3 = nn.Linear(params.act_hid_2, params.action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.__init__(params)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(params.state_dim, params.crit_hid_1)\n",
    "        self.fc2 = nn.Linear(params.crit_hid_1 + params.action_dim, params.crit_hid_2)\n",
    "        self.fc3 = nn.Linear(params.crit_hid_2, 1)\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params):\n",
    "        self.device = params.device\n",
    "\n",
    "        self.actor = Actor(params).to(self.device)\n",
    "        self.actor_target = Actor(params).to(self.device)\n",
    "        self.critic = Critic(params).to(self.device)\n",
    "        self.critic_target = Critic(params).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(\n",
    "            self.actor.parameters(), lr=params.lr_actor\n",
    "        )\n",
    "        self.critic_optimizer = torch.optim.Adam(\n",
    "            self.critic.parameters(), lr=params.lr_critic\n",
    "        )\n",
    "\n",
    "        self.tau_actor = params.tau_actor\n",
    "        self.tau_critic = params.tau_critic\n",
    "\n",
    "        self.__update(self.actor_target, self.actor)\n",
    "        self.__update(self.critic_target, self.critic)\n",
    "\n",
    "    def __update(self, target, local):\n",
    "        target.load_state_dict(local.state_dict())\n",
    "\n",
    "    def __soft_update(self, target, local, tau):\n",
    "        for target_param, param in zip(target.parameters(), local.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "    def update_target_nn(self):\n",
    "        self.__soft_update(self.actor_target, self.actor, self.tau_actor)\n",
    "        self.__soft_update(self.critic_target, self.critic, self.tau_critic)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, params):\n",
    "        self.device = params.device\n",
    "        self.gamma = params.gamma\n",
    "        self.batch_size = params.batch_size\n",
    "        self.act_up, self.act_down = params.act_up, params.act_down\n",
    "\n",
    "        self.explor_noise = Noise(params)\n",
    "        self.buffer = ReplayBuffer(params.buffer_size)\n",
    "        self.model = Model(params)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) <= self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.buffer.transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = self.tensor(batch.state).float()\n",
    "        action_batch = self.tensor(batch.action).float()\n",
    "        reward_batch = self.tensor(batch.reward).float()\n",
    "        next_state_batch = self.tensor(batch.next_state).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.model.actor_target(next_state_batch)\n",
    "        Q_next = self.model.critic_target(next_state_batch, next_actions).detach()\n",
    "\n",
    "        reward_batch = reward_batch.unsqueeze(1)\n",
    "        not_terminate_batch = ~torch.tensor(batch.done).to(self.device).unsqueeze(1)\n",
    "\n",
    "        Q = self.model.critic(state_batch, action_batch)\n",
    "        Q_expected = reward_batch + self.gamma * Q_next * not_terminate_batch\n",
    "\n",
    "        L = F.mse_loss(Q, Q_expected)\n",
    "        self.model.critic_optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        for param in self.model.critic.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.model.critic_optimizer.step()\n",
    "\n",
    "        a = self.model.actor(state_batch)\n",
    "        L_policy = -self.model.critic(state_batch, a).mean()\n",
    "        self.model.actor_optimizer.zero_grad()\n",
    "        L_policy.backward()\n",
    "        for param in self.model.actor.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.model.actor_optimizer.step()\n",
    "\n",
    "        self.model.update_target_nn()\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        state = state.float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.model.actor(state)\n",
    "        action = action + eps * self.explor_noise.make_noise()\n",
    "        return np.clip(action, self.act_down, self.act_up)\n",
    "\n",
    "    def reset(self):\n",
    "        self.explor_noise.reset()\n",
    "\n",
    "    def update_action_bound(self):\n",
    "        self.act_up, self.act_down = params.act_up, params.act_down\n",
    "\n",
    "\n",
    "class Paramaters:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.state_dim  # env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.act_up = env.upper_bounds  # env.action_space.high[0]\n",
    "        self.act_down = env.lower_bounds  # env.action_space.low[0]\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.seed = 1\n",
    "        self.gamma = 0.9\n",
    "        self.episodes = 1  # there is a single episode\n",
    "        self.max_steps = 20\n",
    "        self.batch_size = 256\n",
    "        self.buffer_size = int(1e4)\n",
    "\n",
    "        self.eps = 1.0\n",
    "        self.eps_decay = 0.95\n",
    "        self.eps_min = 0.01\n",
    "\n",
    "        self.tau_actor = 0.1\n",
    "        self.tau_critic = 0.3\n",
    "        self.lr_actor = 1e-4\n",
    "        self.lr_critic = 1e-3\n",
    "\n",
    "        self.mu = 0\n",
    "        self.theta = 0.15\n",
    "        self.sigma = 0.2\n",
    "\n",
    "        self.act_hid_1, self.act_hid_2 = 128, 128\n",
    "        self.crit_hid_1, self.crit_hid_2 = 128, 128\n",
    "\n",
    "        self.reward_coef = 20\n",
    "\n",
    "        self.froze_seed()\n",
    "\n",
    "        self.r = 0.01  # spotlight radius\n",
    "\n",
    "    def froze_seed(self):\n",
    "        self.env.reset()\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def update_eps(self):\n",
    "        self.eps = max(self.eps_min, self.eps * self.eps_decay)\n",
    "\n",
    "    def update_state_dim(self):\n",
    "        self.state_dim = self.env.state_dim\n",
    "\n",
    "    def update_action_bound(self, action):\n",
    "        self.env.update_action_bound(action, self.r)\n",
    "        self.act_down = env.lower_bounds\n",
    "        self.act_up = env.upper_bounds\n",
    "\n",
    "\n",
    "def run(params, agent, plot_reward=True, plot_action=True):\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    for i in range(1, params.episodes + 1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        params.update_eps()\n",
    "        total_reward, steps = 0, 0\n",
    "        done = False\n",
    "        t = 1\n",
    "        while not done:\n",
    "            action = agent.act(state, params.eps).squeeze()\n",
    "            if len(actions) > 0:\n",
    "                d = ((action - actions[-1]) ** 2).sum().sqrt()\n",
    "                # print(d)\n",
    "            actions.append(action)\n",
    "            params.update_action_bound(action)\n",
    "            agent.update_action_bound()\n",
    "            next_state, reward, done = env.step(action)\n",
    "            params.update_state_dim()\n",
    "            agent.buffer.push(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "            agent.update()\n",
    "            # total_reward += reward\n",
    "            rewards.append(reward)\n",
    "            steps += 1\n",
    "\n",
    "            # print(\"Time step {}, Action: {}, Reward {:.2f}\".format(\n",
    "            #     t, action.numpy(), reward))\n",
    "            t = t + 1\n",
    "        # rewards.append(total_reward)\n",
    "        # print(f\"Episode {i}, reward: {total_reward}\")\n",
    "\n",
    "    if plot_action:\n",
    "        actions = torch.stack(actions)\n",
    "        # print(actions.shape)\n",
    "        plt.scatter(actions[:, 0], actions[:, 1], c=torch.arange(0, actions.shape[0]))\n",
    "        plt.title(\"Action\")\n",
    "        plt.show()\n",
    "\n",
    "    rewards = torch.cat(rewards).numpy()\n",
    "\n",
    "    # print(rewards)\n",
    "    # sang\n",
    "\n",
    "    if plot_reward:\n",
    "        plt.plot(rewards)\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.title(\"Training scores\")\n",
    "        plt.show()\n",
    "    return agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = SynthFuncEnv(n_dim=2)\n",
    "    params = Paramaters(env)\n",
    "    agent = DDPG(params)\n",
    "    run(params=params, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eus3c4PYgM9o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 1\n",
      "Fantasy costs:\n",
      "tensor([0.0087, 0.0094, 0.0097, 0.0095])\n",
      "Acquisition values:\n",
      "tensor([4.9270e-02, 3.3761e-02, 3.3761e-02, 3.3761e-02, 3.3761e-02, 3.3761e-02,\n",
      "        3.3761e-02, 6.0077e-03, 6.0077e-03, 2.3409e-13], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.4953, 0.4925, 0.4925, 0.4925, 0.4925, 0.4925, 0.4925, 0.4586, 0.4586,\n",
      "        0.6423])\n",
      "tensor([0.4586, 0.4925, 0.4953, 0.6423, 0.4925, 0.4925, 0.4925, 0.4586, 0.4925,\n",
      "        0.4925])\n",
      "Best value found so far: -0.02620410919189453\n",
      "Remaining budget: 59.80655935406685\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 2\n",
      "Fantasy costs:\n",
      "tensor([0.3751, 0.1961, 0.3115, 0.2473])\n",
      "Acquisition values:\n",
      "tensor([1.6072e-02, 1.6072e-02, 1.5697e-02, 1.5323e-02, 1.5038e-02, 1.5037e-02,\n",
      "        1.5036e-02, 1.4937e-02, 1.4936e-02, 5.0273e-06], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5027, 0.5027, 0.8986, 0.4741, 0.8901, 0.8913, 0.1326, 0.8972, 0.3652,\n",
      "        0.8601])\n",
      "tensor([0.1326, 0.8601, 0.8972, 0.8901, 0.8986, 0.4741, 0.8913, 0.3652, 0.5027,\n",
      "        0.5027])\n",
      "Best value found so far: -0.013779640197753906\n",
      "Remaining budget: 59.79923716187477\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 3\n",
      "Acquisition values:\n",
      "tensor([5.3721e-03, 5.3721e-03, 5.3720e-03, 5.3720e-03, 5.3709e-03, 1.4665e-06,\n",
      "        1.1707e-06, 1.0141e-06, 5.6840e-07, 5.3494e-07], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.9072, 0.9240, 0.5037, 0.5037, 0.0461, 0.9068, 0.3443, 0.5922, 0.8874,\n",
      "        0.9249])\n",
      "tensor([0.9240, 0.9068, 0.0461, 0.9249, 0.5037, 0.8874, 0.3443, 0.5922, 0.5037,\n",
      "        0.9072])\n",
      "Best value found so far: -0.013779640197753906\n",
      "Remaining budget: 59.394645899534225\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 4\n",
      "Acquisition values:\n",
      "tensor([0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052,\n",
      "        0.0052], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5036, 0.5036, 0.5036, 0.0031, 0.8826, 1.0000, 0.1825, 0.9712, 0.7985,\n",
      "        0.3267])\n",
      "tensor([1.0000, 0.5036, 0.7985, 0.9712, 0.5036, 0.8826, 0.0031, 0.3267, 0.1825,\n",
      "        0.5036])\n",
      "Best value found so far: -0.013779640197753906\n",
      "Remaining budget: 58.991020649671555\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 5\n",
      "Acquisition values:\n",
      "tensor([2.8112e-03, 2.8112e-03, 2.8111e-03, 2.8079e-03, 1.4045e-03, 1.4045e-03,\n",
      "        9.1645e-10, 1.1095e-12, 2.3792e-21, 3.3236e-24], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5027, 0.0076, 0.2193, 0.3984, 0.5056, 0.5056, 0.6658, 0.6277, 0.8061,\n",
      "        0.5870])\n",
      "tensor([0.0076, 0.8061, 0.5056, 0.3984, 0.6658, 0.5027, 0.6277, 0.2193, 0.5870,\n",
      "        0.5056])\n",
      "Best value found so far: -0.013779640197753906\n",
      "Remaining budget: 58.99014237523079\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 6\n",
      "Acquisition values:\n",
      "tensor([4.5787e-03, 2.4782e-03, 2.4781e-03, 2.4770e-03, 2.4756e-03, 2.4316e-03,\n",
      "        1.3281e-03, 2.3773e-08, 1.8743e-08, 4.8713e-09], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5026, 0.5026, 0.5026, 0.7513, 0.6838, 0.4751, 0.4389, 0.1010, 0.4149,\n",
      "        0.5910])\n",
      "tensor([0.7513, 0.5910, 0.5026, 0.6838, 0.4389, 0.4751, 0.5026, 0.1010, 0.4149,\n",
      "        0.5026])\n",
      "Best value found so far: -0.013562202453613281\n",
      "Remaining budget: 58.99001690745354\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 7\n",
      "Acquisition values:\n",
      "tensor([2.2044e-03, 2.2043e-03, 2.2043e-03, 2.2042e-03, 2.2034e-03, 2.2034e-03,\n",
      "        2.2003e-03, 2.1871e-03, 1.4841e-07, 5.4709e-13], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5026, 0.1483, 0.4821, 0.9484, 0.2919, 0.4262, 0.1985, 0.0201, 0.1232,\n",
      "        0.3598])\n",
      "tensor([0.4821, 0.1232, 0.2919, 0.4262, 0.9484, 0.3598, 0.5026, 0.1985, 0.1483,\n",
      "        0.0201])\n",
      "Best value found so far: -0.013403892517089844\n",
      "Remaining budget: 58.98998925089836\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 8\n",
      "Acquisition values:\n",
      "tensor([1.9945e-03, 1.9944e-03, 1.9944e-03, 1.9942e-03, 1.9940e-03, 1.9939e-03,\n",
      "        1.9934e-03, 1.1965e-03, 9.0632e-04, 1.0878e-06], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.5025, 0.2210, 0.7203, 0.2279, 0.5589, 0.7740, 0.0351,\n",
      "        0.2853])\n",
      "tensor([0.2853, 0.5025, 0.7740, 0.2210, 0.7203, 0.0351, 0.2279, 0.5025, 0.5589,\n",
      "        0.5025])\n",
      "Best value found so far: -0.013154029846191406\n",
      "Remaining budget: 58.98994556069374\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 9\n",
      "Acquisition values:\n",
      "tensor([1.7801e-03, 1.7801e-03, 1.7801e-03, 1.7800e-03, 1.7800e-03, 1.7800e-03,\n",
      "        1.7800e-03, 1.7798e-03, 1.7739e-03, 1.4055e-06], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.9206, 0.2572, 0.9730, 0.4359, 0.9516, 0.2026, 0.8565,\n",
      "        0.7819])\n",
      "tensor([0.9206, 0.9516, 0.5025, 0.2572, 0.2026, 0.5025, 0.4359, 0.9730, 0.8565,\n",
      "        0.7819])\n",
      "Best value found so far: -0.012919425964355469\n",
      "Remaining budget: 58.989904314279556\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 10\n",
      "Acquisition values:\n",
      "tensor([1.5971e-03, 1.5971e-03, 1.5970e-03, 1.5970e-03, 1.5969e-03, 1.5953e-03,\n",
      "        2.3830e-07, 1.4375e-07, 1.2523e-07, 4.6282e-11], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.8315, 0.8653, 0.2302, 0.4773, 0.5256, 0.1237, 0.6446,\n",
      "        0.5891])\n",
      "tensor([0.8653, 0.6446, 0.4773, 0.1237, 0.5891, 0.5025, 0.5256, 0.5025, 0.2302,\n",
      "        0.8315])\n",
      "Best value found so far: -0.012654304504394531\n",
      "Remaining budget: 58.9898576438427\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 11\n",
      "Acquisition values:\n",
      "tensor([2.7426e-03, 1.4229e-03, 1.4229e-03, 1.4229e-03, 1.4229e-03, 1.4229e-03,\n",
      "        1.4226e-03, 7.8878e-04, 3.3528e-07, 1.8240e-18], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.0611, 0.5024, 0.5024, 0.5024, 0.1729, 0.9181, 0.7936, 0.3181, 0.1427,\n",
      "        0.8070])\n",
      "tensor([0.1729, 0.3181, 0.0611, 0.8070, 0.5024, 0.7936, 0.1427, 0.5024, 0.9181,\n",
      "        0.5024])\n",
      "Best value found so far: -0.012654304504394531\n",
      "Remaining budget: 58.54854245483875\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 12\n",
      "Fantasy costs:\n",
      "tensor([2.8378e-05, 3.3169e-05, 1.2986e-03, 2.0189e-05])\n",
      "Acquisition values:\n",
      "tensor([1.3967e-03, 1.3967e-03, 1.3967e-03, 1.3967e-03, 4.4557e-10, 4.0204e-10,\n",
      "        3.8723e-10, 3.5099e-10, 2.6317e-10, 1.6359e-10], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5024, 0.5024, 0.5024, 0.5024, 0.4153, 0.4071, 0.4187, 0.4048, 0.4014,\n",
      "        0.3975])\n",
      "tensor([0.5024, 0.4048, 0.5024, 0.4014, 0.4187, 0.5024, 0.5024, 0.3975, 0.4153,\n",
      "        0.4071])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 58.107245087623596\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 13\n",
      "Fantasy costs:\n",
      "tensor([3.6191e+00, 1.1982e-01, 7.4060e-03, 1.8305e-04])\n",
      "Acquisition values:\n",
      "tensor([2.5094e-03, 2.3479e-03, 2.3472e-03, 1.3032e-03, 1.3031e-03, 1.2945e-03,\n",
      "        1.1185e-03, 1.1185e-03, 1.1170e-03, 7.9626e-12], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.1777, 0.5025, 0.3418, 0.1909, 0.7773, 0.4230, 0.5021, 0.5021, 0.6515,\n",
      "        0.7589])\n",
      "tensor([0.5021, 0.7773, 0.1777, 0.5025, 0.1909, 0.4230, 0.7589, 0.5021, 0.6515,\n",
      "        0.3418])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 57.78246562182903\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 14\n",
      "Acquisition values:\n",
      "tensor([1.3367e-03, 1.3367e-03, 1.3367e-03, 1.3367e-03, 1.3367e-03, 1.3367e-03,\n",
      "        1.3366e-03, 1.3346e-03, 1.1330e-03, 3.8007e-12], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.5025, 0.5025, 0.1842, 0.9898, 0.5024, 0.0646, 0.1280,\n",
      "        0.3911])\n",
      "tensor([0.0646, 0.5024, 0.5025, 0.5025, 0.1280, 0.5025, 0.3911, 0.9898, 0.1842,\n",
      "        0.5025])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 57.45763078331947\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 15\n",
      "Acquisition values:\n",
      "tensor([0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0011, 0.0011,\n",
      "        0.0011], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.0968, 0.0103, 0.9520, 0.3269, 0.0219, 0.5025, 0.4307, 0.6274, 0.6856,\n",
      "        0.5024])\n",
      "tensor([0.6856, 0.6274, 0.0219, 0.0968, 0.9520, 0.4307, 0.3269, 0.5024, 0.0103,\n",
      "        0.5025])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 57.05194725096226\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 16\n",
      "Acquisition values:\n",
      "tensor([1.2712e-03, 1.2712e-03, 1.2711e-03, 1.2710e-03, 1.2706e-03, 1.2704e-03,\n",
      "        1.2701e-03, 1.2696e-03, 1.2552e-03, 2.3240e-48], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5026, 0.3256, 0.2927, 0.5026, 0.5025, 0.3657, 0.6962, 0.7991, 0.9897,\n",
      "        0.5686])\n",
      "tensor([0.6962, 0.3256, 0.7991, 0.9897, 0.5026, 0.3657, 0.5025, 0.2927, 0.5026,\n",
      "        0.5686])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 56.64616632461548\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 17\n",
      "Acquisition values:\n",
      "tensor([1.2009e-03, 1.2008e-03, 1.2007e-03, 1.2006e-03, 1.2003e-03, 1.1994e-03,\n",
      "        1.1990e-03, 4.2569e-06, 1.4962e-07, 2.5465e-17], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.5604, 0.7177, 0.3693, 0.0987, 0.6530, 0.0431, 0.4753,\n",
      "        0.9569])\n",
      "tensor([0.5025, 0.9569, 0.0431, 0.6530, 0.5025, 0.4753, 0.0987, 0.3693, 0.5604,\n",
      "        0.7177])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 56.64612013101578\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 18\n",
      "Acquisition values:\n",
      "tensor([1.1467e-03, 1.1450e-03, 1.1450e-03, 1.1450e-03, 1.1450e-03, 1.1449e-03,\n",
      "        1.1448e-03, 1.1445e-03, 1.1443e-03, 3.2524e-07], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.2439, 0.4098, 0.3724, 0.7959, 0.5026, 0.5024, 0.5027, 0.6477,\n",
      "        0.3212])\n",
      "tensor([0.6477, 0.5027, 0.3212, 0.5026, 0.5024, 0.7959, 0.3724, 0.4098, 0.5025,\n",
      "        0.2439])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 56.646119475364685\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 19\n",
      "Acquisition values:\n",
      "tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
      "        0.0009], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5025, 0.5025, 0.5025, 0.2497, 0.7792, 0.4346, 0.5687, 0.1805, 0.8425,\n",
      "        0.0750])\n",
      "tensor([0.5025, 0.5687, 0.7792, 0.5025, 0.0750, 0.4346, 0.5025, 0.2497, 0.1805,\n",
      "        0.8425])\n",
      "Best value found so far: -0.012553215026855469\n",
      "Remaining budget: 56.64604711532593\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 20\n",
      "Acquisition values:\n",
      "tensor([9.1226e-04, 9.1226e-04, 9.1178e-04, 9.1130e-04, 9.1130e-04, 9.1125e-04,\n",
      "        9.1076e-04, 9.1024e-04, 6.7257e-09, 5.3842e-14], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5024, 0.5024, 0.7817, 0.2071, 0.8826, 0.7227, 0.9113, 0.5933, 0.6123,\n",
      "        0.6947])\n",
      "tensor([0.5024, 0.2071, 0.5024, 0.9113, 0.6123, 0.5933, 0.6947, 0.7817, 0.7227,\n",
      "        0.8826])\n",
      "Best value found so far: -0.012444496154785156\n",
      "Remaining budget: 56.64599448442459\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 0\n",
      "Iteration: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 33%|      | 1/3 [09:29<18:58, 569.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition values:\n",
      "tensor([9.8761e-04, 8.5988e-04, 8.5988e-04, 8.5988e-04, 8.5952e-04, 8.5886e-04,\n",
      "        8.5881e-04, 8.5870e-04, 8.5845e-04, 5.5764e-07], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.3139, 0.5024, 0.5024, 0.5024, 0.7462, 0.0893, 0.8337, 0.8830, 0.7652,\n",
      "        0.5209])\n",
      "tensor([0.5024, 0.5024, 0.5024, 0.8337, 0.7652, 0.3139, 0.0893, 0.8830, 0.5209,\n",
      "        0.7462])\n",
      "Best value found so far: -0.012444496154785156\n",
      "Remaining budget: 56.45750242471695\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 1\n",
      "Acquisition values:\n",
      "tensor([0.2861, 0.2710, 0.2709, 0.2707, 0.2679, 0.2667, 0.2584, 0.2507, 0.2467,\n",
      "        0.0010], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5683, 0.4951, 0.2677, 0.1956, 0.4259, 1.0000, 1.0000, 0.3142, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1.0000, 0.1956, 1.0000, 0.2677, 1.0000, 0.4259, 0.4951, 1.0000, 0.3142,\n",
      "        0.5683])\n",
      "Best value found so far: -0.8836782581414124\n",
      "Remaining budget: 59.873373568058014\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 2\n",
      "Acquisition values:\n",
      "tensor([0.4517, 0.4286, 0.4286, 0.4286, 0.4286, 0.4285, 0.4282, 0.4277, 0.4248,\n",
      "        0.0018], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.2698, 0.5018, 0.5018, 0.5018, 0.5018, 0.5762, 1.0000, 0.7157, 0.0000,\n",
      "        0.6537])\n",
      "tensor([0.5762, 0.7157, 0.5018, 1.0000, 0.6537, 0.5018, 0.0000, 0.2698, 0.5018,\n",
      "        0.5018])\n",
      "Best value found so far: -0.8836782581414124\n",
      "Remaining budget: 59.5749032497406\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 3\n",
      "Acquisition values:\n",
      "tensor([4.6998e-01, 4.6572e-01, 4.5711e-01, 4.5279e-01, 4.5279e-01, 4.5279e-01,\n",
      "        4.3518e-01, 4.3518e-01, 3.5106e-03, 6.3039e-05], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5074, 0.5035, 0.6109, 0.1426, 0.6004, 0.6538, 0.8987, 0.7343, 0.0000,\n",
      "        0.9354])\n",
      "tensor([0.9354, 0.1426, 0.8987, 0.5074, 0.7343, 0.6109, 0.6004, 0.6538, 0.0000,\n",
      "        0.5035])\n",
      "Best value found so far: -0.04614734649658203\n",
      "Remaining budget: 59.33727616071701\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 4\n",
      "Acquisition values:\n",
      "tensor([0.0163, 0.0151, 0.0151, 0.0145, 0.0143, 0.0135, 0.0126, 0.0006, 0.0005,\n",
      "        0.0005], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.9922, 0.4985, 0.4985, 0.8934, 0.5945, 0.7500, 0.2810, 0.0000, 0.6820,\n",
      "        0.2682])\n",
      "tensor([0.4985, 0.6820, 0.0000, 0.4985, 0.7500, 0.2682, 0.5945, 0.8934, 0.2810,\n",
      "        0.9922])\n",
      "Best value found so far: -0.04614734649658203\n",
      "Remaining budget: 58.852501928806305\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 5\n",
      "Acquisition values:\n",
      "tensor([0.0129, 0.0127, 0.0127, 0.0127, 0.0107, 0.0107, 0.0107, 0.0087, 0.0086,\n",
      "        0.0060], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.4987, 0.4987, 0.4987, 0.4987, 0.8655, 0.0894, 0.7773, 0.2455, 0.6436,\n",
      "        0.9159])\n",
      "tensor([0.2455, 0.7773, 0.8655, 0.9159, 0.6436, 0.0894, 0.4987, 0.4987, 0.4987,\n",
      "        0.4987])\n",
      "Best value found so far: -0.006302833557128906\n",
      "Remaining budget: 58.3590042591095\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 6\n",
      "Acquisition values:\n",
      "tensor([9.2894e-04, 9.2894e-04, 9.2894e-04, 9.2894e-04, 9.2894e-04, 9.7411e-05,\n",
      "        9.7411e-05, 1.9908e-07, 5.6476e-10, 0.0000e+00], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.4945, 0.4945, 0.4945, 0.4945, 0.4945, 0.0000, 0.0000, 0.5092, 0.5160,\n",
      "        0.4440])\n",
      "tensor([0.4945, 0.0000, 0.0000, 0.4945, 0.4440, 0.4945, 0.5092, 0.4945, 0.4945,\n",
      "        0.5160])\n",
      "Best value found so far: -0.006302833557128906\n",
      "Remaining budget: 58.354831486940384\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 7\n",
      "Acquisition values:\n",
      "tensor([2.0185e-04, 2.0185e-04, 2.0185e-04, 2.0185e-04, 2.0185e-04, 1.1998e-04,\n",
      "        1.1998e-04, 1.1998e-04, 1.1998e-04, 8.0534e-07], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.4957, 0.4957, 0.4957, 0.4957, 0.4957, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5084])\n",
      "tensor([0.5084, 0.4957, 0.4957, 0.4957, 0.4957, 0.0000, 0.0000, 0.4957, 0.0000,\n",
      "        0.0000])\n",
      "Best value found so far: -0.006302833557128906\n",
      "Remaining budget: 58.35365357995033\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 8\n",
      "Acquisition values:\n",
      "tensor([1.2994e-04, 1.2994e-04, 1.2994e-04, 1.2994e-04, 1.2994e-04, 3.3859e-05,\n",
      "        3.3859e-05, 3.3859e-05, 5.2179e-08, 1.3462e-27], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5018, 0.5018, 0.5018, 0.4835,\n",
      "        0.7431])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.5018, 0.5018, 0.4835, 0.7431, 0.0000, 0.0000,\n",
      "        0.5018])\n",
      "Best value found so far: -0.006302833557128906\n",
      "Remaining budget: 57.85793596506119\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 9\n",
      "Fantasy costs:\n",
      "tensor([0.0013, 0.0016, 0.0012, 0.0012])\n",
      "Acquisition values:\n",
      "tensor([5.9627e-05, 1.5287e-06, 1.5287e-06, 6.0868e-07, 1.7221e-08, 9.8761e-09,\n",
      "        5.2801e-10, 4.4018e-12, 4.2169e-12, 5.9489e-13], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.4962, 0.5025, 0.5025, 0.5048, 0.5101, 0.4856, 0.4821, 0.4778, 0.5180,\n",
      "        0.4764])\n",
      "tensor([0.5180, 0.5025, 0.4764, 0.4856, 0.4962, 0.5048, 0.5025, 0.5101, 0.4821,\n",
      "        0.4778])\n",
      "Best value found so far: -0.006302833557128906\n",
      "Remaining budget: 57.36178159713745\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 10\n",
      "Fantasy costs:\n",
      "tensor([0.6334, 0.0306, 0.0077, 0.0068])\n",
      "Acquisition values:\n",
      "tensor([6.2480e-05, 6.2480e-05, 6.2480e-05, 6.2480e-05, 5.9844e-05, 5.9170e-05,\n",
      "        5.4297e-05, 5.4280e-05, 5.3509e-05, 5.3198e-05], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5009, 0.5009, 0.5009, 0.5009, 0.5683, 0.6362, 0.7650, 0.1774, 0.3223,\n",
      "        0.7347])\n",
      "tensor([0.5009, 0.5009, 0.5009, 0.5683, 0.1774, 0.3223, 0.7347, 0.5009, 0.6362,\n",
      "        0.7650])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 57.357044994831085\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 11\n",
      "Acquisition values:\n",
      "tensor([7.1086e-05, 7.1086e-05, 7.1065e-05, 7.1044e-05, 7.1032e-05, 7.1010e-05,\n",
      "        7.0937e-05, 6.7621e-05, 8.7737e-08, 4.3553e-08], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5011, 0.5011, 0.6776, 0.8464, 0.4410, 0.0838, 0.6163, 0.4118, 0.3310,\n",
      "        0.4848])\n",
      "tensor([0.8464, 0.6776, 0.0838, 0.5011, 0.5011, 0.4848, 0.3310, 0.6163, 0.4118,\n",
      "        0.4410])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 57.35687041282654\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 12\n",
      "Acquisition values:\n",
      "tensor([9.8731e-05, 9.8731e-05, 9.8730e-05, 9.8729e-05, 9.8723e-05, 9.8723e-05,\n",
      "        9.8260e-05, 9.7754e-05, 1.0365e-08, 4.3061e-62], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5012, 0.5012, 0.8042, 0.8773, 0.6497, 0.1304, 0.2926, 0.6352, 0.0439,\n",
      "        0.5446])\n",
      "tensor([0.1304, 0.0439, 0.6497, 0.5012, 0.6352, 0.2926, 0.8773, 0.8042, 0.5446,\n",
      "        0.5012])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 57.35668748617172\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 13\n",
      "Acquisition values:\n",
      "tensor([ 1.1526e-04,  1.1526e-04,  1.1526e-04,  1.1525e-04,  1.1525e-04,\n",
      "         1.1525e-04,  1.1523e-04,  1.1520e-04,  1.1514e-04, -8.4940e-18],\n",
      "       dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5012, 0.5012, 0.8343, 0.8957, 0.8377, 0.6648, 0.0677, 0.0340, 0.5655,\n",
      "        0.0390])\n",
      "tensor([0.8957, 0.5655, 0.0340, 0.6648, 0.8377, 0.0677, 0.5012, 0.0390, 0.5012,\n",
      "        0.8343])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 57.356679797172546\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 14\n",
      "Acquisition values:\n",
      "tensor([1.2689e-04, 1.2677e-04, 1.2677e-04, 1.2676e-04, 1.2674e-04, 1.2669e-04,\n",
      "        1.2534e-04, 1.2258e-04, 1.0511e-06, 9.7985e-08], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.1507, 0.5011, 0.7950, 0.7592, 0.5011, 0.5011, 0.3657, 0.4494, 0.0460,\n",
      "        0.5893])\n",
      "tensor([0.3657, 0.0460, 0.4494, 0.7950, 0.5011, 0.5011, 0.7592, 0.5011, 0.5893,\n",
      "        0.1507])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 57.00615380704403\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 15\n",
      "Acquisition values:\n",
      "tensor([2.3215e-04, 1.3296e-04, 1.3296e-04, 1.3275e-04, 1.3225e-04, 1.3191e-04,\n",
      "        1.2647e-04, 9.2207e-05, 8.5149e-05, 4.4451e-06], dtype=torch.float64)\n",
      "Candidates:\n",
      "tensor([0.5011, 0.9748, 0.6446, 0.1175, 0.2493, 0.3411, 0.4536, 0.5015, 0.4629,\n",
      "        0.5088])\n",
      "tensor([0.5088, 0.4629, 0.5015, 0.3411, 0.4536, 0.6446, 0.5011, 0.9748, 0.1175,\n",
      "        0.2493])\n",
      "Best value found so far: -0.004286766052246094\n",
      "Remaining budget: 56.65580889582634\n",
      "Problem: ackley\n",
      "Sampling policy: B-MS-EI_111_1_60\n",
      "Trial: 1\n",
      "Iteration: 16\n"
     ]
    }
   ],
   "source": [
    "# @title BudgetBO\n",
    "# from botorch.settings import debug\n",
    "# from math import pi\n",
    "# from typing import Callable\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "# debug._set_state(True)\n",
    "\n",
    "# script_dir = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "# sys.path.append(script_dir[:-12])\n",
    "# sys.path.append(script_dir[:-11] + \"budgeted_bo\")\n",
    "# sys.path.append(script_dir[:-11] + \"budgeted_bo/acquisition_functions\")\n",
    "\n",
    "\n",
    "# Algos\n",
    "algo = \"B-MS-EI\"\n",
    "\n",
    "if algo == \"B-MS-EI\":\n",
    "    algo_params = {\n",
    "        \"lookahead_n_fantasies\": [1, 1, 1],\n",
    "        \"refill_until_lower_bound_is_reached\": True,\n",
    "        \"soft_plus_transform_budget\": False,\n",
    "    }\n",
    "else:\n",
    "    algo_params = {}\n",
    "\n",
    "n_init_evals = 8\n",
    "n_dim = \"1\"  # @param [1, 2]\n",
    "n_dim = int(n_dim)\n",
    "nonmyopic = True  # @param {type:\"boolean\"}\n",
    "budget = 20  # @param {type:\"integer\"}\n",
    "save = True\n",
    "draw_legend = False\n",
    "\n",
    "if n_dim == 1:\n",
    "    dim_xi = [n_dim]\n",
    "    prev_x = torch.tensor([0.3])\n",
    "    prev_x_str = \"0.3\"\n",
    "elif n_dim == 2:\n",
    "    dim_xi = [1, n_dim]\n",
    "    prev_x = torch.tensor([[0.4, 0.4]])\n",
    "    prev_x_str = \"0.4_0.4\"\n",
    "\n",
    "neighbor_size = 1 / budget  # 0.02\n",
    "n_init = 0\n",
    "ehig_opt_epoch = 50\n",
    "ehig_opt_lr = 0.1\n",
    "use_lr_schedule = False\n",
    "train = False\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "\n",
    "# Run experiment\n",
    "first_trial = 0\n",
    "last_trial = 2\n",
    "# if len(sys.argv) == 3:\n",
    "#     first_trial = int(sys.argv[1])\n",
    "#     last_trial =  int(sys.argv[2])\n",
    "# elif len(sys.argv) == 2:\n",
    "#     first_trial = int(sys.argv[1])\n",
    "#     last_trial =  int(sys.argv[1])\n",
    "\n",
    "optimal_loss_results = []\n",
    "\n",
    "exp_name = f\"{n_dim}D_nonmyopic{nonmyopic}_budget{budget}_\\\n",
    "neighbor_size{neighbor_size}_prev_x{prev_x_str}\"\n",
    "\n",
    "# create folder\n",
    "if not os.path.exists(f\"{path}/{exp_name}\"):\n",
    "    os.makedirs(f\"{path}/{exp_name}\")\n",
    "    print(f\"Saving to: {path}/{exp_name}\")\n",
    "\n",
    "\n",
    "Xs, objective_Xs = experiment_manager(\n",
    "    problem=\"ackley\",\n",
    "    algo=algo,\n",
    "    algo_params=algo_params,\n",
    "    restart=False,\n",
    "    first_trial=first_trial,\n",
    "    last_trial=last_trial,\n",
    "    get_objective_cost_function=get_objective_cost_function,\n",
    "    input_dim=n_dim,\n",
    "    n_init_evals=n_init_evals,\n",
    "    budget=60.0,\n",
    "    n_max_iter=20,\n",
    ")\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    X = Xs[seed]\n",
    "    objective_X = objective_Xs[seed]\n",
    "\n",
    "    train_x = X[:n_init_evals]\n",
    "    train_y = objective_X[:n_init_evals]\n",
    "\n",
    "    model = init_model(\n",
    "        n_dim=n_dim, prev_x=prev_x, train_x=train_x, train_y=train_y, train=train\n",
    "    )\n",
    "\n",
    "    if nonmyopic:\n",
    "        horizon = budget\n",
    "    else:\n",
    "        horizon = 1\n",
    "    optimal_loss = []\n",
    "    for step in range(budget):\n",
    "        with torch.no_grad():\n",
    "            next_x = X[n_init_evals + step]\n",
    "            next_y = objective_X[n_init_evals + step]\n",
    "\n",
    "            # draw_posterior(n_dim=n_dim, model=model)\n",
    "            # if n_dim == 2: plt.show()\n",
    "\n",
    "            # update data and model\n",
    "            train_x = torch.cat([train_x, next_x.reshape(-1, n_dim)])\n",
    "            train_y = torch.cat([train_y, next_y])\n",
    "\n",
    "            model = init_model(\n",
    "                n_dim=n_dim,\n",
    "                prev_x=prev_x,\n",
    "                train_x=train_x,\n",
    "                train_y=train_y,\n",
    "                train=train,\n",
    "            )\n",
    "\n",
    "        # eval\n",
    "        a = (torch.rand([n_dim] if n_dim == 1 else [1, n_dim])).requires_grad_(True)\n",
    "        optimizer = optim.Adam([a], lr=ehig_opt_lr)\n",
    "\n",
    "        loss = []\n",
    "        for epoch in range(10):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            a = torch.sigmoid(a) * (neighbor_size * 2) + (next_x - neighbor_size)\n",
    "\n",
    "            p_y_on_a_D = model.posterior(a)\n",
    "            ell = p_y_on_a_D.mean.mean()\n",
    "            ell.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            loss.append(ell.clone().detach())\n",
    "\n",
    "        loss = torch.stack(loss).squeeze()\n",
    "        print(f\"Current optimal loss {loss.min().item()};\" f\"optimal action: {a}\")\n",
    "        optimal_loss.append(loss.min())\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dim_xi = [n_dim]\n",
    "        elif n_dim == 2:\n",
    "            dim_xi = [1, n_dim]\n",
    "\n",
    "        # plot & update prev_x\n",
    "        if n_dim == 1:\n",
    "            ground_truth(draw_true_model=True, n_dim=n_dim)\n",
    "\n",
    "        plt.title(f\"Step {step} with horizon of {horizon}\")\n",
    "\n",
    "        # draw_posterior(n_dim=n_dim, model=model)\n",
    "\n",
    "        if n_dim == 1:\n",
    "            dmin = -4\n",
    "            dmax = 4\n",
    "            plt.vlines(prev_x, dmin, dmax, color=color[\"C5\"], label=\"Current location\")\n",
    "            plt.vlines(\n",
    "                prev_x - neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                prev_x + neighbor_size, dmin, dmax, color=\"black\", linestyle=\"--\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.plot(\n",
    "                train_x.squeeze().cpu().numpy(), train_y.squeeze().cpu().numpy(), \"k*\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[0], dmin, dmax, color=color[\"C3\"], label=\"Optimal query\"\n",
    "            )\n",
    "            plt.vlines(\n",
    "                best_result[-1], dmin, dmax, color=color[\"C4\"], label=\"Optimal action\"\n",
    "            )\n",
    "            plt.ylim(dmin, dmax)\n",
    "\n",
    "        elif n_dim == 2:\n",
    "            plt.hlines(prev_x[0, 1] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.hlines(prev_x[0, 1] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] - neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.vlines(prev_x[0, 0] + neighbor_size, 0, 1, linestyle=\"--\")\n",
    "            plt.scatter(\n",
    "                prev_x[0, 0], prev_x[0, 1], color=color[\"C5\"], label=\"Current location\"\n",
    "            )\n",
    "            prev_x = next_x\n",
    "            plt.scatter(train_x[:, 0], train_x[:, 1], marker=\"*\", color=\"black\")\n",
    "            plt.scatter(\n",
    "                best_result[0][0, 0],\n",
    "                best_result[0][0, 1],\n",
    "                color=color[\"C3\"],\n",
    "                label=\"Optimal query\",\n",
    "            )\n",
    "            plt.scatter(\n",
    "                best_result[-1][..., 0],\n",
    "                best_result[-1][..., 1],\n",
    "                color=color[\"C4\"],\n",
    "                label=\"Optimal action\",\n",
    "            )\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if draw_legend:\n",
    "            plt.legend()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(\n",
    "                f\"{path}/{exp_name}/seed{seed}_step{step}.{imgtype}\",\n",
    "                dpi=dpi,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # plot optimal loss over query\n",
    "    optimal_loss = torch.stack(optimal_loss).numpy()\n",
    "\n",
    "    optimal_loss_results.append(optimal_loss)\n",
    "\n",
    "optimal_loss_results = np.array(optimal_loss_results)\n",
    "mean = optimal_loss_results.mean(0)\n",
    "std = optimal_loss_results.std(0)\n",
    "\n",
    "plt.plot(-mean, label=color[\"C1\"])\n",
    "plt.fill_between(\n",
    "    np.arange(len(mean)),\n",
    "    -(mean - 2 * std),\n",
    "    -(mean + 2 * std),\n",
    "    color=color[\"C2\"],\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.save(f\"{path}/{exp_name}/data.npy\", optimal_loss_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_AHhTRtKuQzv",
    "Vy5q11RivXUg",
    "-S8W0QP9vhrZ",
    "M4JXv67JHgGE",
    "IUKH5pJlI3Ap",
    "PqAWyNBLPIeA",
    "Gt5d-q6rRAI1",
    "Mel2sfi-C4bz"
   ],
   "name": "",
   "provenance": [
    {
     "file_id": "17Al06PL_nmOQjPQOcJT5CdIL5mCutuNQ",
     "timestamp": 1665960005636
    },
    {
     "file_id": "1HZgAwnVqIGd6JGtCuAUSCENqXA01xPEX",
     "timestamp": 1664851779510
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}