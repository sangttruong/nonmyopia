# Model for PPO finetuning
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
use_peft: true
lora_alpha: 16
lora_r: 8
lora_dropout: 0.1
lora_target_modules: ["q_proj", "v_proj"]
output_dir: ckpts/HES-TS-AM-10-10seq/0
verify_rollout: acqfs.spotlight_cost_fn
max_rollout_retry: 32
alter_response: acqfs.random_edit_seq
discount_reward_factor: 0.95

# Reward model
reward_model: http://localhost:8000

# Dataset
query_dataset: data/ppo_2024-09-05T12:48:39.226187
max_new_tokens: 1024
ppo_epochs: 4
mini_batch_size: 1
batch_size: 1

# Hyperparameters
gradient_accumulation_steps: 1
gradient_checkpointing: true
learning_rate: 1.0e-4
save_steps: 1000